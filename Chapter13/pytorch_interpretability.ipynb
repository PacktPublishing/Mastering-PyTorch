{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x111ba6a90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training and inference routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # us argmax to get the most likely prediction\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset),\n",
    "        100. * success / len(test_dataloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
    "    batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,)) \n",
    "                   ])),\n",
    "    batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define optimizer and run training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.324445\n",
      "epoch: 1 [320/60000 (1%)]\t training loss: 1.727462\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 1.428922\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 0.717944\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 0.572199\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 0.847963\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.542137\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.384015\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 0.228308\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.645798\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.228496\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.163134\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.362703\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.564533\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.438789\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.337578\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.147629\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.138111\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.365426\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.272302\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.515796\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.082148\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.262095\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.392221\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.571694\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.312862\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 0.184791\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.271020\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.090411\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.270769\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.109064\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.168325\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.176047\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.167648\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.083286\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.158884\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.097113\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.185290\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.129792\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.136585\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.302540\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.110367\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.066394\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.146977\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.105319\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.070504\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.093309\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.064225\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.079789\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.150749\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.155598\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.095000\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.086708\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.066358\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 0.148567\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.148145\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.327620\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.408916\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.191311\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.119815\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.092802\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.063648\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.068533\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.049867\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.243833\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.121979\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.375598\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.097202\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.299345\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.082849\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.063586\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.013879\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.333295\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.041122\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.039314\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.181948\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.012823\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.250396\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.152133\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.102777\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.010696\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 0.115096\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.225304\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.312823\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.163087\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.228723\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.038268\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.410418\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.082726\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.162318\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.090068\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.017724\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.013146\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.274080\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 0.179229\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.136015\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.071711\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.036780\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.023768\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.111370\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.230612\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.158286\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.055899\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.032352\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.164713\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.022729\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.199687\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.293791\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.065565\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.080718\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.110381\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.218280\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.018556\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.008146\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.008403\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.091116\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.213662\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.105502\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.159514\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.144104\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.052964\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 0.090900\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.163139\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.301166\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.255332\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.129499\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.221139\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.216212\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.007415\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.044144\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.026546\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.003964\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.198030\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.006516\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.123108\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.194054\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.000510\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.060231\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.154855\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.104090\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.160464\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.034415\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.418779\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.220907\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.087795\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.031318\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.010477\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.143398\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.161950\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 0.066478\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.026126\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.090441\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.070423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.019394\n",
      "epoch: 1 [49280/60000 (82%)]\t training loss: 0.187325\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.038311\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.034758\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.039249\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.079248\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.051400\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.162171\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.229157\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.147724\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.060992\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.125837\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.227963\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.009805\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.002539\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.032409\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.038749\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.015961\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.524187\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.003081\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.170955\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.075366\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.210022\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.191859\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.044687\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.148947\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.002516\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.069967\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.129378\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.002436\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.085025\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.255428\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.007597\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.239532\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.010049\n",
      "\n",
      "Test dataset: Overall Loss: 0.0505, Overall Accuracy: 9830/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.026652\n",
      "epoch: 2 [320/60000 (1%)]\t training loss: 0.070869\n",
      "epoch: 2 [640/60000 (1%)]\t training loss: 0.057445\n",
      "epoch: 2 [960/60000 (2%)]\t training loss: 0.085456\n",
      "epoch: 2 [1280/60000 (2%)]\t training loss: 0.117348\n",
      "epoch: 2 [1600/60000 (3%)]\t training loss: 0.174696\n",
      "epoch: 2 [1920/60000 (3%)]\t training loss: 0.007304\n",
      "epoch: 2 [2240/60000 (4%)]\t training loss: 0.090935\n",
      "epoch: 2 [2560/60000 (4%)]\t training loss: 0.055922\n",
      "epoch: 2 [2880/60000 (5%)]\t training loss: 0.073811\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.057724\n",
      "epoch: 2 [3520/60000 (6%)]\t training loss: 0.136447\n",
      "epoch: 2 [3840/60000 (6%)]\t training loss: 0.018730\n",
      "epoch: 2 [4160/60000 (7%)]\t training loss: 0.108603\n",
      "epoch: 2 [4480/60000 (7%)]\t training loss: 0.391342\n",
      "epoch: 2 [4800/60000 (8%)]\t training loss: 0.022759\n",
      "epoch: 2 [5120/60000 (9%)]\t training loss: 0.055079\n",
      "epoch: 2 [5440/60000 (9%)]\t training loss: 0.126274\n",
      "epoch: 2 [5760/60000 (10%)]\t training loss: 0.041692\n",
      "epoch: 2 [6080/60000 (10%)]\t training loss: 0.004615\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.018584\n",
      "epoch: 2 [6720/60000 (11%)]\t training loss: 0.025280\n",
      "epoch: 2 [7040/60000 (12%)]\t training loss: 0.064174\n",
      "epoch: 2 [7360/60000 (12%)]\t training loss: 0.010033\n",
      "epoch: 2 [7680/60000 (13%)]\t training loss: 0.028199\n",
      "epoch: 2 [8000/60000 (13%)]\t training loss: 0.002471\n",
      "epoch: 2 [8320/60000 (14%)]\t training loss: 0.187729\n",
      "epoch: 2 [8640/60000 (14%)]\t training loss: 0.009139\n",
      "epoch: 2 [8960/60000 (15%)]\t training loss: 0.346025\n",
      "epoch: 2 [9280/60000 (15%)]\t training loss: 0.087234\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.203256\n",
      "epoch: 2 [9920/60000 (17%)]\t training loss: 0.037920\n",
      "epoch: 2 [10240/60000 (17%)]\t training loss: 0.006958\n",
      "epoch: 2 [10560/60000 (18%)]\t training loss: 0.048929\n",
      "epoch: 2 [10880/60000 (18%)]\t training loss: 0.033129\n",
      "epoch: 2 [11200/60000 (19%)]\t training loss: 0.087738\n",
      "epoch: 2 [11520/60000 (19%)]\t training loss: 0.003521\n",
      "epoch: 2 [11840/60000 (20%)]\t training loss: 0.031817\n",
      "epoch: 2 [12160/60000 (20%)]\t training loss: 0.018999\n",
      "epoch: 2 [12480/60000 (21%)]\t training loss: 0.358556\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.042506\n",
      "epoch: 2 [13120/60000 (22%)]\t training loss: 0.025516\n",
      "epoch: 2 [13440/60000 (22%)]\t training loss: 0.012223\n",
      "epoch: 2 [13760/60000 (23%)]\t training loss: 0.308418\n",
      "epoch: 2 [14080/60000 (23%)]\t training loss: 0.087963\n",
      "epoch: 2 [14400/60000 (24%)]\t training loss: 0.213901\n",
      "epoch: 2 [14720/60000 (25%)]\t training loss: 0.067540\n",
      "epoch: 2 [15040/60000 (25%)]\t training loss: 0.106260\n",
      "epoch: 2 [15360/60000 (26%)]\t training loss: 0.073747\n",
      "epoch: 2 [15680/60000 (26%)]\t training loss: 0.141864\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.039989\n",
      "epoch: 2 [16320/60000 (27%)]\t training loss: 0.174354\n",
      "epoch: 2 [16640/60000 (28%)]\t training loss: 0.206837\n",
      "epoch: 2 [16960/60000 (28%)]\t training loss: 0.010143\n",
      "epoch: 2 [17280/60000 (29%)]\t training loss: 0.013667\n",
      "epoch: 2 [17600/60000 (29%)]\t training loss: 0.038229\n",
      "epoch: 2 [17920/60000 (30%)]\t training loss: 0.217477\n",
      "epoch: 2 [18240/60000 (30%)]\t training loss: 0.007758\n",
      "epoch: 2 [18560/60000 (31%)]\t training loss: 0.200778\n",
      "epoch: 2 [18880/60000 (31%)]\t training loss: 0.125584\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.081187\n",
      "epoch: 2 [19520/60000 (33%)]\t training loss: 0.046055\n",
      "epoch: 2 [19840/60000 (33%)]\t training loss: 0.024323\n",
      "epoch: 2 [20160/60000 (34%)]\t training loss: 0.122499\n",
      "epoch: 2 [20480/60000 (34%)]\t training loss: 0.035082\n",
      "epoch: 2 [20800/60000 (35%)]\t training loss: 0.047155\n",
      "epoch: 2 [21120/60000 (35%)]\t training loss: 0.067457\n",
      "epoch: 2 [21440/60000 (36%)]\t training loss: 0.013462\n",
      "epoch: 2 [21760/60000 (36%)]\t training loss: 0.012814\n",
      "epoch: 2 [22080/60000 (37%)]\t training loss: 0.125509\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.308333\n",
      "epoch: 2 [22720/60000 (38%)]\t training loss: 0.014304\n",
      "epoch: 2 [23040/60000 (38%)]\t training loss: 0.038671\n",
      "epoch: 2 [23360/60000 (39%)]\t training loss: 0.116996\n",
      "epoch: 2 [23680/60000 (39%)]\t training loss: 0.009471\n",
      "epoch: 2 [24000/60000 (40%)]\t training loss: 0.065241\n",
      "epoch: 2 [24320/60000 (41%)]\t training loss: 0.018996\n",
      "epoch: 2 [24640/60000 (41%)]\t training loss: 0.021061\n",
      "epoch: 2 [24960/60000 (42%)]\t training loss: 0.008845\n",
      "epoch: 2 [25280/60000 (42%)]\t training loss: 0.176385\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.018136\n",
      "epoch: 2 [25920/60000 (43%)]\t training loss: 0.186989\n",
      "epoch: 2 [26240/60000 (44%)]\t training loss: 0.075497\n",
      "epoch: 2 [26560/60000 (44%)]\t training loss: 0.017575\n",
      "epoch: 2 [26880/60000 (45%)]\t training loss: 0.074589\n",
      "epoch: 2 [27200/60000 (45%)]\t training loss: 0.027338\n",
      "epoch: 2 [27520/60000 (46%)]\t training loss: 0.140608\n",
      "epoch: 2 [27840/60000 (46%)]\t training loss: 0.024984\n",
      "epoch: 2 [28160/60000 (47%)]\t training loss: 0.027213\n",
      "epoch: 2 [28480/60000 (47%)]\t training loss: 0.019386\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.007937\n",
      "epoch: 2 [29120/60000 (49%)]\t training loss: 0.116454\n",
      "epoch: 2 [29440/60000 (49%)]\t training loss: 0.005290\n",
      "epoch: 2 [29760/60000 (50%)]\t training loss: 0.002524\n",
      "epoch: 2 [30080/60000 (50%)]\t training loss: 0.035815\n",
      "epoch: 2 [30400/60000 (51%)]\t training loss: 0.002861\n",
      "epoch: 2 [30720/60000 (51%)]\t training loss: 0.028686\n",
      "epoch: 2 [31040/60000 (52%)]\t training loss: 0.002700\n",
      "epoch: 2 [31360/60000 (52%)]\t training loss: 0.077069\n",
      "epoch: 2 [31680/60000 (53%)]\t training loss: 0.021522\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.270036\n",
      "epoch: 2 [32320/60000 (54%)]\t training loss: 0.118578\n",
      "epoch: 2 [32640/60000 (54%)]\t training loss: 0.091518\n",
      "epoch: 2 [32960/60000 (55%)]\t training loss: 0.031242\n",
      "epoch: 2 [33280/60000 (55%)]\t training loss: 0.112431\n",
      "epoch: 2 [33600/60000 (56%)]\t training loss: 0.122917\n",
      "epoch: 2 [33920/60000 (57%)]\t training loss: 0.032982\n",
      "epoch: 2 [34240/60000 (57%)]\t training loss: 0.016649\n",
      "epoch: 2 [34560/60000 (58%)]\t training loss: 0.007780\n",
      "epoch: 2 [34880/60000 (58%)]\t training loss: 0.065210\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.008417\n",
      "epoch: 2 [35520/60000 (59%)]\t training loss: 0.005487\n",
      "epoch: 2 [35840/60000 (60%)]\t training loss: 0.044910\n",
      "epoch: 2 [36160/60000 (60%)]\t training loss: 0.143551\n",
      "epoch: 2 [36480/60000 (61%)]\t training loss: 0.005923\n",
      "epoch: 2 [36800/60000 (61%)]\t training loss: 0.009386\n",
      "epoch: 2 [37120/60000 (62%)]\t training loss: 0.098139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 [37440/60000 (62%)]\t training loss: 0.036920\n",
      "epoch: 2 [37760/60000 (63%)]\t training loss: 0.077472\n",
      "epoch: 2 [38080/60000 (63%)]\t training loss: 0.134896\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.005788\n",
      "epoch: 2 [38720/60000 (65%)]\t training loss: 0.068943\n",
      "epoch: 2 [39040/60000 (65%)]\t training loss: 0.081437\n",
      "epoch: 2 [39360/60000 (66%)]\t training loss: 0.007516\n",
      "epoch: 2 [39680/60000 (66%)]\t training loss: 0.080189\n",
      "epoch: 2 [40000/60000 (67%)]\t training loss: 0.002070\n",
      "epoch: 2 [40320/60000 (67%)]\t training loss: 0.098127\n",
      "epoch: 2 [40640/60000 (68%)]\t training loss: 0.028956\n",
      "epoch: 2 [40960/60000 (68%)]\t training loss: 0.018094\n",
      "epoch: 2 [41280/60000 (69%)]\t training loss: 0.003738\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.016676\n",
      "epoch: 2 [41920/60000 (70%)]\t training loss: 0.026290\n",
      "epoch: 2 [42240/60000 (70%)]\t training loss: 0.125815\n",
      "epoch: 2 [42560/60000 (71%)]\t training loss: 0.238040\n",
      "epoch: 2 [42880/60000 (71%)]\t training loss: 0.104155\n",
      "epoch: 2 [43200/60000 (72%)]\t training loss: 0.011201\n",
      "epoch: 2 [43520/60000 (73%)]\t training loss: 0.175732\n",
      "epoch: 2 [43840/60000 (73%)]\t training loss: 0.042091\n",
      "epoch: 2 [44160/60000 (74%)]\t training loss: 0.025688\n",
      "epoch: 2 [44480/60000 (74%)]\t training loss: 0.211947\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.048630\n",
      "epoch: 2 [45120/60000 (75%)]\t training loss: 0.114814\n",
      "epoch: 2 [45440/60000 (76%)]\t training loss: 0.075092\n",
      "epoch: 2 [45760/60000 (76%)]\t training loss: 0.006968\n",
      "epoch: 2 [46080/60000 (77%)]\t training loss: 0.012426\n",
      "epoch: 2 [46400/60000 (77%)]\t training loss: 0.012113\n",
      "epoch: 2 [46720/60000 (78%)]\t training loss: 0.014911\n",
      "epoch: 2 [47040/60000 (78%)]\t training loss: 0.060860\n",
      "epoch: 2 [47360/60000 (79%)]\t training loss: 0.111169\n",
      "epoch: 2 [47680/60000 (79%)]\t training loss: 0.289105\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.005683\n",
      "epoch: 2 [48320/60000 (81%)]\t training loss: 0.346652\n",
      "epoch: 2 [48640/60000 (81%)]\t training loss: 0.006729\n",
      "epoch: 2 [48960/60000 (82%)]\t training loss: 0.000575\n",
      "epoch: 2 [49280/60000 (82%)]\t training loss: 0.146273\n",
      "epoch: 2 [49600/60000 (83%)]\t training loss: 0.007920\n",
      "epoch: 2 [49920/60000 (83%)]\t training loss: 0.002877\n",
      "epoch: 2 [50240/60000 (84%)]\t training loss: 0.007187\n",
      "epoch: 2 [50560/60000 (84%)]\t training loss: 0.028408\n",
      "epoch: 2 [50880/60000 (85%)]\t training loss: 0.047759\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.077178\n",
      "epoch: 2 [51520/60000 (86%)]\t training loss: 0.002737\n",
      "epoch: 2 [51840/60000 (86%)]\t training loss: 0.029900\n",
      "epoch: 2 [52160/60000 (87%)]\t training loss: 0.001232\n",
      "epoch: 2 [52480/60000 (87%)]\t training loss: 0.011492\n",
      "epoch: 2 [52800/60000 (88%)]\t training loss: 0.154808\n",
      "epoch: 2 [53120/60000 (89%)]\t training loss: 0.029054\n",
      "epoch: 2 [53440/60000 (89%)]\t training loss: 0.006488\n",
      "epoch: 2 [53760/60000 (90%)]\t training loss: 0.148243\n",
      "epoch: 2 [54080/60000 (90%)]\t training loss: 0.139013\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 0.090420\n",
      "epoch: 2 [54720/60000 (91%)]\t training loss: 0.201699\n",
      "epoch: 2 [55040/60000 (92%)]\t training loss: 0.005958\n",
      "epoch: 2 [55360/60000 (92%)]\t training loss: 0.003726\n",
      "epoch: 2 [55680/60000 (93%)]\t training loss: 0.053425\n",
      "epoch: 2 [56000/60000 (93%)]\t training loss: 0.038958\n",
      "epoch: 2 [56320/60000 (94%)]\t training loss: 0.047118\n",
      "epoch: 2 [56640/60000 (94%)]\t training loss: 0.011511\n",
      "epoch: 2 [56960/60000 (95%)]\t training loss: 0.059546\n",
      "epoch: 2 [57280/60000 (95%)]\t training loss: 0.099137\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.005876\n",
      "epoch: 2 [57920/60000 (97%)]\t training loss: 0.156780\n",
      "epoch: 2 [58240/60000 (97%)]\t training loss: 0.003235\n",
      "epoch: 2 [58560/60000 (98%)]\t training loss: 0.054964\n",
      "epoch: 2 [58880/60000 (98%)]\t training loss: 0.000733\n",
      "epoch: 2 [59200/60000 (99%)]\t training loss: 0.008825\n",
      "epoch: 2 [59520/60000 (99%)]\t training loss: 0.045002\n",
      "epoch: 2 [59840/60000 (100%)]\t training loss: 0.091497\n",
      "\n",
      "Test dataset: Overall Loss: 0.0393, Overall Accuracy: 9869/10000 (99%)\n",
      "\n",
      "epoch: 3 [0/60000 (0%)]\t training loss: 0.000966\n",
      "epoch: 3 [320/60000 (1%)]\t training loss: 0.133099\n",
      "epoch: 3 [640/60000 (1%)]\t training loss: 0.033448\n",
      "epoch: 3 [960/60000 (2%)]\t training loss: 0.001536\n",
      "epoch: 3 [1280/60000 (2%)]\t training loss: 0.080074\n",
      "epoch: 3 [1600/60000 (3%)]\t training loss: 0.002650\n",
      "epoch: 3 [1920/60000 (3%)]\t training loss: 0.156422\n",
      "epoch: 3 [2240/60000 (4%)]\t training loss: 0.004641\n",
      "epoch: 3 [2560/60000 (4%)]\t training loss: 0.001839\n",
      "epoch: 3 [2880/60000 (5%)]\t training loss: 0.001261\n",
      "epoch: 3 [3200/60000 (5%)]\t training loss: 0.010333\n",
      "epoch: 3 [3520/60000 (6%)]\t training loss: 0.000750\n",
      "epoch: 3 [3840/60000 (6%)]\t training loss: 0.066565\n",
      "epoch: 3 [4160/60000 (7%)]\t training loss: 0.028304\n",
      "epoch: 3 [4480/60000 (7%)]\t training loss: 0.019446\n",
      "epoch: 3 [4800/60000 (8%)]\t training loss: 0.017495\n",
      "epoch: 3 [5120/60000 (9%)]\t training loss: 0.015415\n",
      "epoch: 3 [5440/60000 (9%)]\t training loss: 0.173513\n",
      "epoch: 3 [5760/60000 (10%)]\t training loss: 0.172110\n",
      "epoch: 3 [6080/60000 (10%)]\t training loss: 0.172384\n",
      "epoch: 3 [6400/60000 (11%)]\t training loss: 0.010512\n",
      "epoch: 3 [6720/60000 (11%)]\t training loss: 0.163824\n",
      "epoch: 3 [7040/60000 (12%)]\t training loss: 0.026460\n",
      "epoch: 3 [7360/60000 (12%)]\t training loss: 0.054186\n",
      "epoch: 3 [7680/60000 (13%)]\t training loss: 0.018192\n",
      "epoch: 3 [8000/60000 (13%)]\t training loss: 0.000815\n",
      "epoch: 3 [8320/60000 (14%)]\t training loss: 0.006644\n",
      "epoch: 3 [8640/60000 (14%)]\t training loss: 0.091548\n",
      "epoch: 3 [8960/60000 (15%)]\t training loss: 0.001785\n",
      "epoch: 3 [9280/60000 (15%)]\t training loss: 0.109807\n",
      "epoch: 3 [9600/60000 (16%)]\t training loss: 0.007066\n",
      "epoch: 3 [9920/60000 (17%)]\t training loss: 0.152273\n",
      "epoch: 3 [10240/60000 (17%)]\t training loss: 0.026270\n",
      "epoch: 3 [10560/60000 (18%)]\t training loss: 0.013815\n",
      "epoch: 3 [10880/60000 (18%)]\t training loss: 0.068917\n",
      "epoch: 3 [11200/60000 (19%)]\t training loss: 0.137066\n",
      "epoch: 3 [11520/60000 (19%)]\t training loss: 0.102618\n",
      "epoch: 3 [11840/60000 (20%)]\t training loss: 0.201293\n",
      "epoch: 3 [12160/60000 (20%)]\t training loss: 0.021606\n",
      "epoch: 3 [12480/60000 (21%)]\t training loss: 0.005770\n",
      "epoch: 3 [12800/60000 (21%)]\t training loss: 0.018102\n",
      "epoch: 3 [13120/60000 (22%)]\t training loss: 0.006232\n",
      "epoch: 3 [13440/60000 (22%)]\t training loss: 0.010599\n",
      "epoch: 3 [13760/60000 (23%)]\t training loss: 0.257019\n",
      "epoch: 3 [14080/60000 (23%)]\t training loss: 0.000853\n",
      "epoch: 3 [14400/60000 (24%)]\t training loss: 0.012942\n",
      "epoch: 3 [14720/60000 (25%)]\t training loss: 0.014272\n",
      "epoch: 3 [15040/60000 (25%)]\t training loss: 0.026997\n",
      "epoch: 3 [15360/60000 (26%)]\t training loss: 0.013795\n",
      "epoch: 3 [15680/60000 (26%)]\t training loss: 0.013397\n",
      "epoch: 3 [16000/60000 (27%)]\t training loss: 0.000748\n",
      "epoch: 3 [16320/60000 (27%)]\t training loss: 0.006821\n",
      "epoch: 3 [16640/60000 (28%)]\t training loss: 0.046557\n",
      "epoch: 3 [16960/60000 (28%)]\t training loss: 0.007804\n",
      "epoch: 3 [17280/60000 (29%)]\t training loss: 0.028838\n",
      "epoch: 3 [17600/60000 (29%)]\t training loss: 0.006441\n",
      "epoch: 3 [17920/60000 (30%)]\t training loss: 0.084011\n",
      "epoch: 3 [18240/60000 (30%)]\t training loss: 0.037582\n",
      "epoch: 3 [18560/60000 (31%)]\t training loss: 0.037977\n",
      "epoch: 3 [18880/60000 (31%)]\t training loss: 0.067436\n",
      "epoch: 3 [19200/60000 (32%)]\t training loss: 0.040391\n",
      "epoch: 3 [19520/60000 (33%)]\t training loss: 0.174864\n",
      "epoch: 3 [19840/60000 (33%)]\t training loss: 0.021367\n",
      "epoch: 3 [20160/60000 (34%)]\t training loss: 0.099070\n",
      "epoch: 3 [20480/60000 (34%)]\t training loss: 0.001456\n",
      "epoch: 3 [20800/60000 (35%)]\t training loss: 0.004306\n",
      "epoch: 3 [21120/60000 (35%)]\t training loss: 0.175876\n",
      "epoch: 3 [21440/60000 (36%)]\t training loss: 0.025332\n",
      "epoch: 3 [21760/60000 (36%)]\t training loss: 0.258571\n",
      "epoch: 3 [22080/60000 (37%)]\t training loss: 0.002933\n",
      "epoch: 3 [22400/60000 (37%)]\t training loss: 0.016720\n",
      "epoch: 3 [22720/60000 (38%)]\t training loss: 0.023869\n",
      "epoch: 3 [23040/60000 (38%)]\t training loss: 0.001386\n",
      "epoch: 3 [23360/60000 (39%)]\t training loss: 0.097420\n",
      "epoch: 3 [23680/60000 (39%)]\t training loss: 0.015469\n",
      "epoch: 3 [24000/60000 (40%)]\t training loss: 0.076215\n",
      "epoch: 3 [24320/60000 (41%)]\t training loss: 0.059975\n",
      "epoch: 3 [24640/60000 (41%)]\t training loss: 0.003338\n",
      "epoch: 3 [24960/60000 (42%)]\t training loss: 0.177915\n",
      "epoch: 3 [25280/60000 (42%)]\t training loss: 0.046249\n",
      "epoch: 3 [25600/60000 (43%)]\t training loss: 0.002349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 [25920/60000 (43%)]\t training loss: 0.021503\n",
      "epoch: 3 [26240/60000 (44%)]\t training loss: 0.097679\n",
      "epoch: 3 [26560/60000 (44%)]\t training loss: 0.053034\n",
      "epoch: 3 [26880/60000 (45%)]\t training loss: 0.020304\n",
      "epoch: 3 [27200/60000 (45%)]\t training loss: 0.166495\n",
      "epoch: 3 [27520/60000 (46%)]\t training loss: 0.006343\n",
      "epoch: 3 [27840/60000 (46%)]\t training loss: 0.006085\n",
      "epoch: 3 [28160/60000 (47%)]\t training loss: 0.042773\n",
      "epoch: 3 [28480/60000 (47%)]\t training loss: 0.018266\n",
      "epoch: 3 [28800/60000 (48%)]\t training loss: 0.007297\n",
      "epoch: 3 [29120/60000 (49%)]\t training loss: 0.076372\n",
      "epoch: 3 [29440/60000 (49%)]\t training loss: 0.003285\n",
      "epoch: 3 [29760/60000 (50%)]\t training loss: 0.008646\n",
      "epoch: 3 [30080/60000 (50%)]\t training loss: 0.115879\n",
      "epoch: 3 [30400/60000 (51%)]\t training loss: 0.027911\n",
      "epoch: 3 [30720/60000 (51%)]\t training loss: 0.032725\n",
      "epoch: 3 [31040/60000 (52%)]\t training loss: 0.006361\n",
      "epoch: 3 [31360/60000 (52%)]\t training loss: 0.019331\n",
      "epoch: 3 [31680/60000 (53%)]\t training loss: 0.009630\n",
      "epoch: 3 [32000/60000 (53%)]\t training loss: 0.146616\n",
      "epoch: 3 [32320/60000 (54%)]\t training loss: 0.063275\n",
      "epoch: 3 [32640/60000 (54%)]\t training loss: 0.055103\n",
      "epoch: 3 [32960/60000 (55%)]\t training loss: 0.069110\n",
      "epoch: 3 [33280/60000 (55%)]\t training loss: 0.002696\n",
      "epoch: 3 [33600/60000 (56%)]\t training loss: 0.021789\n",
      "epoch: 3 [33920/60000 (57%)]\t training loss: 0.048828\n",
      "epoch: 3 [34240/60000 (57%)]\t training loss: 0.034447\n",
      "epoch: 3 [34560/60000 (58%)]\t training loss: 0.079262\n",
      "epoch: 3 [34880/60000 (58%)]\t training loss: 0.234765\n",
      "epoch: 3 [35200/60000 (59%)]\t training loss: 0.117930\n",
      "epoch: 3 [35520/60000 (59%)]\t training loss: 0.002661\n",
      "epoch: 3 [35840/60000 (60%)]\t training loss: 0.127541\n",
      "epoch: 3 [36160/60000 (60%)]\t training loss: 0.004609\n",
      "epoch: 3 [36480/60000 (61%)]\t training loss: 0.002920\n",
      "epoch: 3 [36800/60000 (61%)]\t training loss: 0.005417\n",
      "epoch: 3 [37120/60000 (62%)]\t training loss: 0.236478\n",
      "epoch: 3 [37440/60000 (62%)]\t training loss: 0.006361\n",
      "epoch: 3 [37760/60000 (63%)]\t training loss: 0.266753\n",
      "epoch: 3 [38080/60000 (63%)]\t training loss: 0.115547\n",
      "epoch: 3 [38400/60000 (64%)]\t training loss: 0.102363\n",
      "epoch: 3 [38720/60000 (65%)]\t training loss: 0.001987\n",
      "epoch: 3 [39040/60000 (65%)]\t training loss: 0.004229\n",
      "epoch: 3 [39360/60000 (66%)]\t training loss: 0.002499\n",
      "epoch: 3 [39680/60000 (66%)]\t training loss: 0.065925\n",
      "epoch: 3 [40000/60000 (67%)]\t training loss: 0.161164\n",
      "epoch: 3 [40320/60000 (67%)]\t training loss: 0.004448\n",
      "epoch: 3 [40640/60000 (68%)]\t training loss: 0.019049\n",
      "epoch: 3 [40960/60000 (68%)]\t training loss: 0.168487\n",
      "epoch: 3 [41280/60000 (69%)]\t training loss: 0.000319\n",
      "epoch: 3 [41600/60000 (69%)]\t training loss: 0.001118\n",
      "epoch: 3 [41920/60000 (70%)]\t training loss: 0.006189\n",
      "epoch: 3 [42240/60000 (70%)]\t training loss: 0.024215\n",
      "epoch: 3 [42560/60000 (71%)]\t training loss: 0.001148\n",
      "epoch: 3 [42880/60000 (71%)]\t training loss: 0.011071\n",
      "epoch: 3 [43200/60000 (72%)]\t training loss: 0.041499\n",
      "epoch: 3 [43520/60000 (73%)]\t training loss: 0.010558\n",
      "epoch: 3 [43840/60000 (73%)]\t training loss: 0.219584\n",
      "epoch: 3 [44160/60000 (74%)]\t training loss: 0.005443\n",
      "epoch: 3 [44480/60000 (74%)]\t training loss: 0.015918\n",
      "epoch: 3 [44800/60000 (75%)]\t training loss: 0.190327\n",
      "epoch: 3 [45120/60000 (75%)]\t training loss: 0.004152\n",
      "epoch: 3 [45440/60000 (76%)]\t training loss: 0.016173\n",
      "epoch: 3 [45760/60000 (76%)]\t training loss: 0.139343\n",
      "epoch: 3 [46080/60000 (77%)]\t training loss: 0.010434\n",
      "epoch: 3 [46400/60000 (77%)]\t training loss: 0.034104\n",
      "epoch: 3 [46720/60000 (78%)]\t training loss: 0.497432\n",
      "epoch: 3 [47040/60000 (78%)]\t training loss: 0.039089\n",
      "epoch: 3 [47360/60000 (79%)]\t training loss: 0.242075\n",
      "epoch: 3 [47680/60000 (79%)]\t training loss: 0.046647\n",
      "epoch: 3 [48000/60000 (80%)]\t training loss: 0.092914\n",
      "epoch: 3 [48320/60000 (81%)]\t training loss: 0.053166\n",
      "epoch: 3 [48640/60000 (81%)]\t training loss: 0.073610\n",
      "epoch: 3 [48960/60000 (82%)]\t training loss: 0.220203\n",
      "epoch: 3 [49280/60000 (82%)]\t training loss: 0.242567\n",
      "epoch: 3 [49600/60000 (83%)]\t training loss: 0.017333\n",
      "epoch: 3 [49920/60000 (83%)]\t training loss: 0.178944\n",
      "epoch: 3 [50240/60000 (84%)]\t training loss: 0.004475\n",
      "epoch: 3 [50560/60000 (84%)]\t training loss: 0.064826\n",
      "epoch: 3 [50880/60000 (85%)]\t training loss: 0.015670\n",
      "epoch: 3 [51200/60000 (85%)]\t training loss: 0.005221\n",
      "epoch: 3 [51520/60000 (86%)]\t training loss: 0.393210\n",
      "epoch: 3 [51840/60000 (86%)]\t training loss: 0.017660\n",
      "epoch: 3 [52160/60000 (87%)]\t training loss: 0.029596\n",
      "epoch: 3 [52480/60000 (87%)]\t training loss: 0.073161\n",
      "epoch: 3 [52800/60000 (88%)]\t training loss: 0.005333\n",
      "epoch: 3 [53120/60000 (89%)]\t training loss: 0.036066\n",
      "epoch: 3 [53440/60000 (89%)]\t training loss: 0.007379\n",
      "epoch: 3 [53760/60000 (90%)]\t training loss: 0.279576\n",
      "epoch: 3 [54080/60000 (90%)]\t training loss: 0.003683\n",
      "epoch: 3 [54400/60000 (91%)]\t training loss: 0.481151\n",
      "epoch: 3 [54720/60000 (91%)]\t training loss: 0.001906\n",
      "epoch: 3 [55040/60000 (92%)]\t training loss: 0.186858\n",
      "epoch: 3 [55360/60000 (92%)]\t training loss: 0.010283\n",
      "epoch: 3 [55680/60000 (93%)]\t training loss: 0.016110\n",
      "epoch: 3 [56000/60000 (93%)]\t training loss: 0.017635\n",
      "epoch: 3 [56320/60000 (94%)]\t training loss: 0.019956\n",
      "epoch: 3 [56640/60000 (94%)]\t training loss: 0.005368\n",
      "epoch: 3 [56960/60000 (95%)]\t training loss: 0.011763\n",
      "epoch: 3 [57280/60000 (95%)]\t training loss: 0.041573\n",
      "epoch: 3 [57600/60000 (96%)]\t training loss: 0.008222\n",
      "epoch: 3 [57920/60000 (97%)]\t training loss: 0.079517\n",
      "epoch: 3 [58240/60000 (97%)]\t training loss: 0.008409\n",
      "epoch: 3 [58560/60000 (98%)]\t training loss: 0.001809\n",
      "epoch: 3 [58880/60000 (98%)]\t training loss: 0.004878\n",
      "epoch: 3 [59200/60000 (99%)]\t training loss: 0.001063\n",
      "epoch: 3 [59520/60000 (99%)]\t training loss: 0.005337\n",
      "epoch: 3 [59840/60000 (100%)]\t training loss: 0.138211\n",
      "\n",
      "Test dataset: Overall Loss: 0.0342, Overall Accuracy: 9889/10000 (99%)\n",
      "\n",
      "epoch: 4 [0/60000 (0%)]\t training loss: 0.019890\n",
      "epoch: 4 [320/60000 (1%)]\t training loss: 0.011072\n",
      "epoch: 4 [640/60000 (1%)]\t training loss: 0.036847\n",
      "epoch: 4 [960/60000 (2%)]\t training loss: 0.002691\n",
      "epoch: 4 [1280/60000 (2%)]\t training loss: 0.001192\n",
      "epoch: 4 [1600/60000 (3%)]\t training loss: 0.121183\n",
      "epoch: 4 [1920/60000 (3%)]\t training loss: 0.022460\n",
      "epoch: 4 [2240/60000 (4%)]\t training loss: 0.008592\n",
      "epoch: 4 [2560/60000 (4%)]\t training loss: 0.097591\n",
      "epoch: 4 [2880/60000 (5%)]\t training loss: 0.060041\n",
      "epoch: 4 [3200/60000 (5%)]\t training loss: 0.050574\n",
      "epoch: 4 [3520/60000 (6%)]\t training loss: 0.087786\n",
      "epoch: 4 [3840/60000 (6%)]\t training loss: 0.001536\n",
      "epoch: 4 [4160/60000 (7%)]\t training loss: 0.000891\n",
      "epoch: 4 [4480/60000 (7%)]\t training loss: 0.020300\n",
      "epoch: 4 [4800/60000 (8%)]\t training loss: 0.001249\n",
      "epoch: 4 [5120/60000 (9%)]\t training loss: 0.019393\n",
      "epoch: 4 [5440/60000 (9%)]\t training loss: 0.121416\n",
      "epoch: 4 [5760/60000 (10%)]\t training loss: 0.000776\n",
      "epoch: 4 [6080/60000 (10%)]\t training loss: 0.005417\n",
      "epoch: 4 [6400/60000 (11%)]\t training loss: 0.016959\n",
      "epoch: 4 [6720/60000 (11%)]\t training loss: 0.053219\n",
      "epoch: 4 [7040/60000 (12%)]\t training loss: 0.187523\n",
      "epoch: 4 [7360/60000 (12%)]\t training loss: 0.126891\n",
      "epoch: 4 [7680/60000 (13%)]\t training loss: 0.026094\n",
      "epoch: 4 [8000/60000 (13%)]\t training loss: 0.006266\n",
      "epoch: 4 [8320/60000 (14%)]\t training loss: 0.081337\n",
      "epoch: 4 [8640/60000 (14%)]\t training loss: 0.008565\n",
      "epoch: 4 [8960/60000 (15%)]\t training loss: 0.033516\n",
      "epoch: 4 [9280/60000 (15%)]\t training loss: 0.207415\n",
      "epoch: 4 [9600/60000 (16%)]\t training loss: 0.158209\n",
      "epoch: 4 [9920/60000 (17%)]\t training loss: 0.025753\n",
      "epoch: 4 [10240/60000 (17%)]\t training loss: 0.058756\n",
      "epoch: 4 [10560/60000 (18%)]\t training loss: 0.077759\n",
      "epoch: 4 [10880/60000 (18%)]\t training loss: 0.000618\n",
      "epoch: 4 [11200/60000 (19%)]\t training loss: 0.007460\n",
      "epoch: 4 [11520/60000 (19%)]\t training loss: 0.001139\n",
      "epoch: 4 [11840/60000 (20%)]\t training loss: 0.018146\n",
      "epoch: 4 [12160/60000 (20%)]\t training loss: 0.002777\n",
      "epoch: 4 [12480/60000 (21%)]\t training loss: 0.035835\n",
      "epoch: 4 [12800/60000 (21%)]\t training loss: 0.074958\n",
      "epoch: 4 [13120/60000 (22%)]\t training loss: 0.037626\n",
      "epoch: 4 [13440/60000 (22%)]\t training loss: 0.003598\n",
      "epoch: 4 [13760/60000 (23%)]\t training loss: 0.024558\n",
      "epoch: 4 [14080/60000 (23%)]\t training loss: 0.024758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 [14400/60000 (24%)]\t training loss: 0.011734\n",
      "epoch: 4 [14720/60000 (25%)]\t training loss: 0.001433\n",
      "epoch: 4 [15040/60000 (25%)]\t training loss: 0.042009\n",
      "epoch: 4 [15360/60000 (26%)]\t training loss: 0.001167\n",
      "epoch: 4 [15680/60000 (26%)]\t training loss: 0.007411\n",
      "epoch: 4 [16000/60000 (27%)]\t training loss: 0.176279\n",
      "epoch: 4 [16320/60000 (27%)]\t training loss: 0.119038\n",
      "epoch: 4 [16640/60000 (28%)]\t training loss: 0.001750\n",
      "epoch: 4 [16960/60000 (28%)]\t training loss: 0.002272\n",
      "epoch: 4 [17280/60000 (29%)]\t training loss: 0.111455\n",
      "epoch: 4 [17600/60000 (29%)]\t training loss: 0.020838\n",
      "epoch: 4 [17920/60000 (30%)]\t training loss: 0.047867\n",
      "epoch: 4 [18240/60000 (30%)]\t training loss: 0.007386\n",
      "epoch: 4 [18560/60000 (31%)]\t training loss: 0.002484\n",
      "epoch: 4 [18880/60000 (31%)]\t training loss: 0.004090\n",
      "epoch: 4 [19200/60000 (32%)]\t training loss: 0.004576\n",
      "epoch: 4 [19520/60000 (33%)]\t training loss: 0.020260\n",
      "epoch: 4 [19840/60000 (33%)]\t training loss: 0.002878\n",
      "epoch: 4 [20160/60000 (34%)]\t training loss: 0.122992\n",
      "epoch: 4 [20480/60000 (34%)]\t training loss: 0.143038\n",
      "epoch: 4 [20800/60000 (35%)]\t training loss: 0.003682\n",
      "epoch: 4 [21120/60000 (35%)]\t training loss: 0.001727\n",
      "epoch: 4 [21440/60000 (36%)]\t training loss: 0.002261\n",
      "epoch: 4 [21760/60000 (36%)]\t training loss: 0.105641\n",
      "epoch: 4 [22080/60000 (37%)]\t training loss: 0.337045\n",
      "epoch: 4 [22400/60000 (37%)]\t training loss: 0.008832\n",
      "epoch: 4 [22720/60000 (38%)]\t training loss: 0.073169\n",
      "epoch: 4 [23040/60000 (38%)]\t training loss: 0.233129\n",
      "epoch: 4 [23360/60000 (39%)]\t training loss: 0.057752\n",
      "epoch: 4 [23680/60000 (39%)]\t training loss: 0.050691\n",
      "epoch: 4 [24000/60000 (40%)]\t training loss: 0.005587\n",
      "epoch: 4 [24320/60000 (41%)]\t training loss: 0.090421\n",
      "epoch: 4 [24640/60000 (41%)]\t training loss: 0.041051\n",
      "epoch: 4 [24960/60000 (42%)]\t training loss: 0.195957\n",
      "epoch: 4 [25280/60000 (42%)]\t training loss: 0.053578\n",
      "epoch: 4 [25600/60000 (43%)]\t training loss: 0.023659\n",
      "epoch: 4 [25920/60000 (43%)]\t training loss: 0.021057\n",
      "epoch: 4 [26240/60000 (44%)]\t training loss: 0.001078\n",
      "epoch: 4 [26560/60000 (44%)]\t training loss: 0.009551\n",
      "epoch: 4 [26880/60000 (45%)]\t training loss: 0.007034\n",
      "epoch: 4 [27200/60000 (45%)]\t training loss: 0.018451\n",
      "epoch: 4 [27520/60000 (46%)]\t training loss: 0.224514\n",
      "epoch: 4 [27840/60000 (46%)]\t training loss: 0.004859\n",
      "epoch: 4 [28160/60000 (47%)]\t training loss: 0.023759\n",
      "epoch: 4 [28480/60000 (47%)]\t training loss: 0.005166\n",
      "epoch: 4 [28800/60000 (48%)]\t training loss: 0.020730\n",
      "epoch: 4 [29120/60000 (49%)]\t training loss: 0.010426\n",
      "epoch: 4 [29440/60000 (49%)]\t training loss: 0.000872\n",
      "epoch: 4 [29760/60000 (50%)]\t training loss: 0.043681\n",
      "epoch: 4 [30080/60000 (50%)]\t training loss: 0.004866\n",
      "epoch: 4 [30400/60000 (51%)]\t training loss: 0.001789\n",
      "epoch: 4 [30720/60000 (51%)]\t training loss: 0.082513\n",
      "epoch: 4 [31040/60000 (52%)]\t training loss: 0.003776\n",
      "epoch: 4 [31360/60000 (52%)]\t training loss: 0.000976\n",
      "epoch: 4 [31680/60000 (53%)]\t training loss: 0.020105\n",
      "epoch: 4 [32000/60000 (53%)]\t training loss: 0.131463\n",
      "epoch: 4 [32320/60000 (54%)]\t training loss: 0.010587\n",
      "epoch: 4 [32640/60000 (54%)]\t training loss: 0.074564\n",
      "epoch: 4 [32960/60000 (55%)]\t training loss: 0.064196\n",
      "epoch: 4 [33280/60000 (55%)]\t training loss: 0.005270\n",
      "epoch: 4 [33600/60000 (56%)]\t training loss: 0.025890\n",
      "epoch: 4 [33920/60000 (57%)]\t training loss: 0.164326\n",
      "epoch: 4 [34240/60000 (57%)]\t training loss: 0.002016\n",
      "epoch: 4 [34560/60000 (58%)]\t training loss: 0.003051\n",
      "epoch: 4 [34880/60000 (58%)]\t training loss: 0.053834\n",
      "epoch: 4 [35200/60000 (59%)]\t training loss: 0.002810\n",
      "epoch: 4 [35520/60000 (59%)]\t training loss: 0.123644\n",
      "epoch: 4 [35840/60000 (60%)]\t training loss: 0.016670\n",
      "epoch: 4 [36160/60000 (60%)]\t training loss: 0.002461\n",
      "epoch: 4 [36480/60000 (61%)]\t training loss: 0.001109\n",
      "epoch: 4 [36800/60000 (61%)]\t training loss: 0.099978\n",
      "epoch: 4 [37120/60000 (62%)]\t training loss: 0.089733\n",
      "epoch: 4 [37440/60000 (62%)]\t training loss: 0.016327\n",
      "epoch: 4 [37760/60000 (63%)]\t training loss: 0.159536\n",
      "epoch: 4 [38080/60000 (63%)]\t training loss: 0.002891\n",
      "epoch: 4 [38400/60000 (64%)]\t training loss: 0.085696\n",
      "epoch: 4 [38720/60000 (65%)]\t training loss: 0.001195\n",
      "epoch: 4 [39040/60000 (65%)]\t training loss: 0.030467\n",
      "epoch: 4 [39360/60000 (66%)]\t training loss: 0.020789\n",
      "epoch: 4 [39680/60000 (66%)]\t training loss: 0.042641\n",
      "epoch: 4 [40000/60000 (67%)]\t training loss: 0.001252\n",
      "epoch: 4 [40320/60000 (67%)]\t training loss: 0.014745\n",
      "epoch: 4 [40640/60000 (68%)]\t training loss: 0.125610\n",
      "epoch: 4 [40960/60000 (68%)]\t training loss: 0.186314\n",
      "epoch: 4 [41280/60000 (69%)]\t training loss: 0.017316\n",
      "epoch: 4 [41600/60000 (69%)]\t training loss: 0.030860\n",
      "epoch: 4 [41920/60000 (70%)]\t training loss: 0.022662\n",
      "epoch: 4 [42240/60000 (70%)]\t training loss: 0.064298\n",
      "epoch: 4 [42560/60000 (71%)]\t training loss: 0.029519\n",
      "epoch: 4 [42880/60000 (71%)]\t training loss: 0.000361\n",
      "epoch: 4 [43200/60000 (72%)]\t training loss: 0.003574\n",
      "epoch: 4 [43520/60000 (73%)]\t training loss: 0.024852\n",
      "epoch: 4 [43840/60000 (73%)]\t training loss: 0.002172\n",
      "epoch: 4 [44160/60000 (74%)]\t training loss: 0.006223\n",
      "epoch: 4 [44480/60000 (74%)]\t training loss: 0.106974\n",
      "epoch: 4 [44800/60000 (75%)]\t training loss: 0.221018\n",
      "epoch: 4 [45120/60000 (75%)]\t training loss: 0.020612\n",
      "epoch: 4 [45440/60000 (76%)]\t training loss: 0.024998\n",
      "epoch: 4 [45760/60000 (76%)]\t training loss: 0.055251\n",
      "epoch: 4 [46080/60000 (77%)]\t training loss: 0.189777\n",
      "epoch: 4 [46400/60000 (77%)]\t training loss: 0.010428\n",
      "epoch: 4 [46720/60000 (78%)]\t training loss: 0.001616\n",
      "epoch: 4 [47040/60000 (78%)]\t training loss: 0.003542\n",
      "epoch: 4 [47360/60000 (79%)]\t training loss: 0.015635\n",
      "epoch: 4 [47680/60000 (79%)]\t training loss: 0.121481\n",
      "epoch: 4 [48000/60000 (80%)]\t training loss: 0.022402\n",
      "epoch: 4 [48320/60000 (81%)]\t training loss: 0.007585\n",
      "epoch: 4 [48640/60000 (81%)]\t training loss: 0.000220\n",
      "epoch: 4 [48960/60000 (82%)]\t training loss: 0.201572\n",
      "epoch: 4 [49280/60000 (82%)]\t training loss: 0.130116\n",
      "epoch: 4 [49600/60000 (83%)]\t training loss: 0.087992\n",
      "epoch: 4 [49920/60000 (83%)]\t training loss: 0.003716\n",
      "epoch: 4 [50240/60000 (84%)]\t training loss: 0.015626\n",
      "epoch: 4 [50560/60000 (84%)]\t training loss: 0.497670\n",
      "epoch: 4 [50880/60000 (85%)]\t training loss: 0.025449\n",
      "epoch: 4 [51200/60000 (85%)]\t training loss: 0.144465\n",
      "epoch: 4 [51520/60000 (86%)]\t training loss: 0.171884\n",
      "epoch: 4 [51840/60000 (86%)]\t training loss: 0.026257\n",
      "epoch: 4 [52160/60000 (87%)]\t training loss: 0.002622\n",
      "epoch: 4 [52480/60000 (87%)]\t training loss: 0.001581\n",
      "epoch: 4 [52800/60000 (88%)]\t training loss: 0.049711\n",
      "epoch: 4 [53120/60000 (89%)]\t training loss: 0.004510\n",
      "epoch: 4 [53440/60000 (89%)]\t training loss: 0.006326\n",
      "epoch: 4 [53760/60000 (90%)]\t training loss: 0.000189\n",
      "epoch: 4 [54080/60000 (90%)]\t training loss: 0.162842\n",
      "epoch: 4 [54400/60000 (91%)]\t training loss: 0.007811\n",
      "epoch: 4 [54720/60000 (91%)]\t training loss: 0.004246\n",
      "epoch: 4 [55040/60000 (92%)]\t training loss: 0.065358\n",
      "epoch: 4 [55360/60000 (92%)]\t training loss: 0.009275\n",
      "epoch: 4 [55680/60000 (93%)]\t training loss: 0.095165\n",
      "epoch: 4 [56000/60000 (93%)]\t training loss: 0.142592\n",
      "epoch: 4 [56320/60000 (94%)]\t training loss: 0.060880\n",
      "epoch: 4 [56640/60000 (94%)]\t training loss: 0.056176\n",
      "epoch: 4 [56960/60000 (95%)]\t training loss: 0.032998\n",
      "epoch: 4 [57280/60000 (95%)]\t training loss: 0.006997\n",
      "epoch: 4 [57600/60000 (96%)]\t training loss: 0.024188\n",
      "epoch: 4 [57920/60000 (97%)]\t training loss: 0.001012\n",
      "epoch: 4 [58240/60000 (97%)]\t training loss: 0.021020\n",
      "epoch: 4 [58560/60000 (98%)]\t training loss: 0.071421\n",
      "epoch: 4 [58880/60000 (98%)]\t training loss: 0.010032\n",
      "epoch: 4 [59200/60000 (99%)]\t training loss: 0.028211\n",
      "epoch: 4 [59520/60000 (99%)]\t training loss: 0.101681\n",
      "epoch: 4 [59840/60000 (100%)]\t training loss: 0.094863\n",
      "\n",
      "Test dataset: Overall Loss: 0.0322, Overall Accuracy: 9890/10000 (99%)\n",
      "\n",
      "epoch: 5 [0/60000 (0%)]\t training loss: 0.026842\n",
      "epoch: 5 [320/60000 (1%)]\t training loss: 0.007709\n",
      "epoch: 5 [640/60000 (1%)]\t training loss: 0.038154\n",
      "epoch: 5 [960/60000 (2%)]\t training loss: 0.005133\n",
      "epoch: 5 [1280/60000 (2%)]\t training loss: 0.005514\n",
      "epoch: 5 [1600/60000 (3%)]\t training loss: 0.000652\n",
      "epoch: 5 [1920/60000 (3%)]\t training loss: 0.000745\n",
      "epoch: 5 [2240/60000 (4%)]\t training loss: 0.032082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 [2560/60000 (4%)]\t training loss: 0.000581\n",
      "epoch: 5 [2880/60000 (5%)]\t training loss: 0.006715\n",
      "epoch: 5 [3200/60000 (5%)]\t training loss: 0.003614\n",
      "epoch: 5 [3520/60000 (6%)]\t training loss: 0.030546\n",
      "epoch: 5 [3840/60000 (6%)]\t training loss: 0.040006\n",
      "epoch: 5 [4160/60000 (7%)]\t training loss: 0.018933\n",
      "epoch: 5 [4480/60000 (7%)]\t training loss: 0.005410\n",
      "epoch: 5 [4800/60000 (8%)]\t training loss: 0.062363\n",
      "epoch: 5 [5120/60000 (9%)]\t training loss: 0.005770\n",
      "epoch: 5 [5440/60000 (9%)]\t training loss: 0.025936\n",
      "epoch: 5 [5760/60000 (10%)]\t training loss: 0.038276\n",
      "epoch: 5 [6080/60000 (10%)]\t training loss: 0.024754\n",
      "epoch: 5 [6400/60000 (11%)]\t training loss: 0.017555\n",
      "epoch: 5 [6720/60000 (11%)]\t training loss: 0.003855\n",
      "epoch: 5 [7040/60000 (12%)]\t training loss: 0.004632\n",
      "epoch: 5 [7360/60000 (12%)]\t training loss: 0.115628\n",
      "epoch: 5 [7680/60000 (13%)]\t training loss: 0.490891\n",
      "epoch: 5 [8000/60000 (13%)]\t training loss: 0.000761\n",
      "epoch: 5 [8320/60000 (14%)]\t training loss: 0.019319\n",
      "epoch: 5 [8640/60000 (14%)]\t training loss: 0.009221\n",
      "epoch: 5 [8960/60000 (15%)]\t training loss: 0.020154\n",
      "epoch: 5 [9280/60000 (15%)]\t training loss: 0.132313\n",
      "epoch: 5 [9600/60000 (16%)]\t training loss: 0.001245\n",
      "epoch: 5 [9920/60000 (17%)]\t training loss: 0.028934\n",
      "epoch: 5 [10240/60000 (17%)]\t training loss: 0.003518\n",
      "epoch: 5 [10560/60000 (18%)]\t training loss: 0.010222\n",
      "epoch: 5 [10880/60000 (18%)]\t training loss: 0.111007\n",
      "epoch: 5 [11200/60000 (19%)]\t training loss: 0.275871\n",
      "epoch: 5 [11520/60000 (19%)]\t training loss: 0.003790\n",
      "epoch: 5 [11840/60000 (20%)]\t training loss: 0.004482\n",
      "epoch: 5 [12160/60000 (20%)]\t training loss: 0.003467\n",
      "epoch: 5 [12480/60000 (21%)]\t training loss: 0.323556\n",
      "epoch: 5 [12800/60000 (21%)]\t training loss: 0.003557\n",
      "epoch: 5 [13120/60000 (22%)]\t training loss: 0.010471\n",
      "epoch: 5 [13440/60000 (22%)]\t training loss: 0.000186\n",
      "epoch: 5 [13760/60000 (23%)]\t training loss: 0.035333\n",
      "epoch: 5 [14080/60000 (23%)]\t training loss: 0.312642\n",
      "epoch: 5 [14400/60000 (24%)]\t training loss: 0.006932\n",
      "epoch: 5 [14720/60000 (25%)]\t training loss: 0.019795\n",
      "epoch: 5 [15040/60000 (25%)]\t training loss: 0.000542\n",
      "epoch: 5 [15360/60000 (26%)]\t training loss: 0.000394\n",
      "epoch: 5 [15680/60000 (26%)]\t training loss: 0.058340\n",
      "epoch: 5 [16000/60000 (27%)]\t training loss: 0.006063\n",
      "epoch: 5 [16320/60000 (27%)]\t training loss: 0.040436\n",
      "epoch: 5 [16640/60000 (28%)]\t training loss: 0.011824\n",
      "epoch: 5 [16960/60000 (28%)]\t training loss: 0.034657\n",
      "epoch: 5 [17280/60000 (29%)]\t training loss: 0.151367\n",
      "epoch: 5 [17600/60000 (29%)]\t training loss: 0.001067\n",
      "epoch: 5 [17920/60000 (30%)]\t training loss: 0.000355\n",
      "epoch: 5 [18240/60000 (30%)]\t training loss: 0.003053\n",
      "epoch: 5 [18560/60000 (31%)]\t training loss: 0.014246\n",
      "epoch: 5 [18880/60000 (31%)]\t training loss: 0.093419\n",
      "epoch: 5 [19200/60000 (32%)]\t training loss: 0.002757\n",
      "epoch: 5 [19520/60000 (33%)]\t training loss: 0.021103\n",
      "epoch: 5 [19840/60000 (33%)]\t training loss: 0.090720\n",
      "epoch: 5 [20160/60000 (34%)]\t training loss: 0.059289\n",
      "epoch: 5 [20480/60000 (34%)]\t training loss: 0.114236\n",
      "epoch: 5 [20800/60000 (35%)]\t training loss: 0.003240\n",
      "epoch: 5 [21120/60000 (35%)]\t training loss: 0.002118\n",
      "epoch: 5 [21440/60000 (36%)]\t training loss: 0.011218\n",
      "epoch: 5 [21760/60000 (36%)]\t training loss: 0.026477\n",
      "epoch: 5 [22080/60000 (37%)]\t training loss: 0.010041\n",
      "epoch: 5 [22400/60000 (37%)]\t training loss: 0.014226\n",
      "epoch: 5 [22720/60000 (38%)]\t training loss: 0.003518\n",
      "epoch: 5 [23040/60000 (38%)]\t training loss: 0.005748\n",
      "epoch: 5 [23360/60000 (39%)]\t training loss: 0.165962\n",
      "epoch: 5 [23680/60000 (39%)]\t training loss: 0.236992\n",
      "epoch: 5 [24000/60000 (40%)]\t training loss: 0.007167\n",
      "epoch: 5 [24320/60000 (41%)]\t training loss: 0.086423\n",
      "epoch: 5 [24640/60000 (41%)]\t training loss: 0.064144\n",
      "epoch: 5 [24960/60000 (42%)]\t training loss: 0.010694\n",
      "epoch: 5 [25280/60000 (42%)]\t training loss: 0.006867\n",
      "epoch: 5 [25600/60000 (43%)]\t training loss: 0.028021\n",
      "epoch: 5 [25920/60000 (43%)]\t training loss: 0.033642\n",
      "epoch: 5 [26240/60000 (44%)]\t training loss: 0.000916\n",
      "epoch: 5 [26560/60000 (44%)]\t training loss: 0.002150\n",
      "epoch: 5 [26880/60000 (45%)]\t training loss: 0.001388\n",
      "epoch: 5 [27200/60000 (45%)]\t training loss: 0.022380\n",
      "epoch: 5 [27520/60000 (46%)]\t training loss: 0.095445\n",
      "epoch: 5 [27840/60000 (46%)]\t training loss: 0.040887\n",
      "epoch: 5 [28160/60000 (47%)]\t training loss: 0.009030\n",
      "epoch: 5 [28480/60000 (47%)]\t training loss: 0.010039\n",
      "epoch: 5 [28800/60000 (48%)]\t training loss: 0.001487\n",
      "epoch: 5 [29120/60000 (49%)]\t training loss: 0.195701\n",
      "epoch: 5 [29440/60000 (49%)]\t training loss: 0.097754\n",
      "epoch: 5 [29760/60000 (50%)]\t training loss: 0.036704\n",
      "epoch: 5 [30080/60000 (50%)]\t training loss: 0.423067\n",
      "epoch: 5 [30400/60000 (51%)]\t training loss: 0.121096\n",
      "epoch: 5 [30720/60000 (51%)]\t training loss: 0.010696\n",
      "epoch: 5 [31040/60000 (52%)]\t training loss: 0.003793\n",
      "epoch: 5 [31360/60000 (52%)]\t training loss: 0.048965\n",
      "epoch: 5 [31680/60000 (53%)]\t training loss: 0.015396\n",
      "epoch: 5 [32000/60000 (53%)]\t training loss: 0.055159\n",
      "epoch: 5 [32320/60000 (54%)]\t training loss: 0.024873\n",
      "epoch: 5 [32640/60000 (54%)]\t training loss: 0.001543\n",
      "epoch: 5 [32960/60000 (55%)]\t training loss: 0.225226\n",
      "epoch: 5 [33280/60000 (55%)]\t training loss: 0.028573\n",
      "epoch: 5 [33600/60000 (56%)]\t training loss: 0.011410\n",
      "epoch: 5 [33920/60000 (57%)]\t training loss: 0.152035\n",
      "epoch: 5 [34240/60000 (57%)]\t training loss: 0.001175\n",
      "epoch: 5 [34560/60000 (58%)]\t training loss: 0.001894\n",
      "epoch: 5 [34880/60000 (58%)]\t training loss: 0.015678\n",
      "epoch: 5 [35200/60000 (59%)]\t training loss: 0.000358\n",
      "epoch: 5 [35520/60000 (59%)]\t training loss: 0.002152\n",
      "epoch: 5 [35840/60000 (60%)]\t training loss: 0.071014\n",
      "epoch: 5 [36160/60000 (60%)]\t training loss: 0.005884\n",
      "epoch: 5 [36480/60000 (61%)]\t training loss: 0.000968\n",
      "epoch: 5 [36800/60000 (61%)]\t training loss: 0.070728\n",
      "epoch: 5 [37120/60000 (62%)]\t training loss: 0.025595\n",
      "epoch: 5 [37440/60000 (62%)]\t training loss: 0.000442\n",
      "epoch: 5 [37760/60000 (63%)]\t training loss: 0.004365\n",
      "epoch: 5 [38080/60000 (63%)]\t training loss: 0.007326\n",
      "epoch: 5 [38400/60000 (64%)]\t training loss: 0.008709\n",
      "epoch: 5 [38720/60000 (65%)]\t training loss: 0.000436\n",
      "epoch: 5 [39040/60000 (65%)]\t training loss: 0.004318\n",
      "epoch: 5 [39360/60000 (66%)]\t training loss: 0.000044\n",
      "epoch: 5 [39680/60000 (66%)]\t training loss: 0.006720\n",
      "epoch: 5 [40000/60000 (67%)]\t training loss: 0.071574\n",
      "epoch: 5 [40320/60000 (67%)]\t training loss: 0.008718\n",
      "epoch: 5 [40640/60000 (68%)]\t training loss: 0.018792\n",
      "epoch: 5 [40960/60000 (68%)]\t training loss: 0.001633\n",
      "epoch: 5 [41280/60000 (69%)]\t training loss: 0.014028\n",
      "epoch: 5 [41600/60000 (69%)]\t training loss: 0.004575\n",
      "epoch: 5 [41920/60000 (70%)]\t training loss: 0.039744\n",
      "epoch: 5 [42240/60000 (70%)]\t training loss: 0.010330\n",
      "epoch: 5 [42560/60000 (71%)]\t training loss: 0.002543\n",
      "epoch: 5 [42880/60000 (71%)]\t training loss: 0.013015\n",
      "epoch: 5 [43200/60000 (72%)]\t training loss: 0.035408\n",
      "epoch: 5 [43520/60000 (73%)]\t training loss: 0.005855\n",
      "epoch: 5 [43840/60000 (73%)]\t training loss: 0.129620\n",
      "epoch: 5 [44160/60000 (74%)]\t training loss: 0.009402\n",
      "epoch: 5 [44480/60000 (74%)]\t training loss: 0.000708\n",
      "epoch: 5 [44800/60000 (75%)]\t training loss: 0.164654\n",
      "epoch: 5 [45120/60000 (75%)]\t training loss: 0.038796\n",
      "epoch: 5 [45440/60000 (76%)]\t training loss: 0.001712\n",
      "epoch: 5 [45760/60000 (76%)]\t training loss: 0.040049\n",
      "epoch: 5 [46080/60000 (77%)]\t training loss: 0.119460\n",
      "epoch: 5 [46400/60000 (77%)]\t training loss: 0.026577\n",
      "epoch: 5 [46720/60000 (78%)]\t training loss: 0.107276\n",
      "epoch: 5 [47040/60000 (78%)]\t training loss: 0.002368\n",
      "epoch: 5 [47360/60000 (79%)]\t training loss: 0.008205\n",
      "epoch: 5 [47680/60000 (79%)]\t training loss: 0.001051\n",
      "epoch: 5 [48000/60000 (80%)]\t training loss: 0.000963\n",
      "epoch: 5 [48320/60000 (81%)]\t training loss: 0.003908\n",
      "epoch: 5 [48640/60000 (81%)]\t training loss: 0.079811\n",
      "epoch: 5 [48960/60000 (82%)]\t training loss: 0.000530\n",
      "epoch: 5 [49280/60000 (82%)]\t training loss: 0.078301\n",
      "epoch: 5 [49600/60000 (83%)]\t training loss: 0.005392\n",
      "epoch: 5 [49920/60000 (83%)]\t training loss: 0.001773\n",
      "epoch: 5 [50240/60000 (84%)]\t training loss: 0.002489\n",
      "epoch: 5 [50560/60000 (84%)]\t training loss: 0.069022\n",
      "epoch: 5 [50880/60000 (85%)]\t training loss: 0.034211\n",
      "epoch: 5 [51200/60000 (85%)]\t training loss: 0.001973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 [51520/60000 (86%)]\t training loss: 0.002790\n",
      "epoch: 5 [51840/60000 (86%)]\t training loss: 0.001269\n",
      "epoch: 5 [52160/60000 (87%)]\t training loss: 0.108304\n",
      "epoch: 5 [52480/60000 (87%)]\t training loss: 0.042755\n",
      "epoch: 5 [52800/60000 (88%)]\t training loss: 0.055161\n",
      "epoch: 5 [53120/60000 (89%)]\t training loss: 0.018383\n",
      "epoch: 5 [53440/60000 (89%)]\t training loss: 0.047648\n",
      "epoch: 5 [53760/60000 (90%)]\t training loss: 0.003690\n",
      "epoch: 5 [54080/60000 (90%)]\t training loss: 0.118251\n",
      "epoch: 5 [54400/60000 (91%)]\t training loss: 0.033978\n",
      "epoch: 5 [54720/60000 (91%)]\t training loss: 0.005687\n",
      "epoch: 5 [55040/60000 (92%)]\t training loss: 0.001541\n",
      "epoch: 5 [55360/60000 (92%)]\t training loss: 0.014180\n",
      "epoch: 5 [55680/60000 (93%)]\t training loss: 0.092758\n",
      "epoch: 5 [56000/60000 (93%)]\t training loss: 0.043715\n",
      "epoch: 5 [56320/60000 (94%)]\t training loss: 0.021230\n",
      "epoch: 5 [56640/60000 (94%)]\t training loss: 0.002472\n",
      "epoch: 5 [56960/60000 (95%)]\t training loss: 0.005075\n",
      "epoch: 5 [57280/60000 (95%)]\t training loss: 0.000686\n",
      "epoch: 5 [57600/60000 (96%)]\t training loss: 0.054886\n",
      "epoch: 5 [57920/60000 (97%)]\t training loss: 0.001175\n",
      "epoch: 5 [58240/60000 (97%)]\t training loss: 0.374911\n",
      "epoch: 5 [58560/60000 (98%)]\t training loss: 0.008496\n",
      "epoch: 5 [58880/60000 (98%)]\t training loss: 0.074076\n",
      "epoch: 5 [59200/60000 (99%)]\t training loss: 0.059889\n",
      "epoch: 5 [59520/60000 (99%)]\t training loss: 0.003479\n",
      "epoch: 5 [59840/60000 (100%)]\t training loss: 0.002969\n",
      "\n",
      "Test dataset: Overall Loss: 0.0305, Overall Accuracy: 9899/10000 (99%)\n",
      "\n",
      "epoch: 6 [0/60000 (0%)]\t training loss: 0.016904\n",
      "epoch: 6 [320/60000 (1%)]\t training loss: 0.007531\n",
      "epoch: 6 [640/60000 (1%)]\t training loss: 0.004276\n",
      "epoch: 6 [960/60000 (2%)]\t training loss: 0.012791\n",
      "epoch: 6 [1280/60000 (2%)]\t training loss: 0.000173\n",
      "epoch: 6 [1600/60000 (3%)]\t training loss: 0.046686\n",
      "epoch: 6 [1920/60000 (3%)]\t training loss: 0.001336\n",
      "epoch: 6 [2240/60000 (4%)]\t training loss: 0.042766\n",
      "epoch: 6 [2560/60000 (4%)]\t training loss: 0.000584\n",
      "epoch: 6 [2880/60000 (5%)]\t training loss: 0.003912\n",
      "epoch: 6 [3200/60000 (5%)]\t training loss: 0.000301\n",
      "epoch: 6 [3520/60000 (6%)]\t training loss: 0.053519\n",
      "epoch: 6 [3840/60000 (6%)]\t training loss: 0.000134\n",
      "epoch: 6 [4160/60000 (7%)]\t training loss: 0.000352\n",
      "epoch: 6 [4480/60000 (7%)]\t training loss: 0.065163\n",
      "epoch: 6 [4800/60000 (8%)]\t training loss: 0.101273\n",
      "epoch: 6 [5120/60000 (9%)]\t training loss: 0.006189\n",
      "epoch: 6 [5440/60000 (9%)]\t training loss: 0.136466\n",
      "epoch: 6 [5760/60000 (10%)]\t training loss: 0.025244\n",
      "epoch: 6 [6080/60000 (10%)]\t training loss: 0.100065\n",
      "epoch: 6 [6400/60000 (11%)]\t training loss: 0.015984\n",
      "epoch: 6 [6720/60000 (11%)]\t training loss: 0.099014\n",
      "epoch: 6 [7040/60000 (12%)]\t training loss: 0.022177\n",
      "epoch: 6 [7360/60000 (12%)]\t training loss: 0.002027\n",
      "epoch: 6 [7680/60000 (13%)]\t training loss: 0.026285\n",
      "epoch: 6 [8000/60000 (13%)]\t training loss: 0.012380\n",
      "epoch: 6 [8320/60000 (14%)]\t training loss: 0.157678\n",
      "epoch: 6 [8640/60000 (14%)]\t training loss: 0.107801\n",
      "epoch: 6 [8960/60000 (15%)]\t training loss: 0.004198\n",
      "epoch: 6 [9280/60000 (15%)]\t training loss: 0.494281\n",
      "epoch: 6 [9600/60000 (16%)]\t training loss: 0.301445\n",
      "epoch: 6 [9920/60000 (17%)]\t training loss: 0.094707\n",
      "epoch: 6 [10240/60000 (17%)]\t training loss: 0.000580\n",
      "epoch: 6 [10560/60000 (18%)]\t training loss: 0.034755\n",
      "epoch: 6 [10880/60000 (18%)]\t training loss: 0.018278\n",
      "epoch: 6 [11200/60000 (19%)]\t training loss: 0.022895\n",
      "epoch: 6 [11520/60000 (19%)]\t training loss: 0.010788\n",
      "epoch: 6 [11840/60000 (20%)]\t training loss: 0.051134\n",
      "epoch: 6 [12160/60000 (20%)]\t training loss: 0.000452\n",
      "epoch: 6 [12480/60000 (21%)]\t training loss: 0.078582\n",
      "epoch: 6 [12800/60000 (21%)]\t training loss: 0.033601\n",
      "epoch: 6 [13120/60000 (22%)]\t training loss: 0.004813\n",
      "epoch: 6 [13440/60000 (22%)]\t training loss: 0.000814\n",
      "epoch: 6 [13760/60000 (23%)]\t training loss: 0.003818\n",
      "epoch: 6 [14080/60000 (23%)]\t training loss: 0.001833\n",
      "epoch: 6 [14400/60000 (24%)]\t training loss: 0.012946\n",
      "epoch: 6 [14720/60000 (25%)]\t training loss: 0.000708\n",
      "epoch: 6 [15040/60000 (25%)]\t training loss: 0.000499\n",
      "epoch: 6 [15360/60000 (26%)]\t training loss: 0.002597\n",
      "epoch: 6 [15680/60000 (26%)]\t training loss: 0.028753\n",
      "epoch: 6 [16000/60000 (27%)]\t training loss: 0.001506\n",
      "epoch: 6 [16320/60000 (27%)]\t training loss: 0.000527\n",
      "epoch: 6 [16640/60000 (28%)]\t training loss: 0.006114\n",
      "epoch: 6 [16960/60000 (28%)]\t training loss: 0.000518\n",
      "epoch: 6 [17280/60000 (29%)]\t training loss: 0.168756\n",
      "epoch: 6 [17600/60000 (29%)]\t training loss: 0.031002\n",
      "epoch: 6 [17920/60000 (30%)]\t training loss: 0.000329\n",
      "epoch: 6 [18240/60000 (30%)]\t training loss: 0.000500\n",
      "epoch: 6 [18560/60000 (31%)]\t training loss: 0.017593\n",
      "epoch: 6 [18880/60000 (31%)]\t training loss: 0.003078\n",
      "epoch: 6 [19200/60000 (32%)]\t training loss: 0.047807\n",
      "epoch: 6 [19520/60000 (33%)]\t training loss: 0.018489\n",
      "epoch: 6 [19840/60000 (33%)]\t training loss: 0.001142\n",
      "epoch: 6 [20160/60000 (34%)]\t training loss: 0.008967\n",
      "epoch: 6 [20480/60000 (34%)]\t training loss: 0.028573\n",
      "epoch: 6 [20800/60000 (35%)]\t training loss: 0.007400\n",
      "epoch: 6 [21120/60000 (35%)]\t training loss: 0.055450\n",
      "epoch: 6 [21440/60000 (36%)]\t training loss: 0.038764\n",
      "epoch: 6 [21760/60000 (36%)]\t training loss: 0.002006\n",
      "epoch: 6 [22080/60000 (37%)]\t training loss: 0.027170\n",
      "epoch: 6 [22400/60000 (37%)]\t training loss: 0.090336\n",
      "epoch: 6 [22720/60000 (38%)]\t training loss: 0.022201\n",
      "epoch: 6 [23040/60000 (38%)]\t training loss: 0.002418\n",
      "epoch: 6 [23360/60000 (39%)]\t training loss: 0.001368\n",
      "epoch: 6 [23680/60000 (39%)]\t training loss: 0.010847\n",
      "epoch: 6 [24000/60000 (40%)]\t training loss: 0.323929\n",
      "epoch: 6 [24320/60000 (41%)]\t training loss: 0.096982\n",
      "epoch: 6 [24640/60000 (41%)]\t training loss: 0.023675\n",
      "epoch: 6 [24960/60000 (42%)]\t training loss: 0.027534\n",
      "epoch: 6 [25280/60000 (42%)]\t training loss: 0.000563\n",
      "epoch: 6 [25600/60000 (43%)]\t training loss: 0.036150\n",
      "epoch: 6 [25920/60000 (43%)]\t training loss: 0.000098\n",
      "epoch: 6 [26240/60000 (44%)]\t training loss: 0.002363\n",
      "epoch: 6 [26560/60000 (44%)]\t training loss: 0.004674\n",
      "epoch: 6 [26880/60000 (45%)]\t training loss: 0.002290\n",
      "epoch: 6 [27200/60000 (45%)]\t training loss: 0.059104\n",
      "epoch: 6 [27520/60000 (46%)]\t training loss: 0.007175\n",
      "epoch: 6 [27840/60000 (46%)]\t training loss: 0.064778\n",
      "epoch: 6 [28160/60000 (47%)]\t training loss: 0.020494\n",
      "epoch: 6 [28480/60000 (47%)]\t training loss: 0.002047\n",
      "epoch: 6 [28800/60000 (48%)]\t training loss: 0.023479\n",
      "epoch: 6 [29120/60000 (49%)]\t training loss: 0.002034\n",
      "epoch: 6 [29440/60000 (49%)]\t training loss: 0.008300\n",
      "epoch: 6 [29760/60000 (50%)]\t training loss: 0.113237\n",
      "epoch: 6 [30080/60000 (50%)]\t training loss: 0.038835\n",
      "epoch: 6 [30400/60000 (51%)]\t training loss: 0.000245\n",
      "epoch: 6 [30720/60000 (51%)]\t training loss: 0.023661\n",
      "epoch: 6 [31040/60000 (52%)]\t training loss: 0.000164\n",
      "epoch: 6 [31360/60000 (52%)]\t training loss: 0.018896\n",
      "epoch: 6 [31680/60000 (53%)]\t training loss: 0.000043\n",
      "epoch: 6 [32000/60000 (53%)]\t training loss: 0.123055\n",
      "epoch: 6 [32320/60000 (54%)]\t training loss: 0.033542\n",
      "epoch: 6 [32640/60000 (54%)]\t training loss: 0.007236\n",
      "epoch: 6 [32960/60000 (55%)]\t training loss: 0.015920\n",
      "epoch: 6 [33280/60000 (55%)]\t training loss: 0.034189\n",
      "epoch: 6 [33600/60000 (56%)]\t training loss: 0.001467\n",
      "epoch: 6 [33920/60000 (57%)]\t training loss: 0.002006\n",
      "epoch: 6 [34240/60000 (57%)]\t training loss: 0.007957\n",
      "epoch: 6 [34560/60000 (58%)]\t training loss: 0.027465\n",
      "epoch: 6 [34880/60000 (58%)]\t training loss: 0.055229\n",
      "epoch: 6 [35200/60000 (59%)]\t training loss: 0.003850\n",
      "epoch: 6 [35520/60000 (59%)]\t training loss: 0.021741\n",
      "epoch: 6 [35840/60000 (60%)]\t training loss: 0.000677\n",
      "epoch: 6 [36160/60000 (60%)]\t training loss: 0.003613\n",
      "epoch: 6 [36480/60000 (61%)]\t training loss: 0.016983\n",
      "epoch: 6 [36800/60000 (61%)]\t training loss: 0.000980\n",
      "epoch: 6 [37120/60000 (62%)]\t training loss: 0.021349\n",
      "epoch: 6 [37440/60000 (62%)]\t training loss: 0.083460\n",
      "epoch: 6 [37760/60000 (63%)]\t training loss: 0.123211\n",
      "epoch: 6 [38080/60000 (63%)]\t training loss: 0.005091\n",
      "epoch: 6 [38400/60000 (64%)]\t training loss: 0.001026\n",
      "epoch: 6 [38720/60000 (65%)]\t training loss: 0.049054\n",
      "epoch: 6 [39040/60000 (65%)]\t training loss: 0.008378\n",
      "epoch: 6 [39360/60000 (66%)]\t training loss: 0.004442\n",
      "epoch: 6 [39680/60000 (66%)]\t training loss: 0.024999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 [40000/60000 (67%)]\t training loss: 0.000037\n",
      "epoch: 6 [40320/60000 (67%)]\t training loss: 0.003416\n",
      "epoch: 6 [40640/60000 (68%)]\t training loss: 0.002653\n",
      "epoch: 6 [40960/60000 (68%)]\t training loss: 0.017554\n",
      "epoch: 6 [41280/60000 (69%)]\t training loss: 0.001790\n",
      "epoch: 6 [41600/60000 (69%)]\t training loss: 0.054449\n",
      "epoch: 6 [41920/60000 (70%)]\t training loss: 0.077256\n",
      "epoch: 6 [42240/60000 (70%)]\t training loss: 0.001251\n",
      "epoch: 6 [42560/60000 (71%)]\t training loss: 0.007798\n",
      "epoch: 6 [42880/60000 (71%)]\t training loss: 0.066735\n",
      "epoch: 6 [43200/60000 (72%)]\t training loss: 0.000833\n",
      "epoch: 6 [43520/60000 (73%)]\t training loss: 0.001176\n",
      "epoch: 6 [43840/60000 (73%)]\t training loss: 0.084820\n",
      "epoch: 6 [44160/60000 (74%)]\t training loss: 0.027561\n",
      "epoch: 6 [44480/60000 (74%)]\t training loss: 0.032898\n",
      "epoch: 6 [44800/60000 (75%)]\t training loss: 0.008467\n",
      "epoch: 6 [45120/60000 (75%)]\t training loss: 0.035253\n",
      "epoch: 6 [45440/60000 (76%)]\t training loss: 0.033659\n",
      "epoch: 6 [45760/60000 (76%)]\t training loss: 0.239153\n",
      "epoch: 6 [46080/60000 (77%)]\t training loss: 0.013382\n",
      "epoch: 6 [46400/60000 (77%)]\t training loss: 0.002618\n",
      "epoch: 6 [46720/60000 (78%)]\t training loss: 0.053130\n",
      "epoch: 6 [47040/60000 (78%)]\t training loss: 0.003269\n",
      "epoch: 6 [47360/60000 (79%)]\t training loss: 0.000220\n",
      "epoch: 6 [47680/60000 (79%)]\t training loss: 0.000147\n",
      "epoch: 6 [48000/60000 (80%)]\t training loss: 0.007741\n",
      "epoch: 6 [48320/60000 (81%)]\t training loss: 0.004621\n",
      "epoch: 6 [48640/60000 (81%)]\t training loss: 0.000031\n",
      "epoch: 6 [48960/60000 (82%)]\t training loss: 0.006931\n",
      "epoch: 6 [49280/60000 (82%)]\t training loss: 0.029767\n",
      "epoch: 6 [49600/60000 (83%)]\t training loss: 0.045015\n",
      "epoch: 6 [49920/60000 (83%)]\t training loss: 0.051318\n",
      "epoch: 6 [50240/60000 (84%)]\t training loss: 0.001693\n",
      "epoch: 6 [50560/60000 (84%)]\t training loss: 0.153923\n",
      "epoch: 6 [50880/60000 (85%)]\t training loss: 0.003452\n",
      "epoch: 6 [51200/60000 (85%)]\t training loss: 0.002812\n",
      "epoch: 6 [51520/60000 (86%)]\t training loss: 0.003791\n",
      "epoch: 6 [51840/60000 (86%)]\t training loss: 0.010821\n",
      "epoch: 6 [52160/60000 (87%)]\t training loss: 0.004059\n",
      "epoch: 6 [52480/60000 (87%)]\t training loss: 0.110273\n",
      "epoch: 6 [52800/60000 (88%)]\t training loss: 0.193867\n",
      "epoch: 6 [53120/60000 (89%)]\t training loss: 0.131335\n",
      "epoch: 6 [53440/60000 (89%)]\t training loss: 0.013671\n",
      "epoch: 6 [53760/60000 (90%)]\t training loss: 0.002896\n",
      "epoch: 6 [54080/60000 (90%)]\t training loss: 0.018746\n",
      "epoch: 6 [54400/60000 (91%)]\t training loss: 0.006554\n",
      "epoch: 6 [54720/60000 (91%)]\t training loss: 0.236211\n",
      "epoch: 6 [55040/60000 (92%)]\t training loss: 0.008973\n",
      "epoch: 6 [55360/60000 (92%)]\t training loss: 0.005393\n",
      "epoch: 6 [55680/60000 (93%)]\t training loss: 0.049449\n",
      "epoch: 6 [56000/60000 (93%)]\t training loss: 0.014995\n",
      "epoch: 6 [56320/60000 (94%)]\t training loss: 0.018884\n",
      "epoch: 6 [56640/60000 (94%)]\t training loss: 0.000350\n",
      "epoch: 6 [56960/60000 (95%)]\t training loss: 0.009543\n",
      "epoch: 6 [57280/60000 (95%)]\t training loss: 0.003473\n",
      "epoch: 6 [57600/60000 (96%)]\t training loss: 0.001411\n",
      "epoch: 6 [57920/60000 (97%)]\t training loss: 0.056163\n",
      "epoch: 6 [58240/60000 (97%)]\t training loss: 0.010890\n",
      "epoch: 6 [58560/60000 (98%)]\t training loss: 0.024135\n",
      "epoch: 6 [58880/60000 (98%)]\t training loss: 0.000798\n",
      "epoch: 6 [59200/60000 (99%)]\t training loss: 0.478776\n",
      "epoch: 6 [59520/60000 (99%)]\t training loss: 0.028107\n",
      "epoch: 6 [59840/60000 (100%)]\t training loss: 0.007967\n",
      "\n",
      "Test dataset: Overall Loss: 0.0342, Overall Accuracy: 9895/10000 (99%)\n",
      "\n",
      "epoch: 7 [0/60000 (0%)]\t training loss: 0.001558\n",
      "epoch: 7 [320/60000 (1%)]\t training loss: 0.097326\n",
      "epoch: 7 [640/60000 (1%)]\t training loss: 0.000327\n",
      "epoch: 7 [960/60000 (2%)]\t training loss: 0.000098\n",
      "epoch: 7 [1280/60000 (2%)]\t training loss: 0.001794\n",
      "epoch: 7 [1600/60000 (3%)]\t training loss: 0.078983\n",
      "epoch: 7 [1920/60000 (3%)]\t training loss: 0.000482\n",
      "epoch: 7 [2240/60000 (4%)]\t training loss: 0.001615\n",
      "epoch: 7 [2560/60000 (4%)]\t training loss: 0.006720\n",
      "epoch: 7 [2880/60000 (5%)]\t training loss: 0.002594\n",
      "epoch: 7 [3200/60000 (5%)]\t training loss: 0.025893\n",
      "epoch: 7 [3520/60000 (6%)]\t training loss: 0.028313\n",
      "epoch: 7 [3840/60000 (6%)]\t training loss: 0.024488\n",
      "epoch: 7 [4160/60000 (7%)]\t training loss: 0.001423\n",
      "epoch: 7 [4480/60000 (7%)]\t training loss: 0.067511\n",
      "epoch: 7 [4800/60000 (8%)]\t training loss: 0.000804\n",
      "epoch: 7 [5120/60000 (9%)]\t training loss: 0.017804\n",
      "epoch: 7 [5440/60000 (9%)]\t training loss: 0.008261\n",
      "epoch: 7 [5760/60000 (10%)]\t training loss: 0.009660\n",
      "epoch: 7 [6080/60000 (10%)]\t training loss: 0.007605\n",
      "epoch: 7 [6400/60000 (11%)]\t training loss: 0.144836\n",
      "epoch: 7 [6720/60000 (11%)]\t training loss: 0.460212\n",
      "epoch: 7 [7040/60000 (12%)]\t training loss: 0.001524\n",
      "epoch: 7 [7360/60000 (12%)]\t training loss: 0.014000\n",
      "epoch: 7 [7680/60000 (13%)]\t training loss: 0.002682\n",
      "epoch: 7 [8000/60000 (13%)]\t training loss: 0.039209\n",
      "epoch: 7 [8320/60000 (14%)]\t training loss: 0.000255\n",
      "epoch: 7 [8640/60000 (14%)]\t training loss: 0.000316\n",
      "epoch: 7 [8960/60000 (15%)]\t training loss: 0.026754\n",
      "epoch: 7 [9280/60000 (15%)]\t training loss: 0.002060\n",
      "epoch: 7 [9600/60000 (16%)]\t training loss: 0.006843\n",
      "epoch: 7 [9920/60000 (17%)]\t training loss: 0.012692\n",
      "epoch: 7 [10240/60000 (17%)]\t training loss: 0.081436\n",
      "epoch: 7 [10560/60000 (18%)]\t training loss: 0.029978\n",
      "epoch: 7 [10880/60000 (18%)]\t training loss: 0.001194\n",
      "epoch: 7 [11200/60000 (19%)]\t training loss: 0.000850\n",
      "epoch: 7 [11520/60000 (19%)]\t training loss: 0.001797\n",
      "epoch: 7 [11840/60000 (20%)]\t training loss: 0.000341\n",
      "epoch: 7 [12160/60000 (20%)]\t training loss: 0.054286\n",
      "epoch: 7 [12480/60000 (21%)]\t training loss: 0.005194\n",
      "epoch: 7 [12800/60000 (21%)]\t training loss: 0.074610\n",
      "epoch: 7 [13120/60000 (22%)]\t training loss: 0.025488\n",
      "epoch: 7 [13440/60000 (22%)]\t training loss: 0.022326\n",
      "epoch: 7 [13760/60000 (23%)]\t training loss: 0.004427\n",
      "epoch: 7 [14080/60000 (23%)]\t training loss: 0.000251\n",
      "epoch: 7 [14400/60000 (24%)]\t training loss: 0.082836\n",
      "epoch: 7 [14720/60000 (25%)]\t training loss: 0.163569\n",
      "epoch: 7 [15040/60000 (25%)]\t training loss: 0.056496\n",
      "epoch: 7 [15360/60000 (26%)]\t training loss: 0.003649\n",
      "epoch: 7 [15680/60000 (26%)]\t training loss: 0.078532\n",
      "epoch: 7 [16000/60000 (27%)]\t training loss: 0.001320\n",
      "epoch: 7 [16320/60000 (27%)]\t training loss: 0.000913\n",
      "epoch: 7 [16640/60000 (28%)]\t training loss: 0.035455\n",
      "epoch: 7 [16960/60000 (28%)]\t training loss: 0.045700\n",
      "epoch: 7 [17280/60000 (29%)]\t training loss: 0.002186\n",
      "epoch: 7 [17600/60000 (29%)]\t training loss: 0.000816\n",
      "epoch: 7 [17920/60000 (30%)]\t training loss: 0.003594\n",
      "epoch: 7 [18240/60000 (30%)]\t training loss: 0.013461\n",
      "epoch: 7 [18560/60000 (31%)]\t training loss: 0.035707\n",
      "epoch: 7 [18880/60000 (31%)]\t training loss: 0.092825\n",
      "epoch: 7 [19200/60000 (32%)]\t training loss: 0.023036\n",
      "epoch: 7 [19520/60000 (33%)]\t training loss: 0.098031\n",
      "epoch: 7 [19840/60000 (33%)]\t training loss: 0.083570\n",
      "epoch: 7 [20160/60000 (34%)]\t training loss: 0.009719\n",
      "epoch: 7 [20480/60000 (34%)]\t training loss: 0.095468\n",
      "epoch: 7 [20800/60000 (35%)]\t training loss: 0.000912\n",
      "epoch: 7 [21120/60000 (35%)]\t training loss: 0.003380\n",
      "epoch: 7 [21440/60000 (36%)]\t training loss: 0.000969\n",
      "epoch: 7 [21760/60000 (36%)]\t training loss: 0.006953\n",
      "epoch: 7 [22080/60000 (37%)]\t training loss: 0.005333\n",
      "epoch: 7 [22400/60000 (37%)]\t training loss: 0.012670\n",
      "epoch: 7 [22720/60000 (38%)]\t training loss: 0.001678\n",
      "epoch: 7 [23040/60000 (38%)]\t training loss: 0.027014\n",
      "epoch: 7 [23360/60000 (39%)]\t training loss: 0.002450\n",
      "epoch: 7 [23680/60000 (39%)]\t training loss: 0.000146\n",
      "epoch: 7 [24000/60000 (40%)]\t training loss: 0.000676\n",
      "epoch: 7 [24320/60000 (41%)]\t training loss: 0.006343\n",
      "epoch: 7 [24640/60000 (41%)]\t training loss: 0.031653\n",
      "epoch: 7 [24960/60000 (42%)]\t training loss: 0.001213\n",
      "epoch: 7 [25280/60000 (42%)]\t training loss: 0.000168\n",
      "epoch: 7 [25600/60000 (43%)]\t training loss: 0.060509\n",
      "epoch: 7 [25920/60000 (43%)]\t training loss: 0.003888\n",
      "epoch: 7 [26240/60000 (44%)]\t training loss: 0.001112\n",
      "epoch: 7 [26560/60000 (44%)]\t training loss: 0.170141\n",
      "epoch: 7 [26880/60000 (45%)]\t training loss: 0.000681\n",
      "epoch: 7 [27200/60000 (45%)]\t training loss: 0.000173\n",
      "epoch: 7 [27520/60000 (46%)]\t training loss: 0.002127\n",
      "epoch: 7 [27840/60000 (46%)]\t training loss: 0.001567\n",
      "epoch: 7 [28160/60000 (47%)]\t training loss: 0.001182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 [28480/60000 (47%)]\t training loss: 0.132422\n",
      "epoch: 7 [28800/60000 (48%)]\t training loss: 0.012652\n",
      "epoch: 7 [29120/60000 (49%)]\t training loss: 0.001845\n",
      "epoch: 7 [29440/60000 (49%)]\t training loss: 0.387857\n",
      "epoch: 7 [29760/60000 (50%)]\t training loss: 0.005290\n",
      "epoch: 7 [30080/60000 (50%)]\t training loss: 0.039206\n",
      "epoch: 7 [30400/60000 (51%)]\t training loss: 0.001702\n",
      "epoch: 7 [30720/60000 (51%)]\t training loss: 0.003981\n",
      "epoch: 7 [31040/60000 (52%)]\t training loss: 0.061687\n",
      "epoch: 7 [31360/60000 (52%)]\t training loss: 0.031186\n",
      "epoch: 7 [31680/60000 (53%)]\t training loss: 0.001579\n",
      "epoch: 7 [32000/60000 (53%)]\t training loss: 0.000328\n",
      "epoch: 7 [32320/60000 (54%)]\t training loss: 0.001270\n",
      "epoch: 7 [32640/60000 (54%)]\t training loss: 0.062429\n",
      "epoch: 7 [32960/60000 (55%)]\t training loss: 0.001480\n",
      "epoch: 7 [33280/60000 (55%)]\t training loss: 0.000701\n",
      "epoch: 7 [33600/60000 (56%)]\t training loss: 0.000226\n",
      "epoch: 7 [33920/60000 (57%)]\t training loss: 0.000942\n",
      "epoch: 7 [34240/60000 (57%)]\t training loss: 0.065267\n",
      "epoch: 7 [34560/60000 (58%)]\t training loss: 0.140396\n",
      "epoch: 7 [34880/60000 (58%)]\t training loss: 0.074935\n",
      "epoch: 7 [35200/60000 (59%)]\t training loss: 0.001406\n",
      "epoch: 7 [35520/60000 (59%)]\t training loss: 0.001361\n",
      "epoch: 7 [35840/60000 (60%)]\t training loss: 0.008588\n",
      "epoch: 7 [36160/60000 (60%)]\t training loss: 0.015500\n",
      "epoch: 7 [36480/60000 (61%)]\t training loss: 0.004844\n",
      "epoch: 7 [36800/60000 (61%)]\t training loss: 0.183980\n",
      "epoch: 7 [37120/60000 (62%)]\t training loss: 0.067573\n",
      "epoch: 7 [37440/60000 (62%)]\t training loss: 0.140301\n",
      "epoch: 7 [37760/60000 (63%)]\t training loss: 0.042477\n",
      "epoch: 7 [38080/60000 (63%)]\t training loss: 0.000647\n",
      "epoch: 7 [38400/60000 (64%)]\t training loss: 0.003972\n",
      "epoch: 7 [38720/60000 (65%)]\t training loss: 0.008307\n",
      "epoch: 7 [39040/60000 (65%)]\t training loss: 0.012949\n",
      "epoch: 7 [39360/60000 (66%)]\t training loss: 0.013570\n",
      "epoch: 7 [39680/60000 (66%)]\t training loss: 0.170513\n",
      "epoch: 7 [40000/60000 (67%)]\t training loss: 0.000789\n",
      "epoch: 7 [40320/60000 (67%)]\t training loss: 0.020800\n",
      "epoch: 7 [40640/60000 (68%)]\t training loss: 0.033811\n",
      "epoch: 7 [40960/60000 (68%)]\t training loss: 0.049771\n",
      "epoch: 7 [41280/60000 (69%)]\t training loss: 0.000341\n",
      "epoch: 7 [41600/60000 (69%)]\t training loss: 0.076302\n",
      "epoch: 7 [41920/60000 (70%)]\t training loss: 0.000155\n",
      "epoch: 7 [42240/60000 (70%)]\t training loss: 0.053569\n",
      "epoch: 7 [42560/60000 (71%)]\t training loss: 0.062540\n",
      "epoch: 7 [42880/60000 (71%)]\t training loss: 0.083379\n",
      "epoch: 7 [43200/60000 (72%)]\t training loss: 0.004161\n",
      "epoch: 7 [43520/60000 (73%)]\t training loss: 0.000754\n",
      "epoch: 7 [43840/60000 (73%)]\t training loss: 0.006255\n",
      "epoch: 7 [44160/60000 (74%)]\t training loss: 0.143651\n",
      "epoch: 7 [44480/60000 (74%)]\t training loss: 0.001861\n",
      "epoch: 7 [44800/60000 (75%)]\t training loss: 0.124301\n",
      "epoch: 7 [45120/60000 (75%)]\t training loss: 0.083339\n",
      "epoch: 7 [45440/60000 (76%)]\t training loss: 0.002248\n",
      "epoch: 7 [45760/60000 (76%)]\t training loss: 0.023867\n",
      "epoch: 7 [46080/60000 (77%)]\t training loss: 0.006835\n",
      "epoch: 7 [46400/60000 (77%)]\t training loss: 0.025502\n",
      "epoch: 7 [46720/60000 (78%)]\t training loss: 0.002014\n",
      "epoch: 7 [47040/60000 (78%)]\t training loss: 0.006779\n",
      "epoch: 7 [47360/60000 (79%)]\t training loss: 0.051501\n",
      "epoch: 7 [47680/60000 (79%)]\t training loss: 0.119230\n",
      "epoch: 7 [48000/60000 (80%)]\t training loss: 0.157191\n",
      "epoch: 7 [48320/60000 (81%)]\t training loss: 0.004454\n",
      "epoch: 7 [48640/60000 (81%)]\t training loss: 0.004759\n",
      "epoch: 7 [48960/60000 (82%)]\t training loss: 0.002776\n",
      "epoch: 7 [49280/60000 (82%)]\t training loss: 0.000360\n",
      "epoch: 7 [49600/60000 (83%)]\t training loss: 0.027511\n",
      "epoch: 7 [49920/60000 (83%)]\t training loss: 0.027459\n",
      "epoch: 7 [50240/60000 (84%)]\t training loss: 0.006972\n",
      "epoch: 7 [50560/60000 (84%)]\t training loss: 0.235726\n",
      "epoch: 7 [50880/60000 (85%)]\t training loss: 0.027263\n",
      "epoch: 7 [51200/60000 (85%)]\t training loss: 0.008733\n",
      "epoch: 7 [51520/60000 (86%)]\t training loss: 0.078260\n",
      "epoch: 7 [51840/60000 (86%)]\t training loss: 0.000106\n",
      "epoch: 7 [52160/60000 (87%)]\t training loss: 0.000108\n",
      "epoch: 7 [52480/60000 (87%)]\t training loss: 0.118525\n",
      "epoch: 7 [52800/60000 (88%)]\t training loss: 0.013370\n",
      "epoch: 7 [53120/60000 (89%)]\t training loss: 0.000054\n",
      "epoch: 7 [53440/60000 (89%)]\t training loss: 0.091865\n",
      "epoch: 7 [53760/60000 (90%)]\t training loss: 0.047083\n",
      "epoch: 7 [54080/60000 (90%)]\t training loss: 0.004968\n",
      "epoch: 7 [54400/60000 (91%)]\t training loss: 0.063891\n",
      "epoch: 7 [54720/60000 (91%)]\t training loss: 0.008891\n",
      "epoch: 7 [55040/60000 (92%)]\t training loss: 0.001651\n",
      "epoch: 7 [55360/60000 (92%)]\t training loss: 0.005434\n",
      "epoch: 7 [55680/60000 (93%)]\t training loss: 0.064193\n",
      "epoch: 7 [56000/60000 (93%)]\t training loss: 0.054582\n",
      "epoch: 7 [56320/60000 (94%)]\t training loss: 0.001418\n",
      "epoch: 7 [56640/60000 (94%)]\t training loss: 0.072884\n",
      "epoch: 7 [56960/60000 (95%)]\t training loss: 0.007902\n",
      "epoch: 7 [57280/60000 (95%)]\t training loss: 0.189332\n",
      "epoch: 7 [57600/60000 (96%)]\t training loss: 0.000902\n",
      "epoch: 7 [57920/60000 (97%)]\t training loss: 0.000240\n",
      "epoch: 7 [58240/60000 (97%)]\t training loss: 0.011421\n",
      "epoch: 7 [58560/60000 (98%)]\t training loss: 0.138466\n",
      "epoch: 7 [58880/60000 (98%)]\t training loss: 0.000655\n",
      "epoch: 7 [59200/60000 (99%)]\t training loss: 0.003655\n",
      "epoch: 7 [59520/60000 (99%)]\t training loss: 0.003384\n",
      "epoch: 7 [59840/60000 (100%)]\t training loss: 0.005301\n",
      "\n",
      "Test dataset: Overall Loss: 0.0282, Overall Accuracy: 9920/10000 (99%)\n",
      "\n",
      "epoch: 8 [0/60000 (0%)]\t training loss: 0.003773\n",
      "epoch: 8 [320/60000 (1%)]\t training loss: 0.000482\n",
      "epoch: 8 [640/60000 (1%)]\t training loss: 0.040244\n",
      "epoch: 8 [960/60000 (2%)]\t training loss: 0.000869\n",
      "epoch: 8 [1280/60000 (2%)]\t training loss: 0.002861\n",
      "epoch: 8 [1600/60000 (3%)]\t training loss: 0.009968\n",
      "epoch: 8 [1920/60000 (3%)]\t training loss: 0.108575\n",
      "epoch: 8 [2240/60000 (4%)]\t training loss: 0.013126\n",
      "epoch: 8 [2560/60000 (4%)]\t training loss: 0.067322\n",
      "epoch: 8 [2880/60000 (5%)]\t training loss: 0.000288\n",
      "epoch: 8 [3200/60000 (5%)]\t training loss: 0.000409\n",
      "epoch: 8 [3520/60000 (6%)]\t training loss: 0.059193\n",
      "epoch: 8 [3840/60000 (6%)]\t training loss: 0.001574\n",
      "epoch: 8 [4160/60000 (7%)]\t training loss: 0.000041\n",
      "epoch: 8 [4480/60000 (7%)]\t training loss: 0.000070\n",
      "epoch: 8 [4800/60000 (8%)]\t training loss: 0.000146\n",
      "epoch: 8 [5120/60000 (9%)]\t training loss: 0.003139\n",
      "epoch: 8 [5440/60000 (9%)]\t training loss: 0.021601\n",
      "epoch: 8 [5760/60000 (10%)]\t training loss: 0.061120\n",
      "epoch: 8 [6080/60000 (10%)]\t training loss: 0.002551\n",
      "epoch: 8 [6400/60000 (11%)]\t training loss: 0.139058\n",
      "epoch: 8 [6720/60000 (11%)]\t training loss: 0.004487\n",
      "epoch: 8 [7040/60000 (12%)]\t training loss: 0.000727\n",
      "epoch: 8 [7360/60000 (12%)]\t training loss: 0.000635\n",
      "epoch: 8 [7680/60000 (13%)]\t training loss: 0.044373\n",
      "epoch: 8 [8000/60000 (13%)]\t training loss: 0.025045\n",
      "epoch: 8 [8320/60000 (14%)]\t training loss: 0.001593\n",
      "epoch: 8 [8640/60000 (14%)]\t training loss: 0.094778\n",
      "epoch: 8 [8960/60000 (15%)]\t training loss: 0.001906\n",
      "epoch: 8 [9280/60000 (15%)]\t training loss: 0.019165\n",
      "epoch: 8 [9600/60000 (16%)]\t training loss: 0.000081\n",
      "epoch: 8 [9920/60000 (17%)]\t training loss: 0.004181\n",
      "epoch: 8 [10240/60000 (17%)]\t training loss: 0.004268\n",
      "epoch: 8 [10560/60000 (18%)]\t training loss: 0.006409\n",
      "epoch: 8 [10880/60000 (18%)]\t training loss: 0.003569\n",
      "epoch: 8 [11200/60000 (19%)]\t training loss: 0.001137\n",
      "epoch: 8 [11520/60000 (19%)]\t training loss: 0.000663\n",
      "epoch: 8 [11840/60000 (20%)]\t training loss: 0.010913\n",
      "epoch: 8 [12160/60000 (20%)]\t training loss: 0.000493\n",
      "epoch: 8 [12480/60000 (21%)]\t training loss: 0.000824\n",
      "epoch: 8 [12800/60000 (21%)]\t training loss: 0.020248\n",
      "epoch: 8 [13120/60000 (22%)]\t training loss: 0.032311\n",
      "epoch: 8 [13440/60000 (22%)]\t training loss: 0.073579\n",
      "epoch: 8 [13760/60000 (23%)]\t training loss: 0.103218\n",
      "epoch: 8 [14080/60000 (23%)]\t training loss: 0.014451\n",
      "epoch: 8 [14400/60000 (24%)]\t training loss: 0.028764\n",
      "epoch: 8 [14720/60000 (25%)]\t training loss: 0.002600\n",
      "epoch: 8 [15040/60000 (25%)]\t training loss: 0.009174\n",
      "epoch: 8 [15360/60000 (26%)]\t training loss: 0.000595\n",
      "epoch: 8 [15680/60000 (26%)]\t training loss: 0.006409\n",
      "epoch: 8 [16000/60000 (27%)]\t training loss: 0.025570\n",
      "epoch: 8 [16320/60000 (27%)]\t training loss: 0.003765\n",
      "epoch: 8 [16640/60000 (28%)]\t training loss: 0.002064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 [16960/60000 (28%)]\t training loss: 0.230575\n",
      "epoch: 8 [17280/60000 (29%)]\t training loss: 0.047843\n",
      "epoch: 8 [17600/60000 (29%)]\t training loss: 0.077518\n",
      "epoch: 8 [17920/60000 (30%)]\t training loss: 0.000045\n",
      "epoch: 8 [18240/60000 (30%)]\t training loss: 0.008665\n",
      "epoch: 8 [18560/60000 (31%)]\t training loss: 0.015173\n",
      "epoch: 8 [18880/60000 (31%)]\t training loss: 0.001064\n",
      "epoch: 8 [19200/60000 (32%)]\t training loss: 0.001941\n",
      "epoch: 8 [19520/60000 (33%)]\t training loss: 0.000567\n",
      "epoch: 8 [19840/60000 (33%)]\t training loss: 0.071398\n",
      "epoch: 8 [20160/60000 (34%)]\t training loss: 0.005542\n",
      "epoch: 8 [20480/60000 (34%)]\t training loss: 0.002950\n",
      "epoch: 8 [20800/60000 (35%)]\t training loss: 0.000274\n",
      "epoch: 8 [21120/60000 (35%)]\t training loss: 0.110373\n",
      "epoch: 8 [21440/60000 (36%)]\t training loss: 0.076120\n",
      "epoch: 8 [21760/60000 (36%)]\t training loss: 0.000736\n",
      "epoch: 8 [22080/60000 (37%)]\t training loss: 0.000653\n",
      "epoch: 8 [22400/60000 (37%)]\t training loss: 0.002018\n",
      "epoch: 8 [22720/60000 (38%)]\t training loss: 0.008181\n",
      "epoch: 8 [23040/60000 (38%)]\t training loss: 0.024863\n",
      "epoch: 8 [23360/60000 (39%)]\t training loss: 0.047228\n",
      "epoch: 8 [23680/60000 (39%)]\t training loss: 0.002152\n",
      "epoch: 8 [24000/60000 (40%)]\t training loss: 0.001990\n",
      "epoch: 8 [24320/60000 (41%)]\t training loss: 0.022661\n",
      "epoch: 8 [24640/60000 (41%)]\t training loss: 0.008303\n",
      "epoch: 8 [24960/60000 (42%)]\t training loss: 0.142603\n",
      "epoch: 8 [25280/60000 (42%)]\t training loss: 0.019498\n",
      "epoch: 8 [25600/60000 (43%)]\t training loss: 0.014777\n",
      "epoch: 8 [25920/60000 (43%)]\t training loss: 0.003866\n",
      "epoch: 8 [26240/60000 (44%)]\t training loss: 0.000480\n",
      "epoch: 8 [26560/60000 (44%)]\t training loss: 0.030541\n",
      "epoch: 8 [26880/60000 (45%)]\t training loss: 0.000742\n",
      "epoch: 8 [27200/60000 (45%)]\t training loss: 0.000311\n",
      "epoch: 8 [27520/60000 (46%)]\t training loss: 0.022359\n",
      "epoch: 8 [27840/60000 (46%)]\t training loss: 0.008245\n",
      "epoch: 8 [28160/60000 (47%)]\t training loss: 0.148810\n",
      "epoch: 8 [28480/60000 (47%)]\t training loss: 0.000525\n",
      "epoch: 8 [28800/60000 (48%)]\t training loss: 0.451616\n",
      "epoch: 8 [29120/60000 (49%)]\t training loss: 0.004105\n",
      "epoch: 8 [29440/60000 (49%)]\t training loss: 0.029154\n",
      "epoch: 8 [29760/60000 (50%)]\t training loss: 0.000778\n",
      "epoch: 8 [30080/60000 (50%)]\t training loss: 0.003420\n",
      "epoch: 8 [30400/60000 (51%)]\t training loss: 0.007716\n",
      "epoch: 8 [30720/60000 (51%)]\t training loss: 0.003593\n",
      "epoch: 8 [31040/60000 (52%)]\t training loss: 0.030546\n",
      "epoch: 8 [31360/60000 (52%)]\t training loss: 0.001260\n",
      "epoch: 8 [31680/60000 (53%)]\t training loss: 0.027621\n",
      "epoch: 8 [32000/60000 (53%)]\t training loss: 0.025781\n",
      "epoch: 8 [32320/60000 (54%)]\t training loss: 0.001584\n",
      "epoch: 8 [32640/60000 (54%)]\t training loss: 0.055502\n",
      "epoch: 8 [32960/60000 (55%)]\t training loss: 0.000488\n",
      "epoch: 8 [33280/60000 (55%)]\t training loss: 0.002288\n",
      "epoch: 8 [33600/60000 (56%)]\t training loss: 0.074923\n",
      "epoch: 8 [33920/60000 (57%)]\t training loss: 0.009530\n",
      "epoch: 8 [34240/60000 (57%)]\t training loss: 0.000207\n",
      "epoch: 8 [34560/60000 (58%)]\t training loss: 0.055774\n",
      "epoch: 8 [34880/60000 (58%)]\t training loss: 0.000459\n",
      "epoch: 8 [35200/60000 (59%)]\t training loss: 0.002032\n",
      "epoch: 8 [35520/60000 (59%)]\t training loss: 0.009364\n",
      "epoch: 8 [35840/60000 (60%)]\t training loss: 0.001405\n",
      "epoch: 8 [36160/60000 (60%)]\t training loss: 0.002138\n",
      "epoch: 8 [36480/60000 (61%)]\t training loss: 0.000579\n",
      "epoch: 8 [36800/60000 (61%)]\t training loss: 0.110490\n",
      "epoch: 8 [37120/60000 (62%)]\t training loss: 0.000098\n",
      "epoch: 8 [37440/60000 (62%)]\t training loss: 0.000284\n",
      "epoch: 8 [37760/60000 (63%)]\t training loss: 0.000471\n",
      "epoch: 8 [38080/60000 (63%)]\t training loss: 0.096632\n",
      "epoch: 8 [38400/60000 (64%)]\t training loss: 0.095857\n",
      "epoch: 8 [38720/60000 (65%)]\t training loss: 0.038705\n",
      "epoch: 8 [39040/60000 (65%)]\t training loss: 0.001928\n",
      "epoch: 8 [39360/60000 (66%)]\t training loss: 0.002863\n",
      "epoch: 8 [39680/60000 (66%)]\t training loss: 0.001342\n",
      "epoch: 8 [40000/60000 (67%)]\t training loss: 0.001971\n",
      "epoch: 8 [40320/60000 (67%)]\t training loss: 0.000819\n",
      "epoch: 8 [40640/60000 (68%)]\t training loss: 0.019996\n",
      "epoch: 8 [40960/60000 (68%)]\t training loss: 0.009367\n",
      "epoch: 8 [41280/60000 (69%)]\t training loss: 0.002365\n",
      "epoch: 8 [41600/60000 (69%)]\t training loss: 0.000083\n",
      "epoch: 8 [41920/60000 (70%)]\t training loss: 0.001170\n",
      "epoch: 8 [42240/60000 (70%)]\t training loss: 0.008771\n",
      "epoch: 8 [42560/60000 (71%)]\t training loss: 0.002521\n",
      "epoch: 8 [42880/60000 (71%)]\t training loss: 0.051144\n",
      "epoch: 8 [43200/60000 (72%)]\t training loss: 0.017124\n",
      "epoch: 8 [43520/60000 (73%)]\t training loss: 0.000079\n",
      "epoch: 8 [43840/60000 (73%)]\t training loss: 0.053118\n",
      "epoch: 8 [44160/60000 (74%)]\t training loss: 0.003509\n",
      "epoch: 8 [44480/60000 (74%)]\t training loss: 0.096103\n",
      "epoch: 8 [44800/60000 (75%)]\t training loss: 0.002896\n",
      "epoch: 8 [45120/60000 (75%)]\t training loss: 0.021088\n",
      "epoch: 8 [45440/60000 (76%)]\t training loss: 0.003921\n",
      "epoch: 8 [45760/60000 (76%)]\t training loss: 0.016923\n",
      "epoch: 8 [46080/60000 (77%)]\t training loss: 0.051880\n",
      "epoch: 8 [46400/60000 (77%)]\t training loss: 0.194977\n",
      "epoch: 8 [46720/60000 (78%)]\t training loss: 0.002439\n",
      "epoch: 8 [47040/60000 (78%)]\t training loss: 0.002979\n",
      "epoch: 8 [47360/60000 (79%)]\t training loss: 0.001642\n",
      "epoch: 8 [47680/60000 (79%)]\t training loss: 0.005478\n",
      "epoch: 8 [48000/60000 (80%)]\t training loss: 0.011958\n",
      "epoch: 8 [48320/60000 (81%)]\t training loss: 0.233324\n",
      "epoch: 8 [48640/60000 (81%)]\t training loss: 0.006332\n",
      "epoch: 8 [48960/60000 (82%)]\t training loss: 0.013049\n",
      "epoch: 8 [49280/60000 (82%)]\t training loss: 0.002365\n",
      "epoch: 8 [49600/60000 (83%)]\t training loss: 0.004993\n",
      "epoch: 8 [49920/60000 (83%)]\t training loss: 0.055511\n",
      "epoch: 8 [50240/60000 (84%)]\t training loss: 0.001221\n",
      "epoch: 8 [50560/60000 (84%)]\t training loss: 0.041660\n",
      "epoch: 8 [50880/60000 (85%)]\t training loss: 0.106151\n",
      "epoch: 8 [51200/60000 (85%)]\t training loss: 0.013970\n",
      "epoch: 8 [51520/60000 (86%)]\t training loss: 0.004108\n",
      "epoch: 8 [51840/60000 (86%)]\t training loss: 0.000335\n",
      "epoch: 8 [52160/60000 (87%)]\t training loss: 0.000001\n",
      "epoch: 8 [52480/60000 (87%)]\t training loss: 0.000029\n",
      "epoch: 8 [52800/60000 (88%)]\t training loss: 0.000220\n",
      "epoch: 8 [53120/60000 (89%)]\t training loss: 0.003467\n",
      "epoch: 8 [53440/60000 (89%)]\t training loss: 0.006774\n",
      "epoch: 8 [53760/60000 (90%)]\t training loss: 0.003387\n",
      "epoch: 8 [54080/60000 (90%)]\t training loss: 0.010096\n",
      "epoch: 8 [54400/60000 (91%)]\t training loss: 0.002614\n",
      "epoch: 8 [54720/60000 (91%)]\t training loss: 0.000147\n",
      "epoch: 8 [55040/60000 (92%)]\t training loss: 0.032661\n",
      "epoch: 8 [55360/60000 (92%)]\t training loss: 0.000899\n",
      "epoch: 8 [55680/60000 (93%)]\t training loss: 0.091459\n",
      "epoch: 8 [56000/60000 (93%)]\t training loss: 0.357329\n",
      "epoch: 8 [56320/60000 (94%)]\t training loss: 0.004715\n",
      "epoch: 8 [56640/60000 (94%)]\t training loss: 0.009227\n",
      "epoch: 8 [56960/60000 (95%)]\t training loss: 0.006016\n",
      "epoch: 8 [57280/60000 (95%)]\t training loss: 0.000591\n",
      "epoch: 8 [57600/60000 (96%)]\t training loss: 0.131462\n",
      "epoch: 8 [57920/60000 (97%)]\t training loss: 0.001092\n",
      "epoch: 8 [58240/60000 (97%)]\t training loss: 0.005213\n",
      "epoch: 8 [58560/60000 (98%)]\t training loss: 0.002897\n",
      "epoch: 8 [58880/60000 (98%)]\t training loss: 0.001509\n",
      "epoch: 8 [59200/60000 (99%)]\t training loss: 0.048649\n",
      "epoch: 8 [59520/60000 (99%)]\t training loss: 0.046486\n",
      "epoch: 8 [59840/60000 (100%)]\t training loss: 0.011363\n",
      "\n",
      "Test dataset: Overall Loss: 0.0316, Overall Accuracy: 9904/10000 (99%)\n",
      "\n",
      "epoch: 9 [0/60000 (0%)]\t training loss: 0.013888\n",
      "epoch: 9 [320/60000 (1%)]\t training loss: 0.002141\n",
      "epoch: 9 [640/60000 (1%)]\t training loss: 0.151072\n",
      "epoch: 9 [960/60000 (2%)]\t training loss: 0.000572\n",
      "epoch: 9 [1280/60000 (2%)]\t training loss: 0.013034\n",
      "epoch: 9 [1600/60000 (3%)]\t training loss: 0.063494\n",
      "epoch: 9 [1920/60000 (3%)]\t training loss: 0.000163\n",
      "epoch: 9 [2240/60000 (4%)]\t training loss: 0.000191\n",
      "epoch: 9 [2560/60000 (4%)]\t training loss: 0.003001\n",
      "epoch: 9 [2880/60000 (5%)]\t training loss: 0.000518\n",
      "epoch: 9 [3200/60000 (5%)]\t training loss: 0.316099\n",
      "epoch: 9 [3520/60000 (6%)]\t training loss: 0.008093\n",
      "epoch: 9 [3840/60000 (6%)]\t training loss: 0.053497\n",
      "epoch: 9 [4160/60000 (7%)]\t training loss: 0.002517\n",
      "epoch: 9 [4480/60000 (7%)]\t training loss: 0.000261\n",
      "epoch: 9 [4800/60000 (8%)]\t training loss: 0.000907\n",
      "epoch: 9 [5120/60000 (9%)]\t training loss: 0.006915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 [5440/60000 (9%)]\t training loss: 0.000716\n",
      "epoch: 9 [5760/60000 (10%)]\t training loss: 0.015976\n",
      "epoch: 9 [6080/60000 (10%)]\t training loss: 0.000487\n",
      "epoch: 9 [6400/60000 (11%)]\t training loss: 0.013870\n",
      "epoch: 9 [6720/60000 (11%)]\t training loss: 0.031214\n",
      "epoch: 9 [7040/60000 (12%)]\t training loss: 0.028990\n",
      "epoch: 9 [7360/60000 (12%)]\t training loss: 0.005884\n",
      "epoch: 9 [7680/60000 (13%)]\t training loss: 0.002003\n",
      "epoch: 9 [8000/60000 (13%)]\t training loss: 0.037963\n",
      "epoch: 9 [8320/60000 (14%)]\t training loss: 0.001370\n",
      "epoch: 9 [8640/60000 (14%)]\t training loss: 0.008793\n",
      "epoch: 9 [8960/60000 (15%)]\t training loss: 0.029197\n",
      "epoch: 9 [9280/60000 (15%)]\t training loss: 0.010197\n",
      "epoch: 9 [9600/60000 (16%)]\t training loss: 0.068738\n",
      "epoch: 9 [9920/60000 (17%)]\t training loss: 0.099905\n",
      "epoch: 9 [10240/60000 (17%)]\t training loss: 0.003335\n",
      "epoch: 9 [10560/60000 (18%)]\t training loss: 0.001287\n",
      "epoch: 9 [10880/60000 (18%)]\t training loss: 0.002020\n",
      "epoch: 9 [11200/60000 (19%)]\t training loss: 0.001195\n",
      "epoch: 9 [11520/60000 (19%)]\t training loss: 0.001240\n",
      "epoch: 9 [11840/60000 (20%)]\t training loss: 0.005399\n",
      "epoch: 9 [12160/60000 (20%)]\t training loss: 0.000549\n",
      "epoch: 9 [12480/60000 (21%)]\t training loss: 0.009253\n",
      "epoch: 9 [12800/60000 (21%)]\t training loss: 0.008110\n",
      "epoch: 9 [13120/60000 (22%)]\t training loss: 0.143453\n",
      "epoch: 9 [13440/60000 (22%)]\t training loss: 0.013632\n",
      "epoch: 9 [13760/60000 (23%)]\t training loss: 0.028072\n",
      "epoch: 9 [14080/60000 (23%)]\t training loss: 0.021339\n",
      "epoch: 9 [14400/60000 (24%)]\t training loss: 0.004389\n",
      "epoch: 9 [14720/60000 (25%)]\t training loss: 0.050861\n",
      "epoch: 9 [15040/60000 (25%)]\t training loss: 0.015856\n",
      "epoch: 9 [15360/60000 (26%)]\t training loss: 0.000225\n",
      "epoch: 9 [15680/60000 (26%)]\t training loss: 0.040318\n",
      "epoch: 9 [16000/60000 (27%)]\t training loss: 0.001194\n",
      "epoch: 9 [16320/60000 (27%)]\t training loss: 0.000222\n",
      "epoch: 9 [16640/60000 (28%)]\t training loss: 0.002515\n",
      "epoch: 9 [16960/60000 (28%)]\t training loss: 0.134873\n",
      "epoch: 9 [17280/60000 (29%)]\t training loss: 0.002913\n",
      "epoch: 9 [17600/60000 (29%)]\t training loss: 0.013119\n",
      "epoch: 9 [17920/60000 (30%)]\t training loss: 0.021542\n",
      "epoch: 9 [18240/60000 (30%)]\t training loss: 0.002071\n",
      "epoch: 9 [18560/60000 (31%)]\t training loss: 0.005952\n",
      "epoch: 9 [18880/60000 (31%)]\t training loss: 0.000458\n",
      "epoch: 9 [19200/60000 (32%)]\t training loss: 0.001631\n",
      "epoch: 9 [19520/60000 (33%)]\t training loss: 0.026031\n",
      "epoch: 9 [19840/60000 (33%)]\t training loss: 0.085370\n",
      "epoch: 9 [20160/60000 (34%)]\t training loss: 0.041389\n",
      "epoch: 9 [20480/60000 (34%)]\t training loss: 0.023380\n",
      "epoch: 9 [20800/60000 (35%)]\t training loss: 0.021541\n",
      "epoch: 9 [21120/60000 (35%)]\t training loss: 0.001509\n",
      "epoch: 9 [21440/60000 (36%)]\t training loss: 0.007479\n",
      "epoch: 9 [21760/60000 (36%)]\t training loss: 0.005667\n",
      "epoch: 9 [22080/60000 (37%)]\t training loss: 0.049334\n",
      "epoch: 9 [22400/60000 (37%)]\t training loss: 0.091419\n",
      "epoch: 9 [22720/60000 (38%)]\t training loss: 0.001146\n",
      "epoch: 9 [23040/60000 (38%)]\t training loss: 0.000219\n",
      "epoch: 9 [23360/60000 (39%)]\t training loss: 0.011720\n",
      "epoch: 9 [23680/60000 (39%)]\t training loss: 0.000129\n",
      "epoch: 9 [24000/60000 (40%)]\t training loss: 0.000235\n",
      "epoch: 9 [24320/60000 (41%)]\t training loss: 0.065309\n",
      "epoch: 9 [24640/60000 (41%)]\t training loss: 0.004066\n",
      "epoch: 9 [24960/60000 (42%)]\t training loss: 0.001781\n",
      "epoch: 9 [25280/60000 (42%)]\t training loss: 0.000183\n",
      "epoch: 9 [25600/60000 (43%)]\t training loss: 0.061957\n",
      "epoch: 9 [25920/60000 (43%)]\t training loss: 0.013809\n",
      "epoch: 9 [26240/60000 (44%)]\t training loss: 0.000040\n",
      "epoch: 9 [26560/60000 (44%)]\t training loss: 0.003855\n",
      "epoch: 9 [26880/60000 (45%)]\t training loss: 0.000900\n",
      "epoch: 9 [27200/60000 (45%)]\t training loss: 0.009112\n",
      "epoch: 9 [27520/60000 (46%)]\t training loss: 0.005413\n",
      "epoch: 9 [27840/60000 (46%)]\t training loss: 0.001631\n",
      "epoch: 9 [28160/60000 (47%)]\t training loss: 0.000089\n",
      "epoch: 9 [28480/60000 (47%)]\t training loss: 0.000146\n",
      "epoch: 9 [28800/60000 (48%)]\t training loss: 0.003034\n",
      "epoch: 9 [29120/60000 (49%)]\t training loss: 0.026042\n",
      "epoch: 9 [29440/60000 (49%)]\t training loss: 0.024520\n",
      "epoch: 9 [29760/60000 (50%)]\t training loss: 0.000004\n",
      "epoch: 9 [30080/60000 (50%)]\t training loss: 0.011596\n",
      "epoch: 9 [30400/60000 (51%)]\t training loss: 0.000671\n",
      "epoch: 9 [30720/60000 (51%)]\t training loss: 0.014786\n",
      "epoch: 9 [31040/60000 (52%)]\t training loss: 0.008062\n",
      "epoch: 9 [31360/60000 (52%)]\t training loss: 0.008146\n",
      "epoch: 9 [31680/60000 (53%)]\t training loss: 0.000874\n",
      "epoch: 9 [32000/60000 (53%)]\t training loss: 0.048275\n",
      "epoch: 9 [32320/60000 (54%)]\t training loss: 0.015202\n",
      "epoch: 9 [32640/60000 (54%)]\t training loss: 0.187426\n",
      "epoch: 9 [32960/60000 (55%)]\t training loss: 0.001336\n",
      "epoch: 9 [33280/60000 (55%)]\t training loss: 0.006330\n",
      "epoch: 9 [33600/60000 (56%)]\t training loss: 0.005385\n",
      "epoch: 9 [33920/60000 (57%)]\t training loss: 0.010949\n",
      "epoch: 9 [34240/60000 (57%)]\t training loss: 0.000163\n",
      "epoch: 9 [34560/60000 (58%)]\t training loss: 0.000299\n",
      "epoch: 9 [34880/60000 (58%)]\t training loss: 0.001033\n",
      "epoch: 9 [35200/60000 (59%)]\t training loss: 0.000028\n",
      "epoch: 9 [35520/60000 (59%)]\t training loss: 0.078379\n",
      "epoch: 9 [35840/60000 (60%)]\t training loss: 0.020450\n",
      "epoch: 9 [36160/60000 (60%)]\t training loss: 0.004901\n",
      "epoch: 9 [36480/60000 (61%)]\t training loss: 0.002930\n",
      "epoch: 9 [36800/60000 (61%)]\t training loss: 0.006230\n",
      "epoch: 9 [37120/60000 (62%)]\t training loss: 0.000013\n",
      "epoch: 9 [37440/60000 (62%)]\t training loss: 0.006476\n",
      "epoch: 9 [37760/60000 (63%)]\t training loss: 0.000217\n",
      "epoch: 9 [38080/60000 (63%)]\t training loss: 0.003795\n",
      "epoch: 9 [38400/60000 (64%)]\t training loss: 0.056514\n",
      "epoch: 9 [38720/60000 (65%)]\t training loss: 0.061711\n",
      "epoch: 9 [39040/60000 (65%)]\t training loss: 0.013733\n",
      "epoch: 9 [39360/60000 (66%)]\t training loss: 0.047809\n",
      "epoch: 9 [39680/60000 (66%)]\t training loss: 0.002265\n",
      "epoch: 9 [40000/60000 (67%)]\t training loss: 0.021311\n",
      "epoch: 9 [40320/60000 (67%)]\t training loss: 0.000745\n",
      "epoch: 9 [40640/60000 (68%)]\t training loss: 0.001170\n",
      "epoch: 9 [40960/60000 (68%)]\t training loss: 0.002923\n",
      "epoch: 9 [41280/60000 (69%)]\t training loss: 0.000428\n",
      "epoch: 9 [41600/60000 (69%)]\t training loss: 0.003592\n",
      "epoch: 9 [41920/60000 (70%)]\t training loss: 0.000600\n",
      "epoch: 9 [42240/60000 (70%)]\t training loss: 0.003748\n",
      "epoch: 9 [42560/60000 (71%)]\t training loss: 0.003025\n",
      "epoch: 9 [42880/60000 (71%)]\t training loss: 0.000178\n",
      "epoch: 9 [43200/60000 (72%)]\t training loss: 0.000182\n",
      "epoch: 9 [43520/60000 (73%)]\t training loss: 0.000092\n",
      "epoch: 9 [43840/60000 (73%)]\t training loss: 0.035680\n",
      "epoch: 9 [44160/60000 (74%)]\t training loss: 0.000518\n",
      "epoch: 9 [44480/60000 (74%)]\t training loss: 0.008683\n",
      "epoch: 9 [44800/60000 (75%)]\t training loss: 0.002349\n",
      "epoch: 9 [45120/60000 (75%)]\t training loss: 0.000096\n",
      "epoch: 9 [45440/60000 (76%)]\t training loss: 0.003251\n",
      "epoch: 9 [45760/60000 (76%)]\t training loss: 0.001847\n",
      "epoch: 9 [46080/60000 (77%)]\t training loss: 0.076838\n",
      "epoch: 9 [46400/60000 (77%)]\t training loss: 0.000765\n",
      "epoch: 9 [46720/60000 (78%)]\t training loss: 0.004689\n",
      "epoch: 9 [47040/60000 (78%)]\t training loss: 0.003056\n",
      "epoch: 9 [47360/60000 (79%)]\t training loss: 0.004135\n",
      "epoch: 9 [47680/60000 (79%)]\t training loss: 0.079560\n",
      "epoch: 9 [48000/60000 (80%)]\t training loss: 0.003295\n",
      "epoch: 9 [48320/60000 (81%)]\t training loss: 0.001943\n",
      "epoch: 9 [48640/60000 (81%)]\t training loss: 0.067679\n",
      "epoch: 9 [48960/60000 (82%)]\t training loss: 0.013191\n",
      "epoch: 9 [49280/60000 (82%)]\t training loss: 0.000029\n",
      "epoch: 9 [49600/60000 (83%)]\t training loss: 0.008306\n",
      "epoch: 9 [49920/60000 (83%)]\t training loss: 0.032536\n",
      "epoch: 9 [50240/60000 (84%)]\t training loss: 0.039851\n",
      "epoch: 9 [50560/60000 (84%)]\t training loss: 0.076758\n",
      "epoch: 9 [50880/60000 (85%)]\t training loss: 0.003067\n",
      "epoch: 9 [51200/60000 (85%)]\t training loss: 0.000146\n",
      "epoch: 9 [51520/60000 (86%)]\t training loss: 0.064434\n",
      "epoch: 9 [51840/60000 (86%)]\t training loss: 0.003694\n",
      "epoch: 9 [52160/60000 (87%)]\t training loss: 0.001771\n",
      "epoch: 9 [52480/60000 (87%)]\t training loss: 0.002302\n",
      "epoch: 9 [52800/60000 (88%)]\t training loss: 0.344408\n",
      "epoch: 9 [53120/60000 (89%)]\t training loss: 0.000976\n",
      "epoch: 9 [53440/60000 (89%)]\t training loss: 0.000477\n",
      "epoch: 9 [53760/60000 (90%)]\t training loss: 0.004217\n",
      "epoch: 9 [54080/60000 (90%)]\t training loss: 0.029490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 [54400/60000 (91%)]\t training loss: 0.001804\n",
      "epoch: 9 [54720/60000 (91%)]\t training loss: 0.413482\n",
      "epoch: 9 [55040/60000 (92%)]\t training loss: 0.001748\n",
      "epoch: 9 [55360/60000 (92%)]\t training loss: 0.001180\n",
      "epoch: 9 [55680/60000 (93%)]\t training loss: 0.000795\n",
      "epoch: 9 [56000/60000 (93%)]\t training loss: 0.017700\n",
      "epoch: 9 [56320/60000 (94%)]\t training loss: 0.058994\n",
      "epoch: 9 [56640/60000 (94%)]\t training loss: 0.039552\n",
      "epoch: 9 [56960/60000 (95%)]\t training loss: 0.000387\n",
      "epoch: 9 [57280/60000 (95%)]\t training loss: 0.005814\n",
      "epoch: 9 [57600/60000 (96%)]\t training loss: 0.009179\n",
      "epoch: 9 [57920/60000 (97%)]\t training loss: 0.012260\n",
      "epoch: 9 [58240/60000 (97%)]\t training loss: 0.000373\n",
      "epoch: 9 [58560/60000 (98%)]\t training loss: 0.319906\n",
      "epoch: 9 [58880/60000 (98%)]\t training loss: 0.476627\n",
      "epoch: 9 [59200/60000 (99%)]\t training loss: 0.006612\n",
      "epoch: 9 [59520/60000 (99%)]\t training loss: 0.003953\n",
      "epoch: 9 [59840/60000 (100%)]\t training loss: 0.004593\n",
      "\n",
      "Test dataset: Overall Loss: 0.0391, Overall Accuracy: 9896/10000 (99%)\n",
      "\n",
      "epoch: 10 [0/60000 (0%)]\t training loss: 0.000016\n",
      "epoch: 10 [320/60000 (1%)]\t training loss: 0.000432\n",
      "epoch: 10 [640/60000 (1%)]\t training loss: 0.001172\n",
      "epoch: 10 [960/60000 (2%)]\t training loss: 0.000741\n",
      "epoch: 10 [1280/60000 (2%)]\t training loss: 0.030607\n",
      "epoch: 10 [1600/60000 (3%)]\t training loss: 0.005879\n",
      "epoch: 10 [1920/60000 (3%)]\t training loss: 0.145667\n",
      "epoch: 10 [2240/60000 (4%)]\t training loss: 0.001517\n",
      "epoch: 10 [2560/60000 (4%)]\t training loss: 0.003138\n",
      "epoch: 10 [2880/60000 (5%)]\t training loss: 0.000698\n",
      "epoch: 10 [3200/60000 (5%)]\t training loss: 0.007281\n",
      "epoch: 10 [3520/60000 (6%)]\t training loss: 0.007603\n",
      "epoch: 10 [3840/60000 (6%)]\t training loss: 0.004331\n",
      "epoch: 10 [4160/60000 (7%)]\t training loss: 0.013266\n",
      "epoch: 10 [4480/60000 (7%)]\t training loss: 0.009134\n",
      "epoch: 10 [4800/60000 (8%)]\t training loss: 0.007018\n",
      "epoch: 10 [5120/60000 (9%)]\t training loss: 0.081350\n",
      "epoch: 10 [5440/60000 (9%)]\t training loss: 0.006087\n",
      "epoch: 10 [5760/60000 (10%)]\t training loss: 0.005891\n",
      "epoch: 10 [6080/60000 (10%)]\t training loss: 0.002203\n",
      "epoch: 10 [6400/60000 (11%)]\t training loss: 0.040389\n",
      "epoch: 10 [6720/60000 (11%)]\t training loss: 0.000559\n",
      "epoch: 10 [7040/60000 (12%)]\t training loss: 0.008355\n",
      "epoch: 10 [7360/60000 (12%)]\t training loss: 0.000386\n",
      "epoch: 10 [7680/60000 (13%)]\t training loss: 0.003670\n",
      "epoch: 10 [8000/60000 (13%)]\t training loss: 0.012805\n",
      "epoch: 10 [8320/60000 (14%)]\t training loss: 0.006683\n",
      "epoch: 10 [8640/60000 (14%)]\t training loss: 0.274988\n",
      "epoch: 10 [8960/60000 (15%)]\t training loss: 0.018989\n",
      "epoch: 10 [9280/60000 (15%)]\t training loss: 0.001198\n",
      "epoch: 10 [9600/60000 (16%)]\t training loss: 0.014401\n",
      "epoch: 10 [9920/60000 (17%)]\t training loss: 0.018387\n",
      "epoch: 10 [10240/60000 (17%)]\t training loss: 0.013713\n",
      "epoch: 10 [10560/60000 (18%)]\t training loss: 0.000166\n",
      "epoch: 10 [10880/60000 (18%)]\t training loss: 0.002367\n",
      "epoch: 10 [11200/60000 (19%)]\t training loss: 0.003246\n",
      "epoch: 10 [11520/60000 (19%)]\t training loss: 0.000624\n",
      "epoch: 10 [11840/60000 (20%)]\t training loss: 0.027893\n",
      "epoch: 10 [12160/60000 (20%)]\t training loss: 0.019566\n",
      "epoch: 10 [12480/60000 (21%)]\t training loss: 0.010461\n",
      "epoch: 10 [12800/60000 (21%)]\t training loss: 0.034486\n",
      "epoch: 10 [13120/60000 (22%)]\t training loss: 0.001257\n",
      "epoch: 10 [13440/60000 (22%)]\t training loss: 0.008074\n",
      "epoch: 10 [13760/60000 (23%)]\t training loss: 0.003683\n",
      "epoch: 10 [14080/60000 (23%)]\t training loss: 0.006309\n",
      "epoch: 10 [14400/60000 (24%)]\t training loss: 0.002172\n",
      "epoch: 10 [14720/60000 (25%)]\t training loss: 0.015710\n",
      "epoch: 10 [15040/60000 (25%)]\t training loss: 0.016745\n",
      "epoch: 10 [15360/60000 (26%)]\t training loss: 0.175726\n",
      "epoch: 10 [15680/60000 (26%)]\t training loss: 0.006997\n",
      "epoch: 10 [16000/60000 (27%)]\t training loss: 0.003176\n",
      "epoch: 10 [16320/60000 (27%)]\t training loss: 0.079979\n",
      "epoch: 10 [16640/60000 (28%)]\t training loss: 0.172257\n",
      "epoch: 10 [16960/60000 (28%)]\t training loss: 0.016768\n",
      "epoch: 10 [17280/60000 (29%)]\t training loss: 0.037659\n",
      "epoch: 10 [17600/60000 (29%)]\t training loss: 0.001952\n",
      "epoch: 10 [17920/60000 (30%)]\t training loss: 0.031364\n",
      "epoch: 10 [18240/60000 (30%)]\t training loss: 0.019095\n",
      "epoch: 10 [18560/60000 (31%)]\t training loss: 0.002141\n",
      "epoch: 10 [18880/60000 (31%)]\t training loss: 0.031858\n",
      "epoch: 10 [19200/60000 (32%)]\t training loss: 0.013245\n",
      "epoch: 10 [19520/60000 (33%)]\t training loss: 0.015320\n",
      "epoch: 10 [19840/60000 (33%)]\t training loss: 0.108590\n",
      "epoch: 10 [20160/60000 (34%)]\t training loss: 0.000553\n",
      "epoch: 10 [20480/60000 (34%)]\t training loss: 0.000782\n",
      "epoch: 10 [20800/60000 (35%)]\t training loss: 0.000260\n",
      "epoch: 10 [21120/60000 (35%)]\t training loss: 0.001735\n",
      "epoch: 10 [21440/60000 (36%)]\t training loss: 0.202086\n",
      "epoch: 10 [21760/60000 (36%)]\t training loss: 0.011041\n",
      "epoch: 10 [22080/60000 (37%)]\t training loss: 0.017610\n",
      "epoch: 10 [22400/60000 (37%)]\t training loss: 0.075219\n",
      "epoch: 10 [22720/60000 (38%)]\t training loss: 0.000070\n",
      "epoch: 10 [23040/60000 (38%)]\t training loss: 0.477873\n",
      "epoch: 10 [23360/60000 (39%)]\t training loss: 0.006096\n",
      "epoch: 10 [23680/60000 (39%)]\t training loss: 0.000129\n",
      "epoch: 10 [24000/60000 (40%)]\t training loss: 0.002840\n",
      "epoch: 10 [24320/60000 (41%)]\t training loss: 0.000183\n",
      "epoch: 10 [24640/60000 (41%)]\t training loss: 0.000061\n",
      "epoch: 10 [24960/60000 (42%)]\t training loss: 0.002375\n",
      "epoch: 10 [25280/60000 (42%)]\t training loss: 0.023008\n",
      "epoch: 10 [25600/60000 (43%)]\t training loss: 0.001412\n",
      "epoch: 10 [25920/60000 (43%)]\t training loss: 0.023242\n",
      "epoch: 10 [26240/60000 (44%)]\t training loss: 0.000416\n",
      "epoch: 10 [26560/60000 (44%)]\t training loss: 0.232751\n",
      "epoch: 10 [26880/60000 (45%)]\t training loss: 0.021445\n",
      "epoch: 10 [27200/60000 (45%)]\t training loss: 0.002534\n",
      "epoch: 10 [27520/60000 (46%)]\t training loss: 0.097698\n",
      "epoch: 10 [27840/60000 (46%)]\t training loss: 0.125014\n",
      "epoch: 10 [28160/60000 (47%)]\t training loss: 0.000089\n",
      "epoch: 10 [28480/60000 (47%)]\t training loss: 0.000004\n",
      "epoch: 10 [28800/60000 (48%)]\t training loss: 0.001630\n",
      "epoch: 10 [29120/60000 (49%)]\t training loss: 0.000317\n",
      "epoch: 10 [29440/60000 (49%)]\t training loss: 0.365385\n",
      "epoch: 10 [29760/60000 (50%)]\t training loss: 0.000462\n",
      "epoch: 10 [30080/60000 (50%)]\t training loss: 0.000911\n",
      "epoch: 10 [30400/60000 (51%)]\t training loss: 0.000412\n",
      "epoch: 10 [30720/60000 (51%)]\t training loss: 0.000018\n",
      "epoch: 10 [31040/60000 (52%)]\t training loss: 0.007270\n",
      "epoch: 10 [31360/60000 (52%)]\t training loss: 0.063534\n",
      "epoch: 10 [31680/60000 (53%)]\t training loss: 0.000850\n",
      "epoch: 10 [32000/60000 (53%)]\t training loss: 0.101126\n",
      "epoch: 10 [32320/60000 (54%)]\t training loss: 0.037336\n",
      "epoch: 10 [32640/60000 (54%)]\t training loss: 0.000330\n",
      "epoch: 10 [32960/60000 (55%)]\t training loss: 0.001618\n",
      "epoch: 10 [33280/60000 (55%)]\t training loss: 0.030759\n",
      "epoch: 10 [33600/60000 (56%)]\t training loss: 0.010493\n",
      "epoch: 10 [33920/60000 (57%)]\t training loss: 0.000703\n",
      "epoch: 10 [34240/60000 (57%)]\t training loss: 0.000576\n",
      "epoch: 10 [34560/60000 (58%)]\t training loss: 0.022697\n",
      "epoch: 10 [34880/60000 (58%)]\t training loss: 0.021534\n",
      "epoch: 10 [35200/60000 (59%)]\t training loss: 0.020317\n",
      "epoch: 10 [35520/60000 (59%)]\t training loss: 0.235750\n",
      "epoch: 10 [35840/60000 (60%)]\t training loss: 0.000099\n",
      "epoch: 10 [36160/60000 (60%)]\t training loss: 0.000150\n",
      "epoch: 10 [36480/60000 (61%)]\t training loss: 0.010955\n",
      "epoch: 10 [36800/60000 (61%)]\t training loss: 0.017196\n",
      "epoch: 10 [37120/60000 (62%)]\t training loss: 0.034838\n",
      "epoch: 10 [37440/60000 (62%)]\t training loss: 0.013196\n",
      "epoch: 10 [37760/60000 (63%)]\t training loss: 0.084000\n",
      "epoch: 10 [38080/60000 (63%)]\t training loss: 0.002489\n",
      "epoch: 10 [38400/60000 (64%)]\t training loss: 0.001114\n",
      "epoch: 10 [38720/60000 (65%)]\t training loss: 0.007292\n",
      "epoch: 10 [39040/60000 (65%)]\t training loss: 0.001069\n",
      "epoch: 10 [39360/60000 (66%)]\t training loss: 0.001542\n",
      "epoch: 10 [39680/60000 (66%)]\t training loss: 0.066281\n",
      "epoch: 10 [40000/60000 (67%)]\t training loss: 0.015830\n",
      "epoch: 10 [40320/60000 (67%)]\t training loss: 0.000341\n",
      "epoch: 10 [40640/60000 (68%)]\t training loss: 0.000265\n",
      "epoch: 10 [40960/60000 (68%)]\t training loss: 0.006657\n",
      "epoch: 10 [41280/60000 (69%)]\t training loss: 0.007413\n",
      "epoch: 10 [41600/60000 (69%)]\t training loss: 0.002022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 [41920/60000 (70%)]\t training loss: 0.004206\n",
      "epoch: 10 [42240/60000 (70%)]\t training loss: 0.000332\n",
      "epoch: 10 [42560/60000 (71%)]\t training loss: 0.011507\n",
      "epoch: 10 [42880/60000 (71%)]\t training loss: 0.021877\n",
      "epoch: 10 [43200/60000 (72%)]\t training loss: 0.011267\n",
      "epoch: 10 [43520/60000 (73%)]\t training loss: 0.000143\n",
      "epoch: 10 [43840/60000 (73%)]\t training loss: 0.008552\n",
      "epoch: 10 [44160/60000 (74%)]\t training loss: 0.002033\n",
      "epoch: 10 [44480/60000 (74%)]\t training loss: 0.049765\n",
      "epoch: 10 [44800/60000 (75%)]\t training loss: 0.026754\n",
      "epoch: 10 [45120/60000 (75%)]\t training loss: 0.000078\n",
      "epoch: 10 [45440/60000 (76%)]\t training loss: 0.001833\n",
      "epoch: 10 [45760/60000 (76%)]\t training loss: 0.050023\n",
      "epoch: 10 [46080/60000 (77%)]\t training loss: 0.013707\n",
      "epoch: 10 [46400/60000 (77%)]\t training loss: 0.006084\n",
      "epoch: 10 [46720/60000 (78%)]\t training loss: 0.011084\n",
      "epoch: 10 [47040/60000 (78%)]\t training loss: 0.055343\n",
      "epoch: 10 [47360/60000 (79%)]\t training loss: 0.016644\n",
      "epoch: 10 [47680/60000 (79%)]\t training loss: 0.000712\n",
      "epoch: 10 [48000/60000 (80%)]\t training loss: 0.026771\n",
      "epoch: 10 [48320/60000 (81%)]\t training loss: 0.073966\n",
      "epoch: 10 [48640/60000 (81%)]\t training loss: 0.010189\n",
      "epoch: 10 [48960/60000 (82%)]\t training loss: 0.026138\n",
      "epoch: 10 [49280/60000 (82%)]\t training loss: 0.218797\n",
      "epoch: 10 [49600/60000 (83%)]\t training loss: 0.073191\n",
      "epoch: 10 [49920/60000 (83%)]\t training loss: 0.128420\n",
      "epoch: 10 [50240/60000 (84%)]\t training loss: 0.000189\n",
      "epoch: 10 [50560/60000 (84%)]\t training loss: 0.000618\n",
      "epoch: 10 [50880/60000 (85%)]\t training loss: 0.001085\n",
      "epoch: 10 [51200/60000 (85%)]\t training loss: 0.000334\n",
      "epoch: 10 [51520/60000 (86%)]\t training loss: 0.005928\n",
      "epoch: 10 [51840/60000 (86%)]\t training loss: 0.004953\n",
      "epoch: 10 [52160/60000 (87%)]\t training loss: 0.281651\n",
      "epoch: 10 [52480/60000 (87%)]\t training loss: 0.008181\n",
      "epoch: 10 [52800/60000 (88%)]\t training loss: 0.024093\n",
      "epoch: 10 [53120/60000 (89%)]\t training loss: 0.067759\n",
      "epoch: 10 [53440/60000 (89%)]\t training loss: 0.002049\n",
      "epoch: 10 [53760/60000 (90%)]\t training loss: 0.005514\n",
      "epoch: 10 [54080/60000 (90%)]\t training loss: 0.045023\n",
      "epoch: 10 [54400/60000 (91%)]\t training loss: 0.000404\n",
      "epoch: 10 [54720/60000 (91%)]\t training loss: 0.004219\n",
      "epoch: 10 [55040/60000 (92%)]\t training loss: 0.003574\n",
      "epoch: 10 [55360/60000 (92%)]\t training loss: 0.006942\n",
      "epoch: 10 [55680/60000 (93%)]\t training loss: 0.000521\n",
      "epoch: 10 [56000/60000 (93%)]\t training loss: 0.000019\n",
      "epoch: 10 [56320/60000 (94%)]\t training loss: 0.006446\n",
      "epoch: 10 [56640/60000 (94%)]\t training loss: 0.009493\n",
      "epoch: 10 [56960/60000 (95%)]\t training loss: 0.004045\n",
      "epoch: 10 [57280/60000 (95%)]\t training loss: 0.027255\n",
      "epoch: 10 [57600/60000 (96%)]\t training loss: 0.000882\n",
      "epoch: 10 [57920/60000 (97%)]\t training loss: 0.022186\n",
      "epoch: 10 [58240/60000 (97%)]\t training loss: 0.002121\n",
      "epoch: 10 [58560/60000 (98%)]\t training loss: 0.002499\n",
      "epoch: 10 [58880/60000 (98%)]\t training loss: 0.001027\n",
      "epoch: 10 [59200/60000 (99%)]\t training loss: 0.002626\n",
      "epoch: 10 [59520/60000 (99%)]\t training loss: 0.000024\n",
      "epoch: 10 [59840/60000 (100%)]\t training loss: 0.000354\n",
      "\n",
      "Test dataset: Overall Loss: 0.0379, Overall Accuracy: 9899/10000 (99%)\n",
      "\n",
      "epoch: 11 [0/60000 (0%)]\t training loss: 0.007651\n",
      "epoch: 11 [320/60000 (1%)]\t training loss: 0.094671\n",
      "epoch: 11 [640/60000 (1%)]\t training loss: 0.273119\n",
      "epoch: 11 [960/60000 (2%)]\t training loss: 0.003666\n",
      "epoch: 11 [1280/60000 (2%)]\t training loss: 0.000672\n",
      "epoch: 11 [1600/60000 (3%)]\t training loss: 0.000055\n",
      "epoch: 11 [1920/60000 (3%)]\t training loss: 0.053887\n",
      "epoch: 11 [2240/60000 (4%)]\t training loss: 0.062433\n",
      "epoch: 11 [2560/60000 (4%)]\t training loss: 0.000515\n",
      "epoch: 11 [2880/60000 (5%)]\t training loss: 0.053730\n",
      "epoch: 11 [3200/60000 (5%)]\t training loss: 0.003708\n",
      "epoch: 11 [3520/60000 (6%)]\t training loss: 0.000995\n",
      "epoch: 11 [3840/60000 (6%)]\t training loss: 0.001882\n",
      "epoch: 11 [4160/60000 (7%)]\t training loss: 0.000495\n",
      "epoch: 11 [4480/60000 (7%)]\t training loss: 0.004123\n",
      "epoch: 11 [4800/60000 (8%)]\t training loss: 0.000236\n",
      "epoch: 11 [5120/60000 (9%)]\t training loss: 0.001428\n",
      "epoch: 11 [5440/60000 (9%)]\t training loss: 0.005242\n",
      "epoch: 11 [5760/60000 (10%)]\t training loss: 0.001025\n",
      "epoch: 11 [6080/60000 (10%)]\t training loss: 0.005158\n",
      "epoch: 11 [6400/60000 (11%)]\t training loss: 0.282588\n",
      "epoch: 11 [6720/60000 (11%)]\t training loss: 0.000228\n",
      "epoch: 11 [7040/60000 (12%)]\t training loss: 0.001146\n",
      "epoch: 11 [7360/60000 (12%)]\t training loss: 0.019109\n",
      "epoch: 11 [7680/60000 (13%)]\t training loss: 0.017954\n",
      "epoch: 11 [8000/60000 (13%)]\t training loss: 0.009122\n",
      "epoch: 11 [8320/60000 (14%)]\t training loss: 0.018314\n",
      "epoch: 11 [8640/60000 (14%)]\t training loss: 0.000519\n",
      "epoch: 11 [8960/60000 (15%)]\t training loss: 0.000111\n",
      "epoch: 11 [9280/60000 (15%)]\t training loss: 0.000277\n",
      "epoch: 11 [9600/60000 (16%)]\t training loss: 0.000192\n",
      "epoch: 11 [9920/60000 (17%)]\t training loss: 0.001802\n",
      "epoch: 11 [10240/60000 (17%)]\t training loss: 0.000499\n",
      "epoch: 11 [10560/60000 (18%)]\t training loss: 0.023407\n",
      "epoch: 11 [10880/60000 (18%)]\t training loss: 0.011344\n",
      "epoch: 11 [11200/60000 (19%)]\t training loss: 0.000009\n",
      "epoch: 11 [11520/60000 (19%)]\t training loss: 0.000324\n",
      "epoch: 11 [11840/60000 (20%)]\t training loss: 0.024745\n",
      "epoch: 11 [12160/60000 (20%)]\t training loss: 0.000223\n",
      "epoch: 11 [12480/60000 (21%)]\t training loss: 0.107970\n",
      "epoch: 11 [12800/60000 (21%)]\t training loss: 0.001016\n",
      "epoch: 11 [13120/60000 (22%)]\t training loss: 0.000030\n",
      "epoch: 11 [13440/60000 (22%)]\t training loss: 0.002701\n",
      "epoch: 11 [13760/60000 (23%)]\t training loss: 0.013054\n",
      "epoch: 11 [14080/60000 (23%)]\t training loss: 0.057469\n",
      "epoch: 11 [14400/60000 (24%)]\t training loss: 0.018147\n",
      "epoch: 11 [14720/60000 (25%)]\t training loss: 0.009651\n",
      "epoch: 11 [15040/60000 (25%)]\t training loss: 0.088506\n",
      "epoch: 11 [15360/60000 (26%)]\t training loss: 0.000686\n",
      "epoch: 11 [15680/60000 (26%)]\t training loss: 0.004113\n",
      "epoch: 11 [16000/60000 (27%)]\t training loss: 0.000835\n",
      "epoch: 11 [16320/60000 (27%)]\t training loss: 0.195955\n",
      "epoch: 11 [16640/60000 (28%)]\t training loss: 0.008744\n",
      "epoch: 11 [16960/60000 (28%)]\t training loss: 0.056388\n",
      "epoch: 11 [17280/60000 (29%)]\t training loss: 0.000077\n",
      "epoch: 11 [17600/60000 (29%)]\t training loss: 0.112404\n",
      "epoch: 11 [17920/60000 (30%)]\t training loss: 0.002180\n",
      "epoch: 11 [18240/60000 (30%)]\t training loss: 0.000370\n",
      "epoch: 11 [18560/60000 (31%)]\t training loss: 0.005174\n",
      "epoch: 11 [18880/60000 (31%)]\t training loss: 0.026362\n",
      "epoch: 11 [19200/60000 (32%)]\t training loss: 0.003659\n",
      "epoch: 11 [19520/60000 (33%)]\t training loss: 0.090889\n",
      "epoch: 11 [19840/60000 (33%)]\t training loss: 0.022906\n",
      "epoch: 11 [20160/60000 (34%)]\t training loss: 0.070573\n",
      "epoch: 11 [20480/60000 (34%)]\t training loss: 0.006135\n",
      "epoch: 11 [20800/60000 (35%)]\t training loss: 0.002366\n",
      "epoch: 11 [21120/60000 (35%)]\t training loss: 0.004124\n",
      "epoch: 11 [21440/60000 (36%)]\t training loss: 0.000802\n",
      "epoch: 11 [21760/60000 (36%)]\t training loss: 0.000390\n",
      "epoch: 11 [22080/60000 (37%)]\t training loss: 0.000453\n",
      "epoch: 11 [22400/60000 (37%)]\t training loss: 0.000385\n",
      "epoch: 11 [22720/60000 (38%)]\t training loss: 0.000093\n",
      "epoch: 11 [23040/60000 (38%)]\t training loss: 0.010669\n",
      "epoch: 11 [23360/60000 (39%)]\t training loss: 0.189495\n",
      "epoch: 11 [23680/60000 (39%)]\t training loss: 0.048235\n",
      "epoch: 11 [24000/60000 (40%)]\t training loss: 0.000471\n",
      "epoch: 11 [24320/60000 (41%)]\t training loss: 0.000023\n",
      "epoch: 11 [24640/60000 (41%)]\t training loss: 0.283650\n",
      "epoch: 11 [24960/60000 (42%)]\t training loss: 0.006178\n",
      "epoch: 11 [25280/60000 (42%)]\t training loss: 0.000519\n",
      "epoch: 11 [25600/60000 (43%)]\t training loss: 0.011354\n",
      "epoch: 11 [25920/60000 (43%)]\t training loss: 0.000389\n",
      "epoch: 11 [26240/60000 (44%)]\t training loss: 0.019609\n",
      "epoch: 11 [26560/60000 (44%)]\t training loss: 0.001426\n",
      "epoch: 11 [26880/60000 (45%)]\t training loss: 0.008415\n",
      "epoch: 11 [27200/60000 (45%)]\t training loss: 0.056493\n",
      "epoch: 11 [27520/60000 (46%)]\t training loss: 0.001879\n",
      "epoch: 11 [27840/60000 (46%)]\t training loss: 0.000264\n",
      "epoch: 11 [28160/60000 (47%)]\t training loss: 0.036221\n",
      "epoch: 11 [28480/60000 (47%)]\t training loss: 0.002527\n",
      "epoch: 11 [28800/60000 (48%)]\t training loss: 0.021987\n",
      "epoch: 11 [29120/60000 (49%)]\t training loss: 0.000191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 [29440/60000 (49%)]\t training loss: 0.001402\n",
      "epoch: 11 [29760/60000 (50%)]\t training loss: 0.003551\n",
      "epoch: 11 [30080/60000 (50%)]\t training loss: 0.000440\n",
      "epoch: 11 [30400/60000 (51%)]\t training loss: 0.002321\n",
      "epoch: 11 [30720/60000 (51%)]\t training loss: 0.000593\n",
      "epoch: 11 [31040/60000 (52%)]\t training loss: 0.000389\n",
      "epoch: 11 [31360/60000 (52%)]\t training loss: 0.000005\n",
      "epoch: 11 [31680/60000 (53%)]\t training loss: 0.001654\n",
      "epoch: 11 [32000/60000 (53%)]\t training loss: 0.000461\n",
      "epoch: 11 [32320/60000 (54%)]\t training loss: 0.008467\n",
      "epoch: 11 [32640/60000 (54%)]\t training loss: 0.002843\n",
      "epoch: 11 [32960/60000 (55%)]\t training loss: 0.022912\n",
      "epoch: 11 [33280/60000 (55%)]\t training loss: 0.000316\n",
      "epoch: 11 [33600/60000 (56%)]\t training loss: 0.010621\n",
      "epoch: 11 [33920/60000 (57%)]\t training loss: 0.003163\n",
      "epoch: 11 [34240/60000 (57%)]\t training loss: 0.000689\n",
      "epoch: 11 [34560/60000 (58%)]\t training loss: 0.024799\n",
      "epoch: 11 [34880/60000 (58%)]\t training loss: 0.004735\n",
      "epoch: 11 [35200/60000 (59%)]\t training loss: 0.000070\n",
      "epoch: 11 [35520/60000 (59%)]\t training loss: 0.000309\n",
      "epoch: 11 [35840/60000 (60%)]\t training loss: 0.023072\n",
      "epoch: 11 [36160/60000 (60%)]\t training loss: 0.026664\n",
      "epoch: 11 [36480/60000 (61%)]\t training loss: 0.000318\n",
      "epoch: 11 [36800/60000 (61%)]\t training loss: 0.000690\n",
      "epoch: 11 [37120/60000 (62%)]\t training loss: 0.012274\n",
      "epoch: 11 [37440/60000 (62%)]\t training loss: 0.002450\n",
      "epoch: 11 [37760/60000 (63%)]\t training loss: 0.017357\n",
      "epoch: 11 [38080/60000 (63%)]\t training loss: 0.007942\n",
      "epoch: 11 [38400/60000 (64%)]\t training loss: 0.005382\n",
      "epoch: 11 [38720/60000 (65%)]\t training loss: 0.064945\n",
      "epoch: 11 [39040/60000 (65%)]\t training loss: 0.001481\n",
      "epoch: 11 [39360/60000 (66%)]\t training loss: 0.032853\n",
      "epoch: 11 [39680/60000 (66%)]\t training loss: 0.014961\n",
      "epoch: 11 [40000/60000 (67%)]\t training loss: 0.187347\n",
      "epoch: 11 [40320/60000 (67%)]\t training loss: 0.000569\n",
      "epoch: 11 [40640/60000 (68%)]\t training loss: 0.209918\n",
      "epoch: 11 [40960/60000 (68%)]\t training loss: 0.103601\n",
      "epoch: 11 [41280/60000 (69%)]\t training loss: 0.032233\n",
      "epoch: 11 [41600/60000 (69%)]\t training loss: 0.052587\n",
      "epoch: 11 [41920/60000 (70%)]\t training loss: 0.349887\n",
      "epoch: 11 [42240/60000 (70%)]\t training loss: 0.001864\n",
      "epoch: 11 [42560/60000 (71%)]\t training loss: 0.002165\n",
      "epoch: 11 [42880/60000 (71%)]\t training loss: 0.005186\n",
      "epoch: 11 [43200/60000 (72%)]\t training loss: 0.010435\n",
      "epoch: 11 [43520/60000 (73%)]\t training loss: 0.082989\n",
      "epoch: 11 [43840/60000 (73%)]\t training loss: 0.087676\n",
      "epoch: 11 [44160/60000 (74%)]\t training loss: 0.097143\n",
      "epoch: 11 [44480/60000 (74%)]\t training loss: 0.005203\n",
      "epoch: 11 [44800/60000 (75%)]\t training loss: 0.003166\n",
      "epoch: 11 [45120/60000 (75%)]\t training loss: 0.000015\n",
      "epoch: 11 [45440/60000 (76%)]\t training loss: 0.002452\n",
      "epoch: 11 [45760/60000 (76%)]\t training loss: 0.000002\n",
      "epoch: 11 [46080/60000 (77%)]\t training loss: 0.000247\n",
      "epoch: 11 [46400/60000 (77%)]\t training loss: 0.001769\n",
      "epoch: 11 [46720/60000 (78%)]\t training loss: 0.000665\n",
      "epoch: 11 [47040/60000 (78%)]\t training loss: 0.000051\n",
      "epoch: 11 [47360/60000 (79%)]\t training loss: 0.000022\n",
      "epoch: 11 [47680/60000 (79%)]\t training loss: 0.000047\n",
      "epoch: 11 [48000/60000 (80%)]\t training loss: 0.141616\n",
      "epoch: 11 [48320/60000 (81%)]\t training loss: 0.002789\n",
      "epoch: 11 [48640/60000 (81%)]\t training loss: 0.013389\n",
      "epoch: 11 [48960/60000 (82%)]\t training loss: 0.004135\n",
      "epoch: 11 [49280/60000 (82%)]\t training loss: 0.000296\n",
      "epoch: 11 [49600/60000 (83%)]\t training loss: 0.000050\n",
      "epoch: 11 [49920/60000 (83%)]\t training loss: 0.008739\n",
      "epoch: 11 [50240/60000 (84%)]\t training loss: 0.002528\n",
      "epoch: 11 [50560/60000 (84%)]\t training loss: 0.051340\n",
      "epoch: 11 [50880/60000 (85%)]\t training loss: 0.027588\n",
      "epoch: 11 [51200/60000 (85%)]\t training loss: 0.004626\n",
      "epoch: 11 [51520/60000 (86%)]\t training loss: 0.002001\n",
      "epoch: 11 [51840/60000 (86%)]\t training loss: 0.000086\n",
      "epoch: 11 [52160/60000 (87%)]\t training loss: 0.000045\n",
      "epoch: 11 [52480/60000 (87%)]\t training loss: 0.003958\n",
      "epoch: 11 [52800/60000 (88%)]\t training loss: 0.000286\n",
      "epoch: 11 [53120/60000 (89%)]\t training loss: 0.012395\n",
      "epoch: 11 [53440/60000 (89%)]\t training loss: 0.001658\n",
      "epoch: 11 [53760/60000 (90%)]\t training loss: 0.000701\n",
      "epoch: 11 [54080/60000 (90%)]\t training loss: 0.069956\n",
      "epoch: 11 [54400/60000 (91%)]\t training loss: 0.000510\n",
      "epoch: 11 [54720/60000 (91%)]\t training loss: 0.001102\n",
      "epoch: 11 [55040/60000 (92%)]\t training loss: 0.065752\n",
      "epoch: 11 [55360/60000 (92%)]\t training loss: 0.206675\n",
      "epoch: 11 [55680/60000 (93%)]\t training loss: 0.165983\n",
      "epoch: 11 [56000/60000 (93%)]\t training loss: 0.000008\n",
      "epoch: 11 [56320/60000 (94%)]\t training loss: 0.000080\n",
      "epoch: 11 [56640/60000 (94%)]\t training loss: 0.000829\n",
      "epoch: 11 [56960/60000 (95%)]\t training loss: 0.003184\n",
      "epoch: 11 [57280/60000 (95%)]\t training loss: 0.159929\n",
      "epoch: 11 [57600/60000 (96%)]\t training loss: 0.009645\n",
      "epoch: 11 [57920/60000 (97%)]\t training loss: 0.006363\n",
      "epoch: 11 [58240/60000 (97%)]\t training loss: 0.007735\n",
      "epoch: 11 [58560/60000 (98%)]\t training loss: 0.036496\n",
      "epoch: 11 [58880/60000 (98%)]\t training loss: 0.013878\n",
      "epoch: 11 [59200/60000 (99%)]\t training loss: 0.000062\n",
      "epoch: 11 [59520/60000 (99%)]\t training loss: 0.108138\n",
      "epoch: 11 [59840/60000 (100%)]\t training loss: 0.005889\n",
      "\n",
      "Test dataset: Overall Loss: 0.0344, Overall Accuracy: 9902/10000 (99%)\n",
      "\n",
      "epoch: 12 [0/60000 (0%)]\t training loss: 0.004454\n",
      "epoch: 12 [320/60000 (1%)]\t training loss: 0.000386\n",
      "epoch: 12 [640/60000 (1%)]\t training loss: 0.366068\n",
      "epoch: 12 [960/60000 (2%)]\t training loss: 0.002804\n",
      "epoch: 12 [1280/60000 (2%)]\t training loss: 0.000125\n",
      "epoch: 12 [1600/60000 (3%)]\t training loss: 0.006586\n",
      "epoch: 12 [1920/60000 (3%)]\t training loss: 0.008760\n",
      "epoch: 12 [2240/60000 (4%)]\t training loss: 0.036572\n",
      "epoch: 12 [2560/60000 (4%)]\t training loss: 0.001148\n",
      "epoch: 12 [2880/60000 (5%)]\t training loss: 0.011736\n",
      "epoch: 12 [3200/60000 (5%)]\t training loss: 0.004799\n",
      "epoch: 12 [3520/60000 (6%)]\t training loss: 0.001195\n",
      "epoch: 12 [3840/60000 (6%)]\t training loss: 0.000048\n",
      "epoch: 12 [4160/60000 (7%)]\t training loss: 0.000089\n",
      "epoch: 12 [4480/60000 (7%)]\t training loss: 0.004134\n",
      "epoch: 12 [4800/60000 (8%)]\t training loss: 0.000034\n",
      "epoch: 12 [5120/60000 (9%)]\t training loss: 0.009513\n",
      "epoch: 12 [5440/60000 (9%)]\t training loss: 0.002053\n",
      "epoch: 12 [5760/60000 (10%)]\t training loss: 0.000350\n",
      "epoch: 12 [6080/60000 (10%)]\t training loss: 0.289834\n",
      "epoch: 12 [6400/60000 (11%)]\t training loss: 0.090455\n",
      "epoch: 12 [6720/60000 (11%)]\t training loss: 0.000133\n",
      "epoch: 12 [7040/60000 (12%)]\t training loss: 0.007862\n",
      "epoch: 12 [7360/60000 (12%)]\t training loss: 0.000177\n",
      "epoch: 12 [7680/60000 (13%)]\t training loss: 0.000032\n",
      "epoch: 12 [8000/60000 (13%)]\t training loss: 0.001564\n",
      "epoch: 12 [8320/60000 (14%)]\t training loss: 0.047921\n",
      "epoch: 12 [8640/60000 (14%)]\t training loss: 0.547424\n",
      "epoch: 12 [8960/60000 (15%)]\t training loss: 0.012333\n",
      "epoch: 12 [9280/60000 (15%)]\t training loss: 0.004617\n",
      "epoch: 12 [9600/60000 (16%)]\t training loss: 0.005507\n",
      "epoch: 12 [9920/60000 (17%)]\t training loss: 0.005772\n",
      "epoch: 12 [10240/60000 (17%)]\t training loss: 0.118007\n",
      "epoch: 12 [10560/60000 (18%)]\t training loss: 0.014979\n",
      "epoch: 12 [10880/60000 (18%)]\t training loss: 0.000012\n",
      "epoch: 12 [11200/60000 (19%)]\t training loss: 0.012408\n",
      "epoch: 12 [11520/60000 (19%)]\t training loss: 0.000834\n",
      "epoch: 12 [11840/60000 (20%)]\t training loss: 0.000849\n",
      "epoch: 12 [12160/60000 (20%)]\t training loss: 0.002643\n",
      "epoch: 12 [12480/60000 (21%)]\t training loss: 0.028338\n",
      "epoch: 12 [12800/60000 (21%)]\t training loss: 0.034891\n",
      "epoch: 12 [13120/60000 (22%)]\t training loss: 0.053542\n",
      "epoch: 12 [13440/60000 (22%)]\t training loss: 0.071233\n",
      "epoch: 12 [13760/60000 (23%)]\t training loss: 0.013725\n",
      "epoch: 12 [14080/60000 (23%)]\t training loss: 0.046919\n",
      "epoch: 12 [14400/60000 (24%)]\t training loss: 0.000289\n",
      "epoch: 12 [14720/60000 (25%)]\t training loss: 0.005638\n",
      "epoch: 12 [15040/60000 (25%)]\t training loss: 0.001050\n",
      "epoch: 12 [15360/60000 (26%)]\t training loss: 0.000572\n",
      "epoch: 12 [15680/60000 (26%)]\t training loss: 0.038193\n",
      "epoch: 12 [16000/60000 (27%)]\t training loss: 0.001553\n",
      "epoch: 12 [16320/60000 (27%)]\t training loss: 0.005030\n",
      "epoch: 12 [16640/60000 (28%)]\t training loss: 0.084243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 [16960/60000 (28%)]\t training loss: 0.004352\n",
      "epoch: 12 [17280/60000 (29%)]\t training loss: 0.001743\n",
      "epoch: 12 [17600/60000 (29%)]\t training loss: 0.024402\n",
      "epoch: 12 [17920/60000 (30%)]\t training loss: 0.000130\n",
      "epoch: 12 [18240/60000 (30%)]\t training loss: 0.018512\n",
      "epoch: 12 [18560/60000 (31%)]\t training loss: 0.000407\n",
      "epoch: 12 [18880/60000 (31%)]\t training loss: 0.001205\n",
      "epoch: 12 [19200/60000 (32%)]\t training loss: 0.000448\n",
      "epoch: 12 [19520/60000 (33%)]\t training loss: 0.004479\n",
      "epoch: 12 [19840/60000 (33%)]\t training loss: 0.014971\n",
      "epoch: 12 [20160/60000 (34%)]\t training loss: 0.100124\n",
      "epoch: 12 [20480/60000 (34%)]\t training loss: 0.033371\n",
      "epoch: 12 [20800/60000 (35%)]\t training loss: 0.000263\n",
      "epoch: 12 [21120/60000 (35%)]\t training loss: 0.133752\n",
      "epoch: 12 [21440/60000 (36%)]\t training loss: 0.001126\n",
      "epoch: 12 [21760/60000 (36%)]\t training loss: 0.006031\n",
      "epoch: 12 [22080/60000 (37%)]\t training loss: 0.035033\n",
      "epoch: 12 [22400/60000 (37%)]\t training loss: 0.005241\n",
      "epoch: 12 [22720/60000 (38%)]\t training loss: 0.000036\n",
      "epoch: 12 [23040/60000 (38%)]\t training loss: 0.003929\n",
      "epoch: 12 [23360/60000 (39%)]\t training loss: 0.038004\n",
      "epoch: 12 [23680/60000 (39%)]\t training loss: 0.134388\n",
      "epoch: 12 [24000/60000 (40%)]\t training loss: 0.001469\n",
      "epoch: 12 [24320/60000 (41%)]\t training loss: 0.000191\n",
      "epoch: 12 [24640/60000 (41%)]\t training loss: 0.000499\n",
      "epoch: 12 [24960/60000 (42%)]\t training loss: 0.003815\n",
      "epoch: 12 [25280/60000 (42%)]\t training loss: 0.012548\n",
      "epoch: 12 [25600/60000 (43%)]\t training loss: 0.229195\n",
      "epoch: 12 [25920/60000 (43%)]\t training loss: 0.011175\n",
      "epoch: 12 [26240/60000 (44%)]\t training loss: 0.000451\n",
      "epoch: 12 [26560/60000 (44%)]\t training loss: 0.004946\n",
      "epoch: 12 [26880/60000 (45%)]\t training loss: 0.187917\n",
      "epoch: 12 [27200/60000 (45%)]\t training loss: 0.012377\n",
      "epoch: 12 [27520/60000 (46%)]\t training loss: 0.045533\n",
      "epoch: 12 [27840/60000 (46%)]\t training loss: 0.000639\n",
      "epoch: 12 [28160/60000 (47%)]\t training loss: 0.009059\n",
      "epoch: 12 [28480/60000 (47%)]\t training loss: 0.043573\n",
      "epoch: 12 [28800/60000 (48%)]\t training loss: 0.004421\n",
      "epoch: 12 [29120/60000 (49%)]\t training loss: 0.000302\n",
      "epoch: 12 [29440/60000 (49%)]\t training loss: 0.001839\n",
      "epoch: 12 [29760/60000 (50%)]\t training loss: 0.000030\n",
      "epoch: 12 [30080/60000 (50%)]\t training loss: 0.003637\n",
      "epoch: 12 [30400/60000 (51%)]\t training loss: 0.003687\n",
      "epoch: 12 [30720/60000 (51%)]\t training loss: 0.240436\n",
      "epoch: 12 [31040/60000 (52%)]\t training loss: 0.004197\n",
      "epoch: 12 [31360/60000 (52%)]\t training loss: 0.026198\n",
      "epoch: 12 [31680/60000 (53%)]\t training loss: 0.000091\n",
      "epoch: 12 [32000/60000 (53%)]\t training loss: 0.131733\n",
      "epoch: 12 [32320/60000 (54%)]\t training loss: 0.013145\n",
      "epoch: 12 [32640/60000 (54%)]\t training loss: 0.003224\n",
      "epoch: 12 [32960/60000 (55%)]\t training loss: 0.007779\n",
      "epoch: 12 [33280/60000 (55%)]\t training loss: 0.077668\n",
      "epoch: 12 [33600/60000 (56%)]\t training loss: 0.000789\n",
      "epoch: 12 [33920/60000 (57%)]\t training loss: 0.001627\n",
      "epoch: 12 [34240/60000 (57%)]\t training loss: 0.310807\n",
      "epoch: 12 [34560/60000 (58%)]\t training loss: 0.000098\n",
      "epoch: 12 [34880/60000 (58%)]\t training loss: 0.001916\n",
      "epoch: 12 [35200/60000 (59%)]\t training loss: 0.003721\n",
      "epoch: 12 [35520/60000 (59%)]\t training loss: 0.001438\n",
      "epoch: 12 [35840/60000 (60%)]\t training loss: 0.023145\n",
      "epoch: 12 [36160/60000 (60%)]\t training loss: 0.014219\n",
      "epoch: 12 [36480/60000 (61%)]\t training loss: 0.013917\n",
      "epoch: 12 [36800/60000 (61%)]\t training loss: 0.000163\n",
      "epoch: 12 [37120/60000 (62%)]\t training loss: 0.000149\n",
      "epoch: 12 [37440/60000 (62%)]\t training loss: 0.001159\n",
      "epoch: 12 [37760/60000 (63%)]\t training loss: 0.002032\n",
      "epoch: 12 [38080/60000 (63%)]\t training loss: 0.000435\n",
      "epoch: 12 [38400/60000 (64%)]\t training loss: 0.000882\n",
      "epoch: 12 [38720/60000 (65%)]\t training loss: 0.000246\n",
      "epoch: 12 [39040/60000 (65%)]\t training loss: 0.012434\n",
      "epoch: 12 [39360/60000 (66%)]\t training loss: 0.004990\n",
      "epoch: 12 [39680/60000 (66%)]\t training loss: 0.166954\n",
      "epoch: 12 [40000/60000 (67%)]\t training loss: 0.070042\n",
      "epoch: 12 [40320/60000 (67%)]\t training loss: 0.000139\n",
      "epoch: 12 [40640/60000 (68%)]\t training loss: 0.000195\n",
      "epoch: 12 [40960/60000 (68%)]\t training loss: 0.001778\n",
      "epoch: 12 [41280/60000 (69%)]\t training loss: 0.000398\n",
      "epoch: 12 [41600/60000 (69%)]\t training loss: 0.065791\n",
      "epoch: 12 [41920/60000 (70%)]\t training loss: 0.017798\n",
      "epoch: 12 [42240/60000 (70%)]\t training loss: 0.000141\n",
      "epoch: 12 [42560/60000 (71%)]\t training loss: 0.010705\n",
      "epoch: 12 [42880/60000 (71%)]\t training loss: 0.001141\n",
      "epoch: 12 [43200/60000 (72%)]\t training loss: 0.000912\n",
      "epoch: 12 [43520/60000 (73%)]\t training loss: 0.001255\n",
      "epoch: 12 [43840/60000 (73%)]\t training loss: 0.000757\n",
      "epoch: 12 [44160/60000 (74%)]\t training loss: 0.024156\n",
      "epoch: 12 [44480/60000 (74%)]\t training loss: 0.000480\n",
      "epoch: 12 [44800/60000 (75%)]\t training loss: 0.000237\n",
      "epoch: 12 [45120/60000 (75%)]\t training loss: 0.085673\n",
      "epoch: 12 [45440/60000 (76%)]\t training loss: 0.060955\n",
      "epoch: 12 [45760/60000 (76%)]\t training loss: 0.000399\n",
      "epoch: 12 [46080/60000 (77%)]\t training loss: 0.001816\n",
      "epoch: 12 [46400/60000 (77%)]\t training loss: 0.076960\n",
      "epoch: 12 [46720/60000 (78%)]\t training loss: 0.000748\n",
      "epoch: 12 [47040/60000 (78%)]\t training loss: 0.001550\n",
      "epoch: 12 [47360/60000 (79%)]\t training loss: 0.000578\n",
      "epoch: 12 [47680/60000 (79%)]\t training loss: 0.000102\n",
      "epoch: 12 [48000/60000 (80%)]\t training loss: 0.055944\n",
      "epoch: 12 [48320/60000 (81%)]\t training loss: 0.000494\n",
      "epoch: 12 [48640/60000 (81%)]\t training loss: 0.146795\n",
      "epoch: 12 [48960/60000 (82%)]\t training loss: 0.031496\n",
      "epoch: 12 [49280/60000 (82%)]\t training loss: 0.007835\n",
      "epoch: 12 [49600/60000 (83%)]\t training loss: 0.033531\n",
      "epoch: 12 [49920/60000 (83%)]\t training loss: 0.015850\n",
      "epoch: 12 [50240/60000 (84%)]\t training loss: 0.008921\n",
      "epoch: 12 [50560/60000 (84%)]\t training loss: 0.000137\n",
      "epoch: 12 [50880/60000 (85%)]\t training loss: 0.010324\n",
      "epoch: 12 [51200/60000 (85%)]\t training loss: 0.006624\n",
      "epoch: 12 [51520/60000 (86%)]\t training loss: 0.046479\n",
      "epoch: 12 [51840/60000 (86%)]\t training loss: 0.004606\n",
      "epoch: 12 [52160/60000 (87%)]\t training loss: 0.017413\n",
      "epoch: 12 [52480/60000 (87%)]\t training loss: 0.000472\n",
      "epoch: 12 [52800/60000 (88%)]\t training loss: 0.001911\n",
      "epoch: 12 [53120/60000 (89%)]\t training loss: 0.006444\n",
      "epoch: 12 [53440/60000 (89%)]\t training loss: 0.053463\n",
      "epoch: 12 [53760/60000 (90%)]\t training loss: 0.011470\n",
      "epoch: 12 [54080/60000 (90%)]\t training loss: 0.090465\n",
      "epoch: 12 [54400/60000 (91%)]\t training loss: 0.011021\n",
      "epoch: 12 [54720/60000 (91%)]\t training loss: 0.000140\n",
      "epoch: 12 [55040/60000 (92%)]\t training loss: 0.009623\n",
      "epoch: 12 [55360/60000 (92%)]\t training loss: 0.000005\n",
      "epoch: 12 [55680/60000 (93%)]\t training loss: 0.001083\n",
      "epoch: 12 [56000/60000 (93%)]\t training loss: 0.001970\n",
      "epoch: 12 [56320/60000 (94%)]\t training loss: 0.124705\n",
      "epoch: 12 [56640/60000 (94%)]\t training loss: 0.093615\n",
      "epoch: 12 [56960/60000 (95%)]\t training loss: 0.004608\n",
      "epoch: 12 [57280/60000 (95%)]\t training loss: 0.000498\n",
      "epoch: 12 [57600/60000 (96%)]\t training loss: 0.014272\n",
      "epoch: 12 [57920/60000 (97%)]\t training loss: 0.034067\n",
      "epoch: 12 [58240/60000 (97%)]\t training loss: 0.027399\n",
      "epoch: 12 [58560/60000 (98%)]\t training loss: 0.042630\n",
      "epoch: 12 [58880/60000 (98%)]\t training loss: 0.009382\n",
      "epoch: 12 [59200/60000 (99%)]\t training loss: 0.000192\n",
      "epoch: 12 [59520/60000 (99%)]\t training loss: 0.000528\n",
      "epoch: 12 [59840/60000 (100%)]\t training loss: 0.138023\n",
      "\n",
      "Test dataset: Overall Loss: 0.0304, Overall Accuracy: 9918/10000 (99%)\n",
      "\n",
      "epoch: 13 [0/60000 (0%)]\t training loss: 0.002642\n",
      "epoch: 13 [320/60000 (1%)]\t training loss: 0.009888\n",
      "epoch: 13 [640/60000 (1%)]\t training loss: 0.001042\n",
      "epoch: 13 [960/60000 (2%)]\t training loss: 0.003110\n",
      "epoch: 13 [1280/60000 (2%)]\t training loss: 0.003655\n",
      "epoch: 13 [1600/60000 (3%)]\t training loss: 0.027676\n",
      "epoch: 13 [1920/60000 (3%)]\t training loss: 0.003847\n",
      "epoch: 13 [2240/60000 (4%)]\t training loss: 0.001874\n",
      "epoch: 13 [2560/60000 (4%)]\t training loss: 0.000270\n",
      "epoch: 13 [2880/60000 (5%)]\t training loss: 0.007093\n",
      "epoch: 13 [3200/60000 (5%)]\t training loss: 0.000009\n",
      "epoch: 13 [3520/60000 (6%)]\t training loss: 0.005045\n",
      "epoch: 13 [3840/60000 (6%)]\t training loss: 0.003596\n",
      "epoch: 13 [4160/60000 (7%)]\t training loss: 0.000584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 [4480/60000 (7%)]\t training loss: 0.010114\n",
      "epoch: 13 [4800/60000 (8%)]\t training loss: 0.000550\n",
      "epoch: 13 [5120/60000 (9%)]\t training loss: 0.370854\n",
      "epoch: 13 [5440/60000 (9%)]\t training loss: 0.000595\n",
      "epoch: 13 [5760/60000 (10%)]\t training loss: 0.000343\n",
      "epoch: 13 [6080/60000 (10%)]\t training loss: 0.001200\n",
      "epoch: 13 [6400/60000 (11%)]\t training loss: 0.000089\n",
      "epoch: 13 [6720/60000 (11%)]\t training loss: 0.001080\n",
      "epoch: 13 [7040/60000 (12%)]\t training loss: 0.119571\n",
      "epoch: 13 [7360/60000 (12%)]\t training loss: 0.020116\n",
      "epoch: 13 [7680/60000 (13%)]\t training loss: 0.045872\n",
      "epoch: 13 [8000/60000 (13%)]\t training loss: 0.004057\n",
      "epoch: 13 [8320/60000 (14%)]\t training loss: 0.003281\n",
      "epoch: 13 [8640/60000 (14%)]\t training loss: 0.000468\n",
      "epoch: 13 [8960/60000 (15%)]\t training loss: 0.091322\n",
      "epoch: 13 [9280/60000 (15%)]\t training loss: 0.018354\n",
      "epoch: 13 [9600/60000 (16%)]\t training loss: 0.000585\n",
      "epoch: 13 [9920/60000 (17%)]\t training loss: 0.008522\n",
      "epoch: 13 [10240/60000 (17%)]\t training loss: 0.001176\n",
      "epoch: 13 [10560/60000 (18%)]\t training loss: 0.001748\n",
      "epoch: 13 [10880/60000 (18%)]\t training loss: 0.000048\n",
      "epoch: 13 [11200/60000 (19%)]\t training loss: 0.000013\n",
      "epoch: 13 [11520/60000 (19%)]\t training loss: 0.003956\n",
      "epoch: 13 [11840/60000 (20%)]\t training loss: 0.011276\n",
      "epoch: 13 [12160/60000 (20%)]\t training loss: 0.000887\n",
      "epoch: 13 [12480/60000 (21%)]\t training loss: 0.167059\n",
      "epoch: 13 [12800/60000 (21%)]\t training loss: 0.000091\n",
      "epoch: 13 [13120/60000 (22%)]\t training loss: 0.001952\n",
      "epoch: 13 [13440/60000 (22%)]\t training loss: 0.028257\n",
      "epoch: 13 [13760/60000 (23%)]\t training loss: 0.003328\n",
      "epoch: 13 [14080/60000 (23%)]\t training loss: 0.000095\n",
      "epoch: 13 [14400/60000 (24%)]\t training loss: 0.016796\n",
      "epoch: 13 [14720/60000 (25%)]\t training loss: 0.002981\n",
      "epoch: 13 [15040/60000 (25%)]\t training loss: 0.003605\n",
      "epoch: 13 [15360/60000 (26%)]\t training loss: 0.000685\n",
      "epoch: 13 [15680/60000 (26%)]\t training loss: 0.000729\n",
      "epoch: 13 [16000/60000 (27%)]\t training loss: 0.000855\n",
      "epoch: 13 [16320/60000 (27%)]\t training loss: 0.000230\n",
      "epoch: 13 [16640/60000 (28%)]\t training loss: 0.007425\n",
      "epoch: 13 [16960/60000 (28%)]\t training loss: 0.000274\n",
      "epoch: 13 [17280/60000 (29%)]\t training loss: 0.004521\n",
      "epoch: 13 [17600/60000 (29%)]\t training loss: 0.000005\n",
      "epoch: 13 [17920/60000 (30%)]\t training loss: 0.016816\n",
      "epoch: 13 [18240/60000 (30%)]\t training loss: 0.000004\n",
      "epoch: 13 [18560/60000 (31%)]\t training loss: 0.011207\n",
      "epoch: 13 [18880/60000 (31%)]\t training loss: 0.000058\n",
      "epoch: 13 [19200/60000 (32%)]\t training loss: 0.001612\n",
      "epoch: 13 [19520/60000 (33%)]\t training loss: 0.015137\n",
      "epoch: 13 [19840/60000 (33%)]\t training loss: 0.000242\n",
      "epoch: 13 [20160/60000 (34%)]\t training loss: 0.000036\n",
      "epoch: 13 [20480/60000 (34%)]\t training loss: 0.003665\n",
      "epoch: 13 [20800/60000 (35%)]\t training loss: 0.000165\n",
      "epoch: 13 [21120/60000 (35%)]\t training loss: 0.000033\n",
      "epoch: 13 [21440/60000 (36%)]\t training loss: 0.000368\n",
      "epoch: 13 [21760/60000 (36%)]\t training loss: 0.011314\n",
      "epoch: 13 [22080/60000 (37%)]\t training loss: 0.000005\n",
      "epoch: 13 [22400/60000 (37%)]\t training loss: 0.000328\n",
      "epoch: 13 [22720/60000 (38%)]\t training loss: 0.000496\n",
      "epoch: 13 [23040/60000 (38%)]\t training loss: 0.001482\n",
      "epoch: 13 [23360/60000 (39%)]\t training loss: 0.018622\n",
      "epoch: 13 [23680/60000 (39%)]\t training loss: 0.009661\n",
      "epoch: 13 [24000/60000 (40%)]\t training loss: 0.062709\n",
      "epoch: 13 [24320/60000 (41%)]\t training loss: 0.000593\n",
      "epoch: 13 [24640/60000 (41%)]\t training loss: 0.004187\n",
      "epoch: 13 [24960/60000 (42%)]\t training loss: 0.014396\n",
      "epoch: 13 [25280/60000 (42%)]\t training loss: 0.000198\n",
      "epoch: 13 [25600/60000 (43%)]\t training loss: 0.005270\n",
      "epoch: 13 [25920/60000 (43%)]\t training loss: 0.041658\n",
      "epoch: 13 [26240/60000 (44%)]\t training loss: 0.012199\n",
      "epoch: 13 [26560/60000 (44%)]\t training loss: 0.002306\n",
      "epoch: 13 [26880/60000 (45%)]\t training loss: 0.000457\n",
      "epoch: 13 [27200/60000 (45%)]\t training loss: 0.005846\n",
      "epoch: 13 [27520/60000 (46%)]\t training loss: 0.000796\n",
      "epoch: 13 [27840/60000 (46%)]\t training loss: 0.000051\n",
      "epoch: 13 [28160/60000 (47%)]\t training loss: 0.226786\n",
      "epoch: 13 [28480/60000 (47%)]\t training loss: 0.003909\n",
      "epoch: 13 [28800/60000 (48%)]\t training loss: 0.001119\n",
      "epoch: 13 [29120/60000 (49%)]\t training loss: 0.002049\n",
      "epoch: 13 [29440/60000 (49%)]\t training loss: 0.011550\n",
      "epoch: 13 [29760/60000 (50%)]\t training loss: 0.000646\n",
      "epoch: 13 [30080/60000 (50%)]\t training loss: 0.000304\n",
      "epoch: 13 [30400/60000 (51%)]\t training loss: 0.000354\n",
      "epoch: 13 [30720/60000 (51%)]\t training loss: 0.066616\n",
      "epoch: 13 [31040/60000 (52%)]\t training loss: 0.054350\n",
      "epoch: 13 [31360/60000 (52%)]\t training loss: 0.017117\n",
      "epoch: 13 [31680/60000 (53%)]\t training loss: 0.000500\n",
      "epoch: 13 [32000/60000 (53%)]\t training loss: 0.056014\n",
      "epoch: 13 [32320/60000 (54%)]\t training loss: 0.004261\n",
      "epoch: 13 [32640/60000 (54%)]\t training loss: 0.004577\n",
      "epoch: 13 [32960/60000 (55%)]\t training loss: 0.006807\n",
      "epoch: 13 [33280/60000 (55%)]\t training loss: 0.000043\n",
      "epoch: 13 [33600/60000 (56%)]\t training loss: 0.005641\n",
      "epoch: 13 [33920/60000 (57%)]\t training loss: 0.001037\n",
      "epoch: 13 [34240/60000 (57%)]\t training loss: 0.029073\n",
      "epoch: 13 [34560/60000 (58%)]\t training loss: 0.000025\n",
      "epoch: 13 [34880/60000 (58%)]\t training loss: 0.052623\n",
      "epoch: 13 [35200/60000 (59%)]\t training loss: 0.001160\n",
      "epoch: 13 [35520/60000 (59%)]\t training loss: 0.081821\n",
      "epoch: 13 [35840/60000 (60%)]\t training loss: 0.002143\n",
      "epoch: 13 [36160/60000 (60%)]\t training loss: 0.010941\n",
      "epoch: 13 [36480/60000 (61%)]\t training loss: 0.015335\n",
      "epoch: 13 [36800/60000 (61%)]\t training loss: 0.000644\n",
      "epoch: 13 [37120/60000 (62%)]\t training loss: 0.000988\n",
      "epoch: 13 [37440/60000 (62%)]\t training loss: 0.000006\n",
      "epoch: 13 [37760/60000 (63%)]\t training loss: 0.000733\n",
      "epoch: 13 [38080/60000 (63%)]\t training loss: 0.009344\n",
      "epoch: 13 [38400/60000 (64%)]\t training loss: 0.002065\n",
      "epoch: 13 [38720/60000 (65%)]\t training loss: 0.036979\n",
      "epoch: 13 [39040/60000 (65%)]\t training loss: 0.000936\n",
      "epoch: 13 [39360/60000 (66%)]\t training loss: 0.000001\n",
      "epoch: 13 [39680/60000 (66%)]\t training loss: 0.000068\n",
      "epoch: 13 [40000/60000 (67%)]\t training loss: 0.024339\n",
      "epoch: 13 [40320/60000 (67%)]\t training loss: 0.001159\n",
      "epoch: 13 [40640/60000 (68%)]\t training loss: 0.095304\n",
      "epoch: 13 [40960/60000 (68%)]\t training loss: 0.102244\n",
      "epoch: 13 [41280/60000 (69%)]\t training loss: 0.015142\n",
      "epoch: 13 [41600/60000 (69%)]\t training loss: 0.006036\n",
      "epoch: 13 [41920/60000 (70%)]\t training loss: 0.001403\n",
      "epoch: 13 [42240/60000 (70%)]\t training loss: 0.010022\n",
      "epoch: 13 [42560/60000 (71%)]\t training loss: 0.004644\n",
      "epoch: 13 [42880/60000 (71%)]\t training loss: 0.000027\n",
      "epoch: 13 [43200/60000 (72%)]\t training loss: 0.045327\n",
      "epoch: 13 [43520/60000 (73%)]\t training loss: 0.000996\n",
      "epoch: 13 [43840/60000 (73%)]\t training loss: 0.009307\n",
      "epoch: 13 [44160/60000 (74%)]\t training loss: 0.002696\n",
      "epoch: 13 [44480/60000 (74%)]\t training loss: 0.109527\n",
      "epoch: 13 [44800/60000 (75%)]\t training loss: 0.001237\n",
      "epoch: 13 [45120/60000 (75%)]\t training loss: 0.000018\n",
      "epoch: 13 [45440/60000 (76%)]\t training loss: 0.002204\n",
      "epoch: 13 [45760/60000 (76%)]\t training loss: 0.000631\n",
      "epoch: 13 [46080/60000 (77%)]\t training loss: 0.000050\n",
      "epoch: 13 [46400/60000 (77%)]\t training loss: 0.003207\n",
      "epoch: 13 [46720/60000 (78%)]\t training loss: 0.000618\n",
      "epoch: 13 [47040/60000 (78%)]\t training loss: 0.007811\n",
      "epoch: 13 [47360/60000 (79%)]\t training loss: 0.055736\n",
      "epoch: 13 [47680/60000 (79%)]\t training loss: 0.019351\n",
      "epoch: 13 [48000/60000 (80%)]\t training loss: 0.000171\n",
      "epoch: 13 [48320/60000 (81%)]\t training loss: 0.000159\n",
      "epoch: 13 [48640/60000 (81%)]\t training loss: 0.000084\n",
      "epoch: 13 [48960/60000 (82%)]\t training loss: 0.000149\n",
      "epoch: 13 [49280/60000 (82%)]\t training loss: 0.003877\n",
      "epoch: 13 [49600/60000 (83%)]\t training loss: 0.026048\n",
      "epoch: 13 [49920/60000 (83%)]\t training loss: 0.000141\n",
      "epoch: 13 [50240/60000 (84%)]\t training loss: 0.000121\n",
      "epoch: 13 [50560/60000 (84%)]\t training loss: 0.000182\n",
      "epoch: 13 [50880/60000 (85%)]\t training loss: 0.094302\n",
      "epoch: 13 [51200/60000 (85%)]\t training loss: 0.005100\n",
      "epoch: 13 [51520/60000 (86%)]\t training loss: 0.000465\n",
      "epoch: 13 [51840/60000 (86%)]\t training loss: 0.001376\n",
      "epoch: 13 [52160/60000 (87%)]\t training loss: 0.309163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 [52480/60000 (87%)]\t training loss: 0.103895\n",
      "epoch: 13 [52800/60000 (88%)]\t training loss: 0.000760\n",
      "epoch: 13 [53120/60000 (89%)]\t training loss: 0.000335\n",
      "epoch: 13 [53440/60000 (89%)]\t training loss: 0.000475\n",
      "epoch: 13 [53760/60000 (90%)]\t training loss: 0.053883\n",
      "epoch: 13 [54080/60000 (90%)]\t training loss: 0.000290\n",
      "epoch: 13 [54400/60000 (91%)]\t training loss: 0.003916\n",
      "epoch: 13 [54720/60000 (91%)]\t training loss: 0.013778\n",
      "epoch: 13 [55040/60000 (92%)]\t training loss: 0.037867\n",
      "epoch: 13 [55360/60000 (92%)]\t training loss: 0.000009\n",
      "epoch: 13 [55680/60000 (93%)]\t training loss: 0.000066\n",
      "epoch: 13 [56000/60000 (93%)]\t training loss: 0.001927\n",
      "epoch: 13 [56320/60000 (94%)]\t training loss: 0.000094\n",
      "epoch: 13 [56640/60000 (94%)]\t training loss: 0.001011\n",
      "epoch: 13 [56960/60000 (95%)]\t training loss: 0.343665\n",
      "epoch: 13 [57280/60000 (95%)]\t training loss: 0.001287\n",
      "epoch: 13 [57600/60000 (96%)]\t training loss: 0.006049\n",
      "epoch: 13 [57920/60000 (97%)]\t training loss: 0.038056\n",
      "epoch: 13 [58240/60000 (97%)]\t training loss: 0.021501\n",
      "epoch: 13 [58560/60000 (98%)]\t training loss: 0.004210\n",
      "epoch: 13 [58880/60000 (98%)]\t training loss: 0.000946\n",
      "epoch: 13 [59200/60000 (99%)]\t training loss: 0.000049\n",
      "epoch: 13 [59520/60000 (99%)]\t training loss: 0.000074\n",
      "epoch: 13 [59840/60000 (100%)]\t training loss: 0.034862\n",
      "\n",
      "Test dataset: Overall Loss: 0.0344, Overall Accuracy: 9911/10000 (99%)\n",
      "\n",
      "epoch: 14 [0/60000 (0%)]\t training loss: 0.000232\n",
      "epoch: 14 [320/60000 (1%)]\t training loss: 0.000011\n",
      "epoch: 14 [640/60000 (1%)]\t training loss: 0.000034\n",
      "epoch: 14 [960/60000 (2%)]\t training loss: 0.000342\n",
      "epoch: 14 [1280/60000 (2%)]\t training loss: 0.003389\n",
      "epoch: 14 [1600/60000 (3%)]\t training loss: 0.000098\n",
      "epoch: 14 [1920/60000 (3%)]\t training loss: 0.030300\n",
      "epoch: 14 [2240/60000 (4%)]\t training loss: 0.005387\n",
      "epoch: 14 [2560/60000 (4%)]\t training loss: 0.008221\n",
      "epoch: 14 [2880/60000 (5%)]\t training loss: 0.042480\n",
      "epoch: 14 [3200/60000 (5%)]\t training loss: 0.014145\n",
      "epoch: 14 [3520/60000 (6%)]\t training loss: 0.016382\n",
      "epoch: 14 [3840/60000 (6%)]\t training loss: 0.006621\n",
      "epoch: 14 [4160/60000 (7%)]\t training loss: 0.002102\n",
      "epoch: 14 [4480/60000 (7%)]\t training loss: 0.004461\n",
      "epoch: 14 [4800/60000 (8%)]\t training loss: 0.004666\n",
      "epoch: 14 [5120/60000 (9%)]\t training loss: 0.076728\n",
      "epoch: 14 [5440/60000 (9%)]\t training loss: 0.000297\n",
      "epoch: 14 [5760/60000 (10%)]\t training loss: 0.001438\n",
      "epoch: 14 [6080/60000 (10%)]\t training loss: 0.000525\n",
      "epoch: 14 [6400/60000 (11%)]\t training loss: 0.000018\n",
      "epoch: 14 [6720/60000 (11%)]\t training loss: 0.001246\n",
      "epoch: 14 [7040/60000 (12%)]\t training loss: 0.029529\n",
      "epoch: 14 [7360/60000 (12%)]\t training loss: 0.000338\n",
      "epoch: 14 [7680/60000 (13%)]\t training loss: 0.036946\n",
      "epoch: 14 [8000/60000 (13%)]\t training loss: 0.003054\n",
      "epoch: 14 [8320/60000 (14%)]\t training loss: 0.013136\n",
      "epoch: 14 [8640/60000 (14%)]\t training loss: 0.009114\n",
      "epoch: 14 [8960/60000 (15%)]\t training loss: 0.000039\n",
      "epoch: 14 [9280/60000 (15%)]\t training loss: 0.000003\n",
      "epoch: 14 [9600/60000 (16%)]\t training loss: 0.150122\n",
      "epoch: 14 [9920/60000 (17%)]\t training loss: 0.000005\n",
      "epoch: 14 [10240/60000 (17%)]\t training loss: 0.013780\n",
      "epoch: 14 [10560/60000 (18%)]\t training loss: 0.000290\n",
      "epoch: 14 [10880/60000 (18%)]\t training loss: 0.011738\n",
      "epoch: 14 [11200/60000 (19%)]\t training loss: 0.004190\n",
      "epoch: 14 [11520/60000 (19%)]\t training loss: 0.000135\n",
      "epoch: 14 [11840/60000 (20%)]\t training loss: 0.018561\n",
      "epoch: 14 [12160/60000 (20%)]\t training loss: 0.000327\n",
      "epoch: 14 [12480/60000 (21%)]\t training loss: 0.014368\n",
      "epoch: 14 [12800/60000 (21%)]\t training loss: 0.007856\n",
      "epoch: 14 [13120/60000 (22%)]\t training loss: 0.006218\n",
      "epoch: 14 [13440/60000 (22%)]\t training loss: 0.004221\n",
      "epoch: 14 [13760/60000 (23%)]\t training loss: 0.000074\n",
      "epoch: 14 [14080/60000 (23%)]\t training loss: 0.019877\n",
      "epoch: 14 [14400/60000 (24%)]\t training loss: 0.003864\n",
      "epoch: 14 [14720/60000 (25%)]\t training loss: 0.050003\n",
      "epoch: 14 [15040/60000 (25%)]\t training loss: 0.026051\n",
      "epoch: 14 [15360/60000 (26%)]\t training loss: 0.051652\n",
      "epoch: 14 [15680/60000 (26%)]\t training loss: 0.003763\n",
      "epoch: 14 [16000/60000 (27%)]\t training loss: 0.054725\n",
      "epoch: 14 [16320/60000 (27%)]\t training loss: 0.036647\n",
      "epoch: 14 [16640/60000 (28%)]\t training loss: 0.000016\n",
      "epoch: 14 [16960/60000 (28%)]\t training loss: 0.000461\n",
      "epoch: 14 [17280/60000 (29%)]\t training loss: 0.000073\n",
      "epoch: 14 [17600/60000 (29%)]\t training loss: 0.021536\n",
      "epoch: 14 [17920/60000 (30%)]\t training loss: 0.003906\n",
      "epoch: 14 [18240/60000 (30%)]\t training loss: 0.000005\n",
      "epoch: 14 [18560/60000 (31%)]\t training loss: 0.000337\n",
      "epoch: 14 [18880/60000 (31%)]\t training loss: 0.000046\n",
      "epoch: 14 [19200/60000 (32%)]\t training loss: 0.000005\n",
      "epoch: 14 [19520/60000 (33%)]\t training loss: 0.000091\n",
      "epoch: 14 [19840/60000 (33%)]\t training loss: 0.000010\n",
      "epoch: 14 [20160/60000 (34%)]\t training loss: 0.006030\n",
      "epoch: 14 [20480/60000 (34%)]\t training loss: 0.021704\n",
      "epoch: 14 [20800/60000 (35%)]\t training loss: 0.219403\n",
      "epoch: 14 [21120/60000 (35%)]\t training loss: 0.005546\n",
      "epoch: 14 [21440/60000 (36%)]\t training loss: 0.017357\n",
      "epoch: 14 [21760/60000 (36%)]\t training loss: 0.010050\n",
      "epoch: 14 [22080/60000 (37%)]\t training loss: 0.158736\n",
      "epoch: 14 [22400/60000 (37%)]\t training loss: 0.025887\n",
      "epoch: 14 [22720/60000 (38%)]\t training loss: 0.002165\n",
      "epoch: 14 [23040/60000 (38%)]\t training loss: 0.008585\n",
      "epoch: 14 [23360/60000 (39%)]\t training loss: 0.000099\n",
      "epoch: 14 [23680/60000 (39%)]\t training loss: 0.001169\n",
      "epoch: 14 [24000/60000 (40%)]\t training loss: 0.000103\n",
      "epoch: 14 [24320/60000 (41%)]\t training loss: 0.013372\n",
      "epoch: 14 [24640/60000 (41%)]\t training loss: 0.118322\n",
      "epoch: 14 [24960/60000 (42%)]\t training loss: 0.047435\n",
      "epoch: 14 [25280/60000 (42%)]\t training loss: 0.026946\n",
      "epoch: 14 [25600/60000 (43%)]\t training loss: 0.017506\n",
      "epoch: 14 [25920/60000 (43%)]\t training loss: 0.000970\n",
      "epoch: 14 [26240/60000 (44%)]\t training loss: 0.002034\n",
      "epoch: 14 [26560/60000 (44%)]\t training loss: 0.000109\n",
      "epoch: 14 [26880/60000 (45%)]\t training loss: 0.000162\n",
      "epoch: 14 [27200/60000 (45%)]\t training loss: 0.006586\n",
      "epoch: 14 [27520/60000 (46%)]\t training loss: 0.031211\n",
      "epoch: 14 [27840/60000 (46%)]\t training loss: 0.051853\n",
      "epoch: 14 [28160/60000 (47%)]\t training loss: 0.101939\n",
      "epoch: 14 [28480/60000 (47%)]\t training loss: 0.000029\n",
      "epoch: 14 [28800/60000 (48%)]\t training loss: 0.000026\n",
      "epoch: 14 [29120/60000 (49%)]\t training loss: 0.002150\n",
      "epoch: 14 [29440/60000 (49%)]\t training loss: 0.000614\n",
      "epoch: 14 [29760/60000 (50%)]\t training loss: 0.001792\n",
      "epoch: 14 [30080/60000 (50%)]\t training loss: 0.001257\n",
      "epoch: 14 [30400/60000 (51%)]\t training loss: 0.000021\n",
      "epoch: 14 [30720/60000 (51%)]\t training loss: 0.005992\n",
      "epoch: 14 [31040/60000 (52%)]\t training loss: 0.002412\n",
      "epoch: 14 [31360/60000 (52%)]\t training loss: 0.002114\n",
      "epoch: 14 [31680/60000 (53%)]\t training loss: 0.000671\n",
      "epoch: 14 [32000/60000 (53%)]\t training loss: 0.016837\n",
      "epoch: 14 [32320/60000 (54%)]\t training loss: 0.063916\n",
      "epoch: 14 [32640/60000 (54%)]\t training loss: 0.004069\n",
      "epoch: 14 [32960/60000 (55%)]\t training loss: 0.000788\n",
      "epoch: 14 [33280/60000 (55%)]\t training loss: 0.001076\n",
      "epoch: 14 [33600/60000 (56%)]\t training loss: 0.010602\n",
      "epoch: 14 [33920/60000 (57%)]\t training loss: 0.000779\n",
      "epoch: 14 [34240/60000 (57%)]\t training loss: 0.022853\n",
      "epoch: 14 [34560/60000 (58%)]\t training loss: 0.000002\n",
      "epoch: 14 [34880/60000 (58%)]\t training loss: 0.001890\n",
      "epoch: 14 [35200/60000 (59%)]\t training loss: 0.068630\n",
      "epoch: 14 [35520/60000 (59%)]\t training loss: 0.001908\n",
      "epoch: 14 [35840/60000 (60%)]\t training loss: 0.002416\n",
      "epoch: 14 [36160/60000 (60%)]\t training loss: 0.060834\n",
      "epoch: 14 [36480/60000 (61%)]\t training loss: 0.003821\n",
      "epoch: 14 [36800/60000 (61%)]\t training loss: 0.166163\n",
      "epoch: 14 [37120/60000 (62%)]\t training loss: 0.277243\n",
      "epoch: 14 [37440/60000 (62%)]\t training loss: 0.000116\n",
      "epoch: 14 [37760/60000 (63%)]\t training loss: 0.000014\n",
      "epoch: 14 [38080/60000 (63%)]\t training loss: 0.042417\n",
      "epoch: 14 [38400/60000 (64%)]\t training loss: 0.001530\n",
      "epoch: 14 [38720/60000 (65%)]\t training loss: 0.000138\n",
      "epoch: 14 [39040/60000 (65%)]\t training loss: 0.040899\n",
      "epoch: 14 [39360/60000 (66%)]\t training loss: 0.005215\n",
      "epoch: 14 [39680/60000 (66%)]\t training loss: 0.001156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 [40000/60000 (67%)]\t training loss: 0.001615\n",
      "epoch: 14 [40320/60000 (67%)]\t training loss: 0.000098\n",
      "epoch: 14 [40640/60000 (68%)]\t training loss: 0.012458\n",
      "epoch: 14 [40960/60000 (68%)]\t training loss: 0.038772\n",
      "epoch: 14 [41280/60000 (69%)]\t training loss: 0.030739\n",
      "epoch: 14 [41600/60000 (69%)]\t training loss: 0.001304\n",
      "epoch: 14 [41920/60000 (70%)]\t training loss: 0.000670\n",
      "epoch: 14 [42240/60000 (70%)]\t training loss: 0.000409\n",
      "epoch: 14 [42560/60000 (71%)]\t training loss: 0.000439\n",
      "epoch: 14 [42880/60000 (71%)]\t training loss: 0.004450\n",
      "epoch: 14 [43200/60000 (72%)]\t training loss: 0.002087\n",
      "epoch: 14 [43520/60000 (73%)]\t training loss: 0.103231\n",
      "epoch: 14 [43840/60000 (73%)]\t training loss: 0.000064\n",
      "epoch: 14 [44160/60000 (74%)]\t training loss: 0.000011\n",
      "epoch: 14 [44480/60000 (74%)]\t training loss: 0.000004\n",
      "epoch: 14 [44800/60000 (75%)]\t training loss: 0.000083\n",
      "epoch: 14 [45120/60000 (75%)]\t training loss: 0.057245\n",
      "epoch: 14 [45440/60000 (76%)]\t training loss: 0.002203\n",
      "epoch: 14 [45760/60000 (76%)]\t training loss: 0.000304\n",
      "epoch: 14 [46080/60000 (77%)]\t training loss: 0.000108\n",
      "epoch: 14 [46400/60000 (77%)]\t training loss: 0.007058\n",
      "epoch: 14 [46720/60000 (78%)]\t training loss: 0.000348\n",
      "epoch: 14 [47040/60000 (78%)]\t training loss: 0.000302\n",
      "epoch: 14 [47360/60000 (79%)]\t training loss: 0.003243\n",
      "epoch: 14 [47680/60000 (79%)]\t training loss: 0.000500\n",
      "epoch: 14 [48000/60000 (80%)]\t training loss: 0.000008\n",
      "epoch: 14 [48320/60000 (81%)]\t training loss: 0.000132\n",
      "epoch: 14 [48640/60000 (81%)]\t training loss: 0.039574\n",
      "epoch: 14 [48960/60000 (82%)]\t training loss: 0.038571\n",
      "epoch: 14 [49280/60000 (82%)]\t training loss: 0.018349\n",
      "epoch: 14 [49600/60000 (83%)]\t training loss: 0.000125\n",
      "epoch: 14 [49920/60000 (83%)]\t training loss: 0.003701\n",
      "epoch: 14 [50240/60000 (84%)]\t training loss: 0.063573\n",
      "epoch: 14 [50560/60000 (84%)]\t training loss: 0.009561\n",
      "epoch: 14 [50880/60000 (85%)]\t training loss: 0.003102\n",
      "epoch: 14 [51200/60000 (85%)]\t training loss: 0.000628\n",
      "epoch: 14 [51520/60000 (86%)]\t training loss: 0.000252\n",
      "epoch: 14 [51840/60000 (86%)]\t training loss: 0.233816\n",
      "epoch: 14 [52160/60000 (87%)]\t training loss: 0.051577\n",
      "epoch: 14 [52480/60000 (87%)]\t training loss: 0.007632\n",
      "epoch: 14 [52800/60000 (88%)]\t training loss: 0.000505\n",
      "epoch: 14 [53120/60000 (89%)]\t training loss: 0.000050\n",
      "epoch: 14 [53440/60000 (89%)]\t training loss: 0.000095\n",
      "epoch: 14 [53760/60000 (90%)]\t training loss: 0.005458\n",
      "epoch: 14 [54080/60000 (90%)]\t training loss: 0.000180\n",
      "epoch: 14 [54400/60000 (91%)]\t training loss: 0.008301\n",
      "epoch: 14 [54720/60000 (91%)]\t training loss: 0.086537\n",
      "epoch: 14 [55040/60000 (92%)]\t training loss: 0.000500\n",
      "epoch: 14 [55360/60000 (92%)]\t training loss: 0.000469\n",
      "epoch: 14 [55680/60000 (93%)]\t training loss: 0.034061\n",
      "epoch: 14 [56000/60000 (93%)]\t training loss: 0.005406\n",
      "epoch: 14 [56320/60000 (94%)]\t training loss: 0.195491\n",
      "epoch: 14 [56640/60000 (94%)]\t training loss: 0.002810\n",
      "epoch: 14 [56960/60000 (95%)]\t training loss: 0.001549\n",
      "epoch: 14 [57280/60000 (95%)]\t training loss: 0.001137\n",
      "epoch: 14 [57600/60000 (96%)]\t training loss: 0.020533\n",
      "epoch: 14 [57920/60000 (97%)]\t training loss: 0.000043\n",
      "epoch: 14 [58240/60000 (97%)]\t training loss: 0.018987\n",
      "epoch: 14 [58560/60000 (98%)]\t training loss: 0.012824\n",
      "epoch: 14 [58880/60000 (98%)]\t training loss: 0.200958\n",
      "epoch: 14 [59200/60000 (99%)]\t training loss: 0.041274\n",
      "epoch: 14 [59520/60000 (99%)]\t training loss: 0.172356\n",
      "epoch: 14 [59840/60000 (100%)]\t training loss: 0.004317\n",
      "\n",
      "Test dataset: Overall Loss: 0.0353, Overall Accuracy: 9909/10000 (99%)\n",
      "\n",
      "epoch: 15 [0/60000 (0%)]\t training loss: 0.000208\n",
      "epoch: 15 [320/60000 (1%)]\t training loss: 0.000032\n",
      "epoch: 15 [640/60000 (1%)]\t training loss: 0.000003\n",
      "epoch: 15 [960/60000 (2%)]\t training loss: 0.000140\n",
      "epoch: 15 [1280/60000 (2%)]\t training loss: 0.009253\n",
      "epoch: 15 [1600/60000 (3%)]\t training loss: 0.011549\n",
      "epoch: 15 [1920/60000 (3%)]\t training loss: 0.002735\n",
      "epoch: 15 [2240/60000 (4%)]\t training loss: 0.010088\n",
      "epoch: 15 [2560/60000 (4%)]\t training loss: 0.003932\n",
      "epoch: 15 [2880/60000 (5%)]\t training loss: 0.002500\n",
      "epoch: 15 [3200/60000 (5%)]\t training loss: 0.007388\n",
      "epoch: 15 [3520/60000 (6%)]\t training loss: 0.391164\n",
      "epoch: 15 [3840/60000 (6%)]\t training loss: 0.028523\n",
      "epoch: 15 [4160/60000 (7%)]\t training loss: 0.006792\n",
      "epoch: 15 [4480/60000 (7%)]\t training loss: 0.003332\n",
      "epoch: 15 [4800/60000 (8%)]\t training loss: 0.004167\n",
      "epoch: 15 [5120/60000 (9%)]\t training loss: 0.002073\n",
      "epoch: 15 [5440/60000 (9%)]\t training loss: 0.000167\n",
      "epoch: 15 [5760/60000 (10%)]\t training loss: 0.070914\n",
      "epoch: 15 [6080/60000 (10%)]\t training loss: 0.000292\n",
      "epoch: 15 [6400/60000 (11%)]\t training loss: 0.001512\n",
      "epoch: 15 [6720/60000 (11%)]\t training loss: 0.002346\n",
      "epoch: 15 [7040/60000 (12%)]\t training loss: 0.000019\n",
      "epoch: 15 [7360/60000 (12%)]\t training loss: 0.201900\n",
      "epoch: 15 [7680/60000 (13%)]\t training loss: 0.000555\n",
      "epoch: 15 [8000/60000 (13%)]\t training loss: 0.007676\n",
      "epoch: 15 [8320/60000 (14%)]\t training loss: 0.061647\n",
      "epoch: 15 [8640/60000 (14%)]\t training loss: 0.002032\n",
      "epoch: 15 [8960/60000 (15%)]\t training loss: 0.008396\n",
      "epoch: 15 [9280/60000 (15%)]\t training loss: 0.004356\n",
      "epoch: 15 [9600/60000 (16%)]\t training loss: 0.000013\n",
      "epoch: 15 [9920/60000 (17%)]\t training loss: 0.002472\n",
      "epoch: 15 [10240/60000 (17%)]\t training loss: 0.002036\n",
      "epoch: 15 [10560/60000 (18%)]\t training loss: 0.003050\n",
      "epoch: 15 [10880/60000 (18%)]\t training loss: 0.063254\n",
      "epoch: 15 [11200/60000 (19%)]\t training loss: 0.000102\n",
      "epoch: 15 [11520/60000 (19%)]\t training loss: 0.001565\n",
      "epoch: 15 [11840/60000 (20%)]\t training loss: 0.000385\n",
      "epoch: 15 [12160/60000 (20%)]\t training loss: 0.074786\n",
      "epoch: 15 [12480/60000 (21%)]\t training loss: 0.000503\n",
      "epoch: 15 [12800/60000 (21%)]\t training loss: 0.000004\n",
      "epoch: 15 [13120/60000 (22%)]\t training loss: 0.000447\n",
      "epoch: 15 [13440/60000 (22%)]\t training loss: 0.003312\n",
      "epoch: 15 [13760/60000 (23%)]\t training loss: 0.004026\n",
      "epoch: 15 [14080/60000 (23%)]\t training loss: 0.008405\n",
      "epoch: 15 [14400/60000 (24%)]\t training loss: 0.003950\n",
      "epoch: 15 [14720/60000 (25%)]\t training loss: 0.001311\n",
      "epoch: 15 [15040/60000 (25%)]\t training loss: 0.038307\n",
      "epoch: 15 [15360/60000 (26%)]\t training loss: 0.000813\n",
      "epoch: 15 [15680/60000 (26%)]\t training loss: 0.000206\n",
      "epoch: 15 [16000/60000 (27%)]\t training loss: 0.000166\n",
      "epoch: 15 [16320/60000 (27%)]\t training loss: 0.007354\n",
      "epoch: 15 [16640/60000 (28%)]\t training loss: 0.133273\n",
      "epoch: 15 [16960/60000 (28%)]\t training loss: 0.017761\n",
      "epoch: 15 [17280/60000 (29%)]\t training loss: 0.004825\n",
      "epoch: 15 [17600/60000 (29%)]\t training loss: 0.000204\n",
      "epoch: 15 [17920/60000 (30%)]\t training loss: 0.000722\n",
      "epoch: 15 [18240/60000 (30%)]\t training loss: 0.061830\n",
      "epoch: 15 [18560/60000 (31%)]\t training loss: 0.001153\n",
      "epoch: 15 [18880/60000 (31%)]\t training loss: 0.002312\n",
      "epoch: 15 [19200/60000 (32%)]\t training loss: 0.000036\n",
      "epoch: 15 [19520/60000 (33%)]\t training loss: 0.075622\n",
      "epoch: 15 [19840/60000 (33%)]\t training loss: 0.000766\n",
      "epoch: 15 [20160/60000 (34%)]\t training loss: 0.000860\n",
      "epoch: 15 [20480/60000 (34%)]\t training loss: 0.000019\n",
      "epoch: 15 [20800/60000 (35%)]\t training loss: 0.043072\n",
      "epoch: 15 [21120/60000 (35%)]\t training loss: 0.014253\n",
      "epoch: 15 [21440/60000 (36%)]\t training loss: 0.000325\n",
      "epoch: 15 [21760/60000 (36%)]\t training loss: 0.014296\n",
      "epoch: 15 [22080/60000 (37%)]\t training loss: 0.000077\n",
      "epoch: 15 [22400/60000 (37%)]\t training loss: 0.070209\n",
      "epoch: 15 [22720/60000 (38%)]\t training loss: 0.000006\n",
      "epoch: 15 [23040/60000 (38%)]\t training loss: 0.000014\n",
      "epoch: 15 [23360/60000 (39%)]\t training loss: 0.001594\n",
      "epoch: 15 [23680/60000 (39%)]\t training loss: 0.143043\n",
      "epoch: 15 [24000/60000 (40%)]\t training loss: 0.000244\n",
      "epoch: 15 [24320/60000 (41%)]\t training loss: 0.003239\n",
      "epoch: 15 [24640/60000 (41%)]\t training loss: 0.000385\n",
      "epoch: 15 [24960/60000 (42%)]\t training loss: 0.004141\n",
      "epoch: 15 [25280/60000 (42%)]\t training loss: 0.000000\n",
      "epoch: 15 [25600/60000 (43%)]\t training loss: 0.003596\n",
      "epoch: 15 [25920/60000 (43%)]\t training loss: 0.271767\n",
      "epoch: 15 [26240/60000 (44%)]\t training loss: 0.000110\n",
      "epoch: 15 [26560/60000 (44%)]\t training loss: 0.000176\n",
      "epoch: 15 [26880/60000 (45%)]\t training loss: 0.019274\n",
      "epoch: 15 [27200/60000 (45%)]\t training loss: 0.005719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 [27520/60000 (46%)]\t training loss: 0.014844\n",
      "epoch: 15 [27840/60000 (46%)]\t training loss: 0.026352\n",
      "epoch: 15 [28160/60000 (47%)]\t training loss: 0.053719\n",
      "epoch: 15 [28480/60000 (47%)]\t training loss: 0.001036\n",
      "epoch: 15 [28800/60000 (48%)]\t training loss: 0.000211\n",
      "epoch: 15 [29120/60000 (49%)]\t training loss: 0.000518\n",
      "epoch: 15 [29440/60000 (49%)]\t training loss: 0.000222\n",
      "epoch: 15 [29760/60000 (50%)]\t training loss: 0.002342\n",
      "epoch: 15 [30080/60000 (50%)]\t training loss: 0.001377\n",
      "epoch: 15 [30400/60000 (51%)]\t training loss: 0.469618\n",
      "epoch: 15 [30720/60000 (51%)]\t training loss: 0.012288\n",
      "epoch: 15 [31040/60000 (52%)]\t training loss: 0.035722\n",
      "epoch: 15 [31360/60000 (52%)]\t training loss: 0.000361\n",
      "epoch: 15 [31680/60000 (53%)]\t training loss: 0.000002\n",
      "epoch: 15 [32000/60000 (53%)]\t training loss: 0.002422\n",
      "epoch: 15 [32320/60000 (54%)]\t training loss: 0.000270\n",
      "epoch: 15 [32640/60000 (54%)]\t training loss: 0.000045\n",
      "epoch: 15 [32960/60000 (55%)]\t training loss: 0.010953\n",
      "epoch: 15 [33280/60000 (55%)]\t training loss: 0.000182\n",
      "epoch: 15 [33600/60000 (56%)]\t training loss: 0.093694\n",
      "epoch: 15 [33920/60000 (57%)]\t training loss: 0.170486\n",
      "epoch: 15 [34240/60000 (57%)]\t training loss: 0.021374\n",
      "epoch: 15 [34560/60000 (58%)]\t training loss: 0.002338\n",
      "epoch: 15 [34880/60000 (58%)]\t training loss: 0.099728\n",
      "epoch: 15 [35200/60000 (59%)]\t training loss: 0.002498\n",
      "epoch: 15 [35520/60000 (59%)]\t training loss: 0.000208\n",
      "epoch: 15 [35840/60000 (60%)]\t training loss: 0.000520\n",
      "epoch: 15 [36160/60000 (60%)]\t training loss: 0.062173\n",
      "epoch: 15 [36480/60000 (61%)]\t training loss: 0.002994\n",
      "epoch: 15 [36800/60000 (61%)]\t training loss: 0.028466\n",
      "epoch: 15 [37120/60000 (62%)]\t training loss: 0.076725\n",
      "epoch: 15 [37440/60000 (62%)]\t training loss: 0.000550\n",
      "epoch: 15 [37760/60000 (63%)]\t training loss: 0.035202\n",
      "epoch: 15 [38080/60000 (63%)]\t training loss: 0.046375\n",
      "epoch: 15 [38400/60000 (64%)]\t training loss: 0.090448\n",
      "epoch: 15 [38720/60000 (65%)]\t training loss: 0.003399\n",
      "epoch: 15 [39040/60000 (65%)]\t training loss: 0.014722\n",
      "epoch: 15 [39360/60000 (66%)]\t training loss: 0.069792\n",
      "epoch: 15 [39680/60000 (66%)]\t training loss: 0.000006\n",
      "epoch: 15 [40000/60000 (67%)]\t training loss: 0.000154\n",
      "epoch: 15 [40320/60000 (67%)]\t training loss: 0.006404\n",
      "epoch: 15 [40640/60000 (68%)]\t training loss: 0.011176\n",
      "epoch: 15 [40960/60000 (68%)]\t training loss: 0.000306\n",
      "epoch: 15 [41280/60000 (69%)]\t training loss: 0.060074\n",
      "epoch: 15 [41600/60000 (69%)]\t training loss: 0.000358\n",
      "epoch: 15 [41920/60000 (70%)]\t training loss: 0.000277\n",
      "epoch: 15 [42240/60000 (70%)]\t training loss: 0.000101\n",
      "epoch: 15 [42560/60000 (71%)]\t training loss: 0.002865\n",
      "epoch: 15 [42880/60000 (71%)]\t training loss: 0.001450\n",
      "epoch: 15 [43200/60000 (72%)]\t training loss: 0.003674\n",
      "epoch: 15 [43520/60000 (73%)]\t training loss: 0.000214\n",
      "epoch: 15 [43840/60000 (73%)]\t training loss: 0.043822\n",
      "epoch: 15 [44160/60000 (74%)]\t training loss: 0.000107\n",
      "epoch: 15 [44480/60000 (74%)]\t training loss: 0.000046\n",
      "epoch: 15 [44800/60000 (75%)]\t training loss: 0.000491\n",
      "epoch: 15 [45120/60000 (75%)]\t training loss: 0.002823\n",
      "epoch: 15 [45440/60000 (76%)]\t training loss: 0.002967\n",
      "epoch: 15 [45760/60000 (76%)]\t training loss: 0.000114\n",
      "epoch: 15 [46080/60000 (77%)]\t training loss: 0.004131\n",
      "epoch: 15 [46400/60000 (77%)]\t training loss: 0.000030\n",
      "epoch: 15 [46720/60000 (78%)]\t training loss: 0.000116\n",
      "epoch: 15 [47040/60000 (78%)]\t training loss: 0.000263\n",
      "epoch: 15 [47360/60000 (79%)]\t training loss: 0.000516\n",
      "epoch: 15 [47680/60000 (79%)]\t training loss: 0.000379\n",
      "epoch: 15 [48000/60000 (80%)]\t training loss: 0.004170\n",
      "epoch: 15 [48320/60000 (81%)]\t training loss: 0.000027\n",
      "epoch: 15 [48640/60000 (81%)]\t training loss: 0.003942\n",
      "epoch: 15 [48960/60000 (82%)]\t training loss: 0.000198\n",
      "epoch: 15 [49280/60000 (82%)]\t training loss: 0.005958\n",
      "epoch: 15 [49600/60000 (83%)]\t training loss: 0.077129\n",
      "epoch: 15 [49920/60000 (83%)]\t training loss: 0.001899\n",
      "epoch: 15 [50240/60000 (84%)]\t training loss: 0.001108\n",
      "epoch: 15 [50560/60000 (84%)]\t training loss: 0.000615\n",
      "epoch: 15 [50880/60000 (85%)]\t training loss: 0.004440\n",
      "epoch: 15 [51200/60000 (85%)]\t training loss: 0.000926\n",
      "epoch: 15 [51520/60000 (86%)]\t training loss: 0.000056\n",
      "epoch: 15 [51840/60000 (86%)]\t training loss: 0.000168\n",
      "epoch: 15 [52160/60000 (87%)]\t training loss: 0.000847\n",
      "epoch: 15 [52480/60000 (87%)]\t training loss: 0.009404\n",
      "epoch: 15 [52800/60000 (88%)]\t training loss: 0.000417\n",
      "epoch: 15 [53120/60000 (89%)]\t training loss: 0.000110\n",
      "epoch: 15 [53440/60000 (89%)]\t training loss: 0.000008\n",
      "epoch: 15 [53760/60000 (90%)]\t training loss: 0.013713\n",
      "epoch: 15 [54080/60000 (90%)]\t training loss: 0.000296\n",
      "epoch: 15 [54400/60000 (91%)]\t training loss: 0.019979\n",
      "epoch: 15 [54720/60000 (91%)]\t training loss: 0.000696\n",
      "epoch: 15 [55040/60000 (92%)]\t training loss: 0.000007\n",
      "epoch: 15 [55360/60000 (92%)]\t training loss: 0.023025\n",
      "epoch: 15 [55680/60000 (93%)]\t training loss: 0.027656\n",
      "epoch: 15 [56000/60000 (93%)]\t training loss: 0.000213\n",
      "epoch: 15 [56320/60000 (94%)]\t training loss: 0.000012\n",
      "epoch: 15 [56640/60000 (94%)]\t training loss: 0.000474\n",
      "epoch: 15 [56960/60000 (95%)]\t training loss: 0.000212\n",
      "epoch: 15 [57280/60000 (95%)]\t training loss: 0.000145\n",
      "epoch: 15 [57600/60000 (96%)]\t training loss: 0.087330\n",
      "epoch: 15 [57920/60000 (97%)]\t training loss: 0.000114\n",
      "epoch: 15 [58240/60000 (97%)]\t training loss: 0.011578\n",
      "epoch: 15 [58560/60000 (98%)]\t training loss: 0.020650\n",
      "epoch: 15 [58880/60000 (98%)]\t training loss: 0.000391\n",
      "epoch: 15 [59200/60000 (99%)]\t training loss: 0.000475\n",
      "epoch: 15 [59520/60000 (99%)]\t training loss: 0.000841\n",
      "epoch: 15 [59840/60000 (100%)]\t training loss: 0.000956\n",
      "\n",
      "Test dataset: Overall Loss: 0.0390, Overall Accuracy: 9908/10000 (99%)\n",
      "\n",
      "epoch: 16 [0/60000 (0%)]\t training loss: 0.000070\n",
      "epoch: 16 [320/60000 (1%)]\t training loss: 0.004333\n",
      "epoch: 16 [640/60000 (1%)]\t training loss: 0.000114\n",
      "epoch: 16 [960/60000 (2%)]\t training loss: 0.015971\n",
      "epoch: 16 [1280/60000 (2%)]\t training loss: 0.002790\n",
      "epoch: 16 [1600/60000 (3%)]\t training loss: 0.001858\n",
      "epoch: 16 [1920/60000 (3%)]\t training loss: 0.000833\n",
      "epoch: 16 [2240/60000 (4%)]\t training loss: 0.000671\n",
      "epoch: 16 [2560/60000 (4%)]\t training loss: 0.000023\n",
      "epoch: 16 [2880/60000 (5%)]\t training loss: 0.000642\n",
      "epoch: 16 [3200/60000 (5%)]\t training loss: 0.007020\n",
      "epoch: 16 [3520/60000 (6%)]\t training loss: 0.001493\n",
      "epoch: 16 [3840/60000 (6%)]\t training loss: 0.000402\n",
      "epoch: 16 [4160/60000 (7%)]\t training loss: 0.000147\n",
      "epoch: 16 [4480/60000 (7%)]\t training loss: 0.000575\n",
      "epoch: 16 [4800/60000 (8%)]\t training loss: 0.097072\n",
      "epoch: 16 [5120/60000 (9%)]\t training loss: 0.030660\n",
      "epoch: 16 [5440/60000 (9%)]\t training loss: 0.000377\n",
      "epoch: 16 [5760/60000 (10%)]\t training loss: 0.012255\n",
      "epoch: 16 [6080/60000 (10%)]\t training loss: 0.005870\n",
      "epoch: 16 [6400/60000 (11%)]\t training loss: 0.010135\n",
      "epoch: 16 [6720/60000 (11%)]\t training loss: 0.012031\n",
      "epoch: 16 [7040/60000 (12%)]\t training loss: 0.002027\n",
      "epoch: 16 [7360/60000 (12%)]\t training loss: 0.000050\n",
      "epoch: 16 [7680/60000 (13%)]\t training loss: 0.008436\n",
      "epoch: 16 [8000/60000 (13%)]\t training loss: 0.054976\n",
      "epoch: 16 [8320/60000 (14%)]\t training loss: 0.000439\n",
      "epoch: 16 [8640/60000 (14%)]\t training loss: 0.041387\n",
      "epoch: 16 [8960/60000 (15%)]\t training loss: 0.000077\n",
      "epoch: 16 [9280/60000 (15%)]\t training loss: 0.001432\n",
      "epoch: 16 [9600/60000 (16%)]\t training loss: 0.000990\n",
      "epoch: 16 [9920/60000 (17%)]\t training loss: 0.000910\n",
      "epoch: 16 [10240/60000 (17%)]\t training loss: 0.002153\n",
      "epoch: 16 [10560/60000 (18%)]\t training loss: 0.000347\n",
      "epoch: 16 [10880/60000 (18%)]\t training loss: 0.019128\n",
      "epoch: 16 [11200/60000 (19%)]\t training loss: 0.007863\n",
      "epoch: 16 [11520/60000 (19%)]\t training loss: 0.037489\n",
      "epoch: 16 [11840/60000 (20%)]\t training loss: 0.000186\n",
      "epoch: 16 [12160/60000 (20%)]\t training loss: 0.000010\n",
      "epoch: 16 [12480/60000 (21%)]\t training loss: 0.001269\n",
      "epoch: 16 [12800/60000 (21%)]\t training loss: 0.000089\n",
      "epoch: 16 [13120/60000 (22%)]\t training loss: 0.030430\n",
      "epoch: 16 [13440/60000 (22%)]\t training loss: 0.040015\n",
      "epoch: 16 [13760/60000 (23%)]\t training loss: 0.001588\n",
      "epoch: 16 [14080/60000 (23%)]\t training loss: 0.002646\n",
      "epoch: 16 [14400/60000 (24%)]\t training loss: 0.000019\n",
      "epoch: 16 [14720/60000 (25%)]\t training loss: 0.000111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 [15040/60000 (25%)]\t training loss: 0.017356\n",
      "epoch: 16 [15360/60000 (26%)]\t training loss: 0.000020\n",
      "epoch: 16 [15680/60000 (26%)]\t training loss: 0.013095\n",
      "epoch: 16 [16000/60000 (27%)]\t training loss: 0.102120\n",
      "epoch: 16 [16320/60000 (27%)]\t training loss: 0.070410\n",
      "epoch: 16 [16640/60000 (28%)]\t training loss: 0.010587\n",
      "epoch: 16 [16960/60000 (28%)]\t training loss: 0.005561\n",
      "epoch: 16 [17280/60000 (29%)]\t training loss: 0.003808\n",
      "epoch: 16 [17600/60000 (29%)]\t training loss: 0.000062\n",
      "epoch: 16 [17920/60000 (30%)]\t training loss: 0.009419\n",
      "epoch: 16 [18240/60000 (30%)]\t training loss: 0.032586\n",
      "epoch: 16 [18560/60000 (31%)]\t training loss: 0.002117\n",
      "epoch: 16 [18880/60000 (31%)]\t training loss: 0.174347\n",
      "epoch: 16 [19200/60000 (32%)]\t training loss: 0.012560\n",
      "epoch: 16 [19520/60000 (33%)]\t training loss: 0.000013\n",
      "epoch: 16 [19840/60000 (33%)]\t training loss: 0.169913\n",
      "epoch: 16 [20160/60000 (34%)]\t training loss: 0.000110\n",
      "epoch: 16 [20480/60000 (34%)]\t training loss: 0.010224\n",
      "epoch: 16 [20800/60000 (35%)]\t training loss: 0.036913\n",
      "epoch: 16 [21120/60000 (35%)]\t training loss: 0.001596\n",
      "epoch: 16 [21440/60000 (36%)]\t training loss: 0.021667\n",
      "epoch: 16 [21760/60000 (36%)]\t training loss: 0.000387\n",
      "epoch: 16 [22080/60000 (37%)]\t training loss: 0.000179\n",
      "epoch: 16 [22400/60000 (37%)]\t training loss: 0.000017\n",
      "epoch: 16 [22720/60000 (38%)]\t training loss: 0.000061\n",
      "epoch: 16 [23040/60000 (38%)]\t training loss: 0.000897\n",
      "epoch: 16 [23360/60000 (39%)]\t training loss: 0.000478\n",
      "epoch: 16 [23680/60000 (39%)]\t training loss: 0.163423\n",
      "epoch: 16 [24000/60000 (40%)]\t training loss: 0.000118\n",
      "epoch: 16 [24320/60000 (41%)]\t training loss: 0.000341\n",
      "epoch: 16 [24640/60000 (41%)]\t training loss: 0.131545\n",
      "epoch: 16 [24960/60000 (42%)]\t training loss: 0.027353\n",
      "epoch: 16 [25280/60000 (42%)]\t training loss: 0.019237\n",
      "epoch: 16 [25600/60000 (43%)]\t training loss: 0.011711\n",
      "epoch: 16 [25920/60000 (43%)]\t training loss: 0.000256\n",
      "epoch: 16 [26240/60000 (44%)]\t training loss: 0.000059\n",
      "epoch: 16 [26560/60000 (44%)]\t training loss: 0.000188\n",
      "epoch: 16 [26880/60000 (45%)]\t training loss: 0.032166\n",
      "epoch: 16 [27200/60000 (45%)]\t training loss: 0.002329\n",
      "epoch: 16 [27520/60000 (46%)]\t training loss: 0.000172\n",
      "epoch: 16 [27840/60000 (46%)]\t training loss: 0.000003\n",
      "epoch: 16 [28160/60000 (47%)]\t training loss: 0.000016\n",
      "epoch: 16 [28480/60000 (47%)]\t training loss: 0.012595\n",
      "epoch: 16 [28800/60000 (48%)]\t training loss: 0.000210\n",
      "epoch: 16 [29120/60000 (49%)]\t training loss: 0.000922\n",
      "epoch: 16 [29440/60000 (49%)]\t training loss: 0.000403\n",
      "epoch: 16 [29760/60000 (50%)]\t training loss: 0.000448\n",
      "epoch: 16 [30080/60000 (50%)]\t training loss: 0.039465\n",
      "epoch: 16 [30400/60000 (51%)]\t training loss: 0.000420\n",
      "epoch: 16 [30720/60000 (51%)]\t training loss: 0.000316\n",
      "epoch: 16 [31040/60000 (52%)]\t training loss: 0.029707\n",
      "epoch: 16 [31360/60000 (52%)]\t training loss: 0.039502\n",
      "epoch: 16 [31680/60000 (53%)]\t training loss: 0.003305\n",
      "epoch: 16 [32000/60000 (53%)]\t training loss: 0.000081\n",
      "epoch: 16 [32320/60000 (54%)]\t training loss: 0.086642\n",
      "epoch: 16 [32640/60000 (54%)]\t training loss: 0.001956\n",
      "epoch: 16 [32960/60000 (55%)]\t training loss: 0.002593\n",
      "epoch: 16 [33280/60000 (55%)]\t training loss: 0.000176\n",
      "epoch: 16 [33600/60000 (56%)]\t training loss: 0.026723\n",
      "epoch: 16 [33920/60000 (57%)]\t training loss: 0.002673\n",
      "epoch: 16 [34240/60000 (57%)]\t training loss: 0.000708\n",
      "epoch: 16 [34560/60000 (58%)]\t training loss: 0.000233\n",
      "epoch: 16 [34880/60000 (58%)]\t training loss: 0.000074\n",
      "epoch: 16 [35200/60000 (59%)]\t training loss: 0.013502\n",
      "epoch: 16 [35520/60000 (59%)]\t training loss: 0.000677\n",
      "epoch: 16 [35840/60000 (60%)]\t training loss: 0.000158\n",
      "epoch: 16 [36160/60000 (60%)]\t training loss: 0.107758\n",
      "epoch: 16 [36480/60000 (61%)]\t training loss: 0.203787\n",
      "epoch: 16 [36800/60000 (61%)]\t training loss: 0.016649\n",
      "epoch: 16 [37120/60000 (62%)]\t training loss: 0.000313\n",
      "epoch: 16 [37440/60000 (62%)]\t training loss: 0.001632\n",
      "epoch: 16 [37760/60000 (63%)]\t training loss: 0.004245\n",
      "epoch: 16 [38080/60000 (63%)]\t training loss: 0.001558\n",
      "epoch: 16 [38400/60000 (64%)]\t training loss: 0.001202\n",
      "epoch: 16 [38720/60000 (65%)]\t training loss: 0.143657\n",
      "epoch: 16 [39040/60000 (65%)]\t training loss: 0.000784\n",
      "epoch: 16 [39360/60000 (66%)]\t training loss: 0.000027\n",
      "epoch: 16 [39680/60000 (66%)]\t training loss: 0.010961\n",
      "epoch: 16 [40000/60000 (67%)]\t training loss: 0.000649\n",
      "epoch: 16 [40320/60000 (67%)]\t training loss: 0.024578\n",
      "epoch: 16 [40640/60000 (68%)]\t training loss: 0.001449\n",
      "epoch: 16 [40960/60000 (68%)]\t training loss: 0.002821\n",
      "epoch: 16 [41280/60000 (69%)]\t training loss: 0.001400\n",
      "epoch: 16 [41600/60000 (69%)]\t training loss: 0.000232\n",
      "epoch: 16 [41920/60000 (70%)]\t training loss: 0.000102\n",
      "epoch: 16 [42240/60000 (70%)]\t training loss: 0.039833\n",
      "epoch: 16 [42560/60000 (71%)]\t training loss: 0.000982\n",
      "epoch: 16 [42880/60000 (71%)]\t training loss: 0.001804\n",
      "epoch: 16 [43200/60000 (72%)]\t training loss: 0.000290\n",
      "epoch: 16 [43520/60000 (73%)]\t training loss: 0.011480\n",
      "epoch: 16 [43840/60000 (73%)]\t training loss: 0.000123\n",
      "epoch: 16 [44160/60000 (74%)]\t training loss: 0.000288\n",
      "epoch: 16 [44480/60000 (74%)]\t training loss: 0.005231\n",
      "epoch: 16 [44800/60000 (75%)]\t training loss: 0.000866\n",
      "epoch: 16 [45120/60000 (75%)]\t training loss: 0.000267\n",
      "epoch: 16 [45440/60000 (76%)]\t training loss: 0.000047\n",
      "epoch: 16 [45760/60000 (76%)]\t training loss: 0.009337\n",
      "epoch: 16 [46080/60000 (77%)]\t training loss: 0.006087\n",
      "epoch: 16 [46400/60000 (77%)]\t training loss: 0.008746\n",
      "epoch: 16 [46720/60000 (78%)]\t training loss: 0.001928\n",
      "epoch: 16 [47040/60000 (78%)]\t training loss: 0.001976\n",
      "epoch: 16 [47360/60000 (79%)]\t training loss: 0.000051\n",
      "epoch: 16 [47680/60000 (79%)]\t training loss: 0.000001\n",
      "epoch: 16 [48000/60000 (80%)]\t training loss: 0.002361\n",
      "epoch: 16 [48320/60000 (81%)]\t training loss: 0.000246\n",
      "epoch: 16 [48640/60000 (81%)]\t training loss: 0.000301\n",
      "epoch: 16 [48960/60000 (82%)]\t training loss: 0.035106\n",
      "epoch: 16 [49280/60000 (82%)]\t training loss: 0.000533\n",
      "epoch: 16 [49600/60000 (83%)]\t training loss: 0.076112\n",
      "epoch: 16 [49920/60000 (83%)]\t training loss: 0.001744\n",
      "epoch: 16 [50240/60000 (84%)]\t training loss: 0.001707\n",
      "epoch: 16 [50560/60000 (84%)]\t training loss: 0.000290\n",
      "epoch: 16 [50880/60000 (85%)]\t training loss: 0.069581\n",
      "epoch: 16 [51200/60000 (85%)]\t training loss: 0.058617\n",
      "epoch: 16 [51520/60000 (86%)]\t training loss: 0.000003\n",
      "epoch: 16 [51840/60000 (86%)]\t training loss: 0.000645\n",
      "epoch: 16 [52160/60000 (87%)]\t training loss: 0.000223\n",
      "epoch: 16 [52480/60000 (87%)]\t training loss: 0.000483\n",
      "epoch: 16 [52800/60000 (88%)]\t training loss: 0.000008\n",
      "epoch: 16 [53120/60000 (89%)]\t training loss: 0.000110\n",
      "epoch: 16 [53440/60000 (89%)]\t training loss: 0.000479\n",
      "epoch: 16 [53760/60000 (90%)]\t training loss: 0.000061\n",
      "epoch: 16 [54080/60000 (90%)]\t training loss: 0.013876\n",
      "epoch: 16 [54400/60000 (91%)]\t training loss: 0.006816\n",
      "epoch: 16 [54720/60000 (91%)]\t training loss: 0.000245\n",
      "epoch: 16 [55040/60000 (92%)]\t training loss: 0.003004\n",
      "epoch: 16 [55360/60000 (92%)]\t training loss: 0.000222\n",
      "epoch: 16 [55680/60000 (93%)]\t training loss: 0.006980\n",
      "epoch: 16 [56000/60000 (93%)]\t training loss: 0.000194\n",
      "epoch: 16 [56320/60000 (94%)]\t training loss: 0.005673\n",
      "epoch: 16 [56640/60000 (94%)]\t training loss: 0.001241\n",
      "epoch: 16 [56960/60000 (95%)]\t training loss: 0.080572\n",
      "epoch: 16 [57280/60000 (95%)]\t training loss: 0.211891\n",
      "epoch: 16 [57600/60000 (96%)]\t training loss: 0.184380\n",
      "epoch: 16 [57920/60000 (97%)]\t training loss: 0.000304\n",
      "epoch: 16 [58240/60000 (97%)]\t training loss: 0.000120\n",
      "epoch: 16 [58560/60000 (98%)]\t training loss: 0.000051\n",
      "epoch: 16 [58880/60000 (98%)]\t training loss: 0.000009\n",
      "epoch: 16 [59200/60000 (99%)]\t training loss: 0.000092\n",
      "epoch: 16 [59520/60000 (99%)]\t training loss: 0.000281\n",
      "epoch: 16 [59840/60000 (100%)]\t training loss: 0.000518\n",
      "\n",
      "Test dataset: Overall Loss: 0.0376, Overall Accuracy: 9915/10000 (99%)\n",
      "\n",
      "epoch: 17 [0/60000 (0%)]\t training loss: 0.002286\n",
      "epoch: 17 [320/60000 (1%)]\t training loss: 0.000407\n",
      "epoch: 17 [640/60000 (1%)]\t training loss: 0.000184\n",
      "epoch: 17 [960/60000 (2%)]\t training loss: 0.000050\n",
      "epoch: 17 [1280/60000 (2%)]\t training loss: 0.007432\n",
      "epoch: 17 [1600/60000 (3%)]\t training loss: 0.000192\n",
      "epoch: 17 [1920/60000 (3%)]\t training loss: 0.000106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 [2240/60000 (4%)]\t training loss: 0.000582\n",
      "epoch: 17 [2560/60000 (4%)]\t training loss: 0.000509\n",
      "epoch: 17 [2880/60000 (5%)]\t training loss: 0.000036\n",
      "epoch: 17 [3200/60000 (5%)]\t training loss: 0.040218\n",
      "epoch: 17 [3520/60000 (6%)]\t training loss: 0.005363\n",
      "epoch: 17 [3840/60000 (6%)]\t training loss: 0.000854\n",
      "epoch: 17 [4160/60000 (7%)]\t training loss: 0.002422\n",
      "epoch: 17 [4480/60000 (7%)]\t training loss: 0.005168\n",
      "epoch: 17 [4800/60000 (8%)]\t training loss: 0.004487\n",
      "epoch: 17 [5120/60000 (9%)]\t training loss: 0.000521\n",
      "epoch: 17 [5440/60000 (9%)]\t training loss: 0.001204\n",
      "epoch: 17 [5760/60000 (10%)]\t training loss: 0.038512\n",
      "epoch: 17 [6080/60000 (10%)]\t training loss: 0.010272\n",
      "epoch: 17 [6400/60000 (11%)]\t training loss: 0.002769\n",
      "epoch: 17 [6720/60000 (11%)]\t training loss: 0.018181\n",
      "epoch: 17 [7040/60000 (12%)]\t training loss: 0.000751\n",
      "epoch: 17 [7360/60000 (12%)]\t training loss: 0.006902\n",
      "epoch: 17 [7680/60000 (13%)]\t training loss: 0.000021\n",
      "epoch: 17 [8000/60000 (13%)]\t training loss: 0.000052\n",
      "epoch: 17 [8320/60000 (14%)]\t training loss: 0.001046\n",
      "epoch: 17 [8640/60000 (14%)]\t training loss: 0.256071\n",
      "epoch: 17 [8960/60000 (15%)]\t training loss: 0.000581\n",
      "epoch: 17 [9280/60000 (15%)]\t training loss: 0.003607\n",
      "epoch: 17 [9600/60000 (16%)]\t training loss: 0.040761\n",
      "epoch: 17 [9920/60000 (17%)]\t training loss: 0.001819\n",
      "epoch: 17 [10240/60000 (17%)]\t training loss: 0.000031\n",
      "epoch: 17 [10560/60000 (18%)]\t training loss: 0.000130\n",
      "epoch: 17 [10880/60000 (18%)]\t training loss: 0.004852\n",
      "epoch: 17 [11200/60000 (19%)]\t training loss: 0.035502\n",
      "epoch: 17 [11520/60000 (19%)]\t training loss: 0.042638\n",
      "epoch: 17 [11840/60000 (20%)]\t training loss: 0.003971\n",
      "epoch: 17 [12160/60000 (20%)]\t training loss: 0.000715\n",
      "epoch: 17 [12480/60000 (21%)]\t training loss: 0.000010\n",
      "epoch: 17 [12800/60000 (21%)]\t training loss: 0.000014\n",
      "epoch: 17 [13120/60000 (22%)]\t training loss: 0.275238\n",
      "epoch: 17 [13440/60000 (22%)]\t training loss: 0.000743\n",
      "epoch: 17 [13760/60000 (23%)]\t training loss: 0.022576\n",
      "epoch: 17 [14080/60000 (23%)]\t training loss: 0.000005\n",
      "epoch: 17 [14400/60000 (24%)]\t training loss: 0.000005\n",
      "epoch: 17 [14720/60000 (25%)]\t training loss: 0.004685\n",
      "epoch: 17 [15040/60000 (25%)]\t training loss: 0.107115\n",
      "epoch: 17 [15360/60000 (26%)]\t training loss: 0.000440\n",
      "epoch: 17 [15680/60000 (26%)]\t training loss: 0.000014\n",
      "epoch: 17 [16000/60000 (27%)]\t training loss: 0.000261\n",
      "epoch: 17 [16320/60000 (27%)]\t training loss: 0.001130\n",
      "epoch: 17 [16640/60000 (28%)]\t training loss: 0.001598\n",
      "epoch: 17 [16960/60000 (28%)]\t training loss: 0.000053\n",
      "epoch: 17 [17280/60000 (29%)]\t training loss: 0.000002\n",
      "epoch: 17 [17600/60000 (29%)]\t training loss: 0.067989\n",
      "epoch: 17 [17920/60000 (30%)]\t training loss: 0.000847\n",
      "epoch: 17 [18240/60000 (30%)]\t training loss: 0.002270\n",
      "epoch: 17 [18560/60000 (31%)]\t training loss: 0.023084\n",
      "epoch: 17 [18880/60000 (31%)]\t training loss: 0.000158\n",
      "epoch: 17 [19200/60000 (32%)]\t training loss: 0.135160\n",
      "epoch: 17 [19520/60000 (33%)]\t training loss: 0.175027\n",
      "epoch: 17 [19840/60000 (33%)]\t training loss: 0.012157\n",
      "epoch: 17 [20160/60000 (34%)]\t training loss: 0.005342\n",
      "epoch: 17 [20480/60000 (34%)]\t training loss: 0.001602\n",
      "epoch: 17 [20800/60000 (35%)]\t training loss: 0.000634\n",
      "epoch: 17 [21120/60000 (35%)]\t training loss: 0.102823\n",
      "epoch: 17 [21440/60000 (36%)]\t training loss: 0.000006\n",
      "epoch: 17 [21760/60000 (36%)]\t training loss: 0.000595\n",
      "epoch: 17 [22080/60000 (37%)]\t training loss: 0.002676\n",
      "epoch: 17 [22400/60000 (37%)]\t training loss: 0.000202\n",
      "epoch: 17 [22720/60000 (38%)]\t training loss: 0.011230\n",
      "epoch: 17 [23040/60000 (38%)]\t training loss: 0.101312\n",
      "epoch: 17 [23360/60000 (39%)]\t training loss: 0.000236\n",
      "epoch: 17 [23680/60000 (39%)]\t training loss: 0.000079\n",
      "epoch: 17 [24000/60000 (40%)]\t training loss: 0.004963\n",
      "epoch: 17 [24320/60000 (41%)]\t training loss: 0.002521\n",
      "epoch: 17 [24640/60000 (41%)]\t training loss: 0.000554\n",
      "epoch: 17 [24960/60000 (42%)]\t training loss: 0.000512\n",
      "epoch: 17 [25280/60000 (42%)]\t training loss: 0.074416\n",
      "epoch: 17 [25600/60000 (43%)]\t training loss: 0.000234\n",
      "epoch: 17 [25920/60000 (43%)]\t training loss: 0.001026\n",
      "epoch: 17 [26240/60000 (44%)]\t training loss: 0.000175\n",
      "epoch: 17 [26560/60000 (44%)]\t training loss: 0.015755\n",
      "epoch: 17 [26880/60000 (45%)]\t training loss: 0.000896\n",
      "epoch: 17 [27200/60000 (45%)]\t training loss: 0.028174\n",
      "epoch: 17 [27520/60000 (46%)]\t training loss: 0.000005\n",
      "epoch: 17 [27840/60000 (46%)]\t training loss: 0.001799\n",
      "epoch: 17 [28160/60000 (47%)]\t training loss: 0.004838\n",
      "epoch: 17 [28480/60000 (47%)]\t training loss: 0.002723\n",
      "epoch: 17 [28800/60000 (48%)]\t training loss: 0.000135\n",
      "epoch: 17 [29120/60000 (49%)]\t training loss: 0.000036\n",
      "epoch: 17 [29440/60000 (49%)]\t training loss: 0.004707\n",
      "epoch: 17 [29760/60000 (50%)]\t training loss: 0.040201\n",
      "epoch: 17 [30080/60000 (50%)]\t training loss: 0.021269\n",
      "epoch: 17 [30400/60000 (51%)]\t training loss: 0.104339\n",
      "epoch: 17 [30720/60000 (51%)]\t training loss: 0.000061\n",
      "epoch: 17 [31040/60000 (52%)]\t training loss: 0.000520\n",
      "epoch: 17 [31360/60000 (52%)]\t training loss: 0.000042\n",
      "epoch: 17 [31680/60000 (53%)]\t training loss: 0.010140\n",
      "epoch: 17 [32000/60000 (53%)]\t training loss: 0.000003\n",
      "epoch: 17 [32320/60000 (54%)]\t training loss: 0.000305\n",
      "epoch: 17 [32640/60000 (54%)]\t training loss: 0.000105\n",
      "epoch: 17 [32960/60000 (55%)]\t training loss: 0.043256\n",
      "epoch: 17 [33280/60000 (55%)]\t training loss: 0.010722\n",
      "epoch: 17 [33600/60000 (56%)]\t training loss: 0.003123\n",
      "epoch: 17 [33920/60000 (57%)]\t training loss: 0.145660\n",
      "epoch: 17 [34240/60000 (57%)]\t training loss: 0.006424\n",
      "epoch: 17 [34560/60000 (58%)]\t training loss: 0.001603\n",
      "epoch: 17 [34880/60000 (58%)]\t training loss: 0.000012\n",
      "epoch: 17 [35200/60000 (59%)]\t training loss: 0.000823\n",
      "epoch: 17 [35520/60000 (59%)]\t training loss: 0.008775\n",
      "epoch: 17 [35840/60000 (60%)]\t training loss: 0.000098\n",
      "epoch: 17 [36160/60000 (60%)]\t training loss: 0.006025\n",
      "epoch: 17 [36480/60000 (61%)]\t training loss: 0.003466\n",
      "epoch: 17 [36800/60000 (61%)]\t training loss: 0.244447\n",
      "epoch: 17 [37120/60000 (62%)]\t training loss: 0.023445\n",
      "epoch: 17 [37440/60000 (62%)]\t training loss: 0.022713\n",
      "epoch: 17 [37760/60000 (63%)]\t training loss: 0.000465\n",
      "epoch: 17 [38080/60000 (63%)]\t training loss: 0.016495\n",
      "epoch: 17 [38400/60000 (64%)]\t training loss: 0.000584\n",
      "epoch: 17 [38720/60000 (65%)]\t training loss: 0.240062\n",
      "epoch: 17 [39040/60000 (65%)]\t training loss: 0.000064\n",
      "epoch: 17 [39360/60000 (66%)]\t training loss: 0.048128\n",
      "epoch: 17 [39680/60000 (66%)]\t training loss: 0.001020\n",
      "epoch: 17 [40000/60000 (67%)]\t training loss: 0.019903\n",
      "epoch: 17 [40320/60000 (67%)]\t training loss: 0.003911\n",
      "epoch: 17 [40640/60000 (68%)]\t training loss: 0.042381\n",
      "epoch: 17 [40960/60000 (68%)]\t training loss: 0.000361\n",
      "epoch: 17 [41280/60000 (69%)]\t training loss: 0.000417\n",
      "epoch: 17 [41600/60000 (69%)]\t training loss: 0.047515\n",
      "epoch: 17 [41920/60000 (70%)]\t training loss: 0.026756\n",
      "epoch: 17 [42240/60000 (70%)]\t training loss: 0.000917\n",
      "epoch: 17 [42560/60000 (71%)]\t training loss: 0.004100\n",
      "epoch: 17 [42880/60000 (71%)]\t training loss: 0.003720\n",
      "epoch: 17 [43200/60000 (72%)]\t training loss: 0.000815\n",
      "epoch: 17 [43520/60000 (73%)]\t training loss: 0.060648\n",
      "epoch: 17 [43840/60000 (73%)]\t training loss: 0.007129\n",
      "epoch: 17 [44160/60000 (74%)]\t training loss: 0.474524\n",
      "epoch: 17 [44480/60000 (74%)]\t training loss: 0.016097\n",
      "epoch: 17 [44800/60000 (75%)]\t training loss: 0.009971\n",
      "epoch: 17 [45120/60000 (75%)]\t training loss: 0.000764\n",
      "epoch: 17 [45440/60000 (76%)]\t training loss: 0.026190\n",
      "epoch: 17 [45760/60000 (76%)]\t training loss: 0.000295\n",
      "epoch: 17 [46080/60000 (77%)]\t training loss: 0.090339\n",
      "epoch: 17 [46400/60000 (77%)]\t training loss: 0.250371\n",
      "epoch: 17 [46720/60000 (78%)]\t training loss: 0.006305\n",
      "epoch: 17 [47040/60000 (78%)]\t training loss: 0.000064\n",
      "epoch: 17 [47360/60000 (79%)]\t training loss: 0.000015\n",
      "epoch: 17 [47680/60000 (79%)]\t training loss: 0.005017\n",
      "epoch: 17 [48000/60000 (80%)]\t training loss: 0.000179\n",
      "epoch: 17 [48320/60000 (81%)]\t training loss: 0.000196\n",
      "epoch: 17 [48640/60000 (81%)]\t training loss: 0.001807\n",
      "epoch: 17 [48960/60000 (82%)]\t training loss: 0.000113\n",
      "epoch: 17 [49280/60000 (82%)]\t training loss: 0.006243\n",
      "epoch: 17 [49600/60000 (83%)]\t training loss: 0.002396\n",
      "epoch: 17 [49920/60000 (83%)]\t training loss: 0.000045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 [50240/60000 (84%)]\t training loss: 0.032945\n",
      "epoch: 17 [50560/60000 (84%)]\t training loss: 0.000819\n",
      "epoch: 17 [50880/60000 (85%)]\t training loss: 0.005797\n",
      "epoch: 17 [51200/60000 (85%)]\t training loss: 0.023301\n",
      "epoch: 17 [51520/60000 (86%)]\t training loss: 0.000001\n",
      "epoch: 17 [51840/60000 (86%)]\t training loss: 0.061383\n",
      "epoch: 17 [52160/60000 (87%)]\t training loss: 0.002047\n",
      "epoch: 17 [52480/60000 (87%)]\t training loss: 0.000289\n",
      "epoch: 17 [52800/60000 (88%)]\t training loss: 0.000081\n",
      "epoch: 17 [53120/60000 (89%)]\t training loss: 0.034090\n",
      "epoch: 17 [53440/60000 (89%)]\t training loss: 0.000012\n",
      "epoch: 17 [53760/60000 (90%)]\t training loss: 0.000748\n",
      "epoch: 17 [54080/60000 (90%)]\t training loss: 0.000056\n",
      "epoch: 17 [54400/60000 (91%)]\t training loss: 0.000276\n",
      "epoch: 17 [54720/60000 (91%)]\t training loss: 0.000209\n",
      "epoch: 17 [55040/60000 (92%)]\t training loss: 0.000002\n",
      "epoch: 17 [55360/60000 (92%)]\t training loss: 0.088363\n",
      "epoch: 17 [55680/60000 (93%)]\t training loss: 0.000321\n",
      "epoch: 17 [56000/60000 (93%)]\t training loss: 0.343511\n",
      "epoch: 17 [56320/60000 (94%)]\t training loss: 0.002788\n",
      "epoch: 17 [56640/60000 (94%)]\t training loss: 0.067359\n",
      "epoch: 17 [56960/60000 (95%)]\t training loss: 0.000493\n",
      "epoch: 17 [57280/60000 (95%)]\t training loss: 0.000924\n",
      "epoch: 17 [57600/60000 (96%)]\t training loss: 0.000363\n",
      "epoch: 17 [57920/60000 (97%)]\t training loss: 0.000396\n",
      "epoch: 17 [58240/60000 (97%)]\t training loss: 0.068108\n",
      "epoch: 17 [58560/60000 (98%)]\t training loss: 0.001339\n",
      "epoch: 17 [58880/60000 (98%)]\t training loss: 0.000402\n",
      "epoch: 17 [59200/60000 (99%)]\t training loss: 0.000211\n",
      "epoch: 17 [59520/60000 (99%)]\t training loss: 0.074337\n",
      "epoch: 17 [59840/60000 (100%)]\t training loss: 0.000513\n",
      "\n",
      "Test dataset: Overall Loss: 0.0399, Overall Accuracy: 9907/10000 (99%)\n",
      "\n",
      "epoch: 18 [0/60000 (0%)]\t training loss: 0.050997\n",
      "epoch: 18 [320/60000 (1%)]\t training loss: 0.002604\n",
      "epoch: 18 [640/60000 (1%)]\t training loss: 0.004251\n",
      "epoch: 18 [960/60000 (2%)]\t training loss: 0.000305\n",
      "epoch: 18 [1280/60000 (2%)]\t training loss: 0.002918\n",
      "epoch: 18 [1600/60000 (3%)]\t training loss: 0.000575\n",
      "epoch: 18 [1920/60000 (3%)]\t training loss: 0.000096\n",
      "epoch: 18 [2240/60000 (4%)]\t training loss: 0.000062\n",
      "epoch: 18 [2560/60000 (4%)]\t training loss: 0.000126\n",
      "epoch: 18 [2880/60000 (5%)]\t training loss: 0.000026\n",
      "epoch: 18 [3200/60000 (5%)]\t training loss: 0.000091\n",
      "epoch: 18 [3520/60000 (6%)]\t training loss: 0.000087\n",
      "epoch: 18 [3840/60000 (6%)]\t training loss: 0.000918\n",
      "epoch: 18 [4160/60000 (7%)]\t training loss: 0.000091\n",
      "epoch: 18 [4480/60000 (7%)]\t training loss: 0.012531\n",
      "epoch: 18 [4800/60000 (8%)]\t training loss: 0.001528\n",
      "epoch: 18 [5120/60000 (9%)]\t training loss: 0.000439\n",
      "epoch: 18 [5440/60000 (9%)]\t training loss: 0.000400\n",
      "epoch: 18 [5760/60000 (10%)]\t training loss: 0.000077\n",
      "epoch: 18 [6080/60000 (10%)]\t training loss: 0.000010\n",
      "epoch: 18 [6400/60000 (11%)]\t training loss: 0.262463\n",
      "epoch: 18 [6720/60000 (11%)]\t training loss: 0.000562\n",
      "epoch: 18 [7040/60000 (12%)]\t training loss: 0.000955\n",
      "epoch: 18 [7360/60000 (12%)]\t training loss: 0.000120\n",
      "epoch: 18 [7680/60000 (13%)]\t training loss: 0.000481\n",
      "epoch: 18 [8000/60000 (13%)]\t training loss: 0.000244\n",
      "epoch: 18 [8320/60000 (14%)]\t training loss: 0.000872\n",
      "epoch: 18 [8640/60000 (14%)]\t training loss: 0.000841\n",
      "epoch: 18 [8960/60000 (15%)]\t training loss: 0.000002\n",
      "epoch: 18 [9280/60000 (15%)]\t training loss: 0.000344\n",
      "epoch: 18 [9600/60000 (16%)]\t training loss: 0.007107\n",
      "epoch: 18 [9920/60000 (17%)]\t training loss: 0.000025\n",
      "epoch: 18 [10240/60000 (17%)]\t training loss: 0.000769\n",
      "epoch: 18 [10560/60000 (18%)]\t training loss: 0.247850\n",
      "epoch: 18 [10880/60000 (18%)]\t training loss: 0.015567\n",
      "epoch: 18 [11200/60000 (19%)]\t training loss: 0.000817\n",
      "epoch: 18 [11520/60000 (19%)]\t training loss: 0.003377\n",
      "epoch: 18 [11840/60000 (20%)]\t training loss: 0.005050\n",
      "epoch: 18 [12160/60000 (20%)]\t training loss: 0.003318\n",
      "epoch: 18 [12480/60000 (21%)]\t training loss: 0.000142\n",
      "epoch: 18 [12800/60000 (21%)]\t training loss: 0.001538\n",
      "epoch: 18 [13120/60000 (22%)]\t training loss: 0.002158\n",
      "epoch: 18 [13440/60000 (22%)]\t training loss: 0.000010\n",
      "epoch: 18 [13760/60000 (23%)]\t training loss: 0.002778\n",
      "epoch: 18 [14080/60000 (23%)]\t training loss: 0.000976\n",
      "epoch: 18 [14400/60000 (24%)]\t training loss: 0.000016\n",
      "epoch: 18 [14720/60000 (25%)]\t training loss: 0.000346\n",
      "epoch: 18 [15040/60000 (25%)]\t training loss: 0.000053\n",
      "epoch: 18 [15360/60000 (26%)]\t training loss: 0.003504\n",
      "epoch: 18 [15680/60000 (26%)]\t training loss: 0.000313\n",
      "epoch: 18 [16000/60000 (27%)]\t training loss: 0.000019\n",
      "epoch: 18 [16320/60000 (27%)]\t training loss: 0.109472\n",
      "epoch: 18 [16640/60000 (28%)]\t training loss: 0.000499\n",
      "epoch: 18 [16960/60000 (28%)]\t training loss: 0.001403\n",
      "epoch: 18 [17280/60000 (29%)]\t training loss: 0.000078\n",
      "epoch: 18 [17600/60000 (29%)]\t training loss: 0.045375\n",
      "epoch: 18 [17920/60000 (30%)]\t training loss: 0.001537\n",
      "epoch: 18 [18240/60000 (30%)]\t training loss: 0.000518\n",
      "epoch: 18 [18560/60000 (31%)]\t training loss: 0.000053\n",
      "epoch: 18 [18880/60000 (31%)]\t training loss: 0.000230\n",
      "epoch: 18 [19200/60000 (32%)]\t training loss: 0.000328\n",
      "epoch: 18 [19520/60000 (33%)]\t training loss: 0.010663\n",
      "epoch: 18 [19840/60000 (33%)]\t training loss: 0.000018\n",
      "epoch: 18 [20160/60000 (34%)]\t training loss: 0.000068\n",
      "epoch: 18 [20480/60000 (34%)]\t training loss: 0.026583\n",
      "epoch: 18 [20800/60000 (35%)]\t training loss: 0.000723\n",
      "epoch: 18 [21120/60000 (35%)]\t training loss: 0.048793\n",
      "epoch: 18 [21440/60000 (36%)]\t training loss: 0.030121\n",
      "epoch: 18 [21760/60000 (36%)]\t training loss: 0.000726\n",
      "epoch: 18 [22080/60000 (37%)]\t training loss: 0.022174\n",
      "epoch: 18 [22400/60000 (37%)]\t training loss: 0.019391\n",
      "epoch: 18 [22720/60000 (38%)]\t training loss: 0.004462\n",
      "epoch: 18 [23040/60000 (38%)]\t training loss: 0.000299\n",
      "epoch: 18 [23360/60000 (39%)]\t training loss: 0.003176\n",
      "epoch: 18 [23680/60000 (39%)]\t training loss: 0.000514\n",
      "epoch: 18 [24000/60000 (40%)]\t training loss: 0.001949\n",
      "epoch: 18 [24320/60000 (41%)]\t training loss: 0.000050\n",
      "epoch: 18 [24640/60000 (41%)]\t training loss: 0.003480\n",
      "epoch: 18 [24960/60000 (42%)]\t training loss: 0.004082\n",
      "epoch: 18 [25280/60000 (42%)]\t training loss: 0.000289\n",
      "epoch: 18 [25600/60000 (43%)]\t training loss: 0.013740\n",
      "epoch: 18 [25920/60000 (43%)]\t training loss: 0.115202\n",
      "epoch: 18 [26240/60000 (44%)]\t training loss: 0.000329\n",
      "epoch: 18 [26560/60000 (44%)]\t training loss: 0.000272\n",
      "epoch: 18 [26880/60000 (45%)]\t training loss: 0.000390\n",
      "epoch: 18 [27200/60000 (45%)]\t training loss: 0.125970\n",
      "epoch: 18 [27520/60000 (46%)]\t training loss: 0.097685\n",
      "epoch: 18 [27840/60000 (46%)]\t training loss: 0.004469\n",
      "epoch: 18 [28160/60000 (47%)]\t training loss: 0.125900\n",
      "epoch: 18 [28480/60000 (47%)]\t training loss: 0.001531\n",
      "epoch: 18 [28800/60000 (48%)]\t training loss: 0.106530\n",
      "epoch: 18 [29120/60000 (49%)]\t training loss: 0.004150\n",
      "epoch: 18 [29440/60000 (49%)]\t training loss: 0.001398\n",
      "epoch: 18 [29760/60000 (50%)]\t training loss: 0.000874\n",
      "epoch: 18 [30080/60000 (50%)]\t training loss: 0.029451\n",
      "epoch: 18 [30400/60000 (51%)]\t training loss: 0.001279\n",
      "epoch: 18 [30720/60000 (51%)]\t training loss: 0.003346\n",
      "epoch: 18 [31040/60000 (52%)]\t training loss: 0.010954\n",
      "epoch: 18 [31360/60000 (52%)]\t training loss: 0.000006\n",
      "epoch: 18 [31680/60000 (53%)]\t training loss: 0.000401\n",
      "epoch: 18 [32000/60000 (53%)]\t training loss: 0.030338\n",
      "epoch: 18 [32320/60000 (54%)]\t training loss: 0.204471\n",
      "epoch: 18 [32640/60000 (54%)]\t training loss: 0.010632\n",
      "epoch: 18 [32960/60000 (55%)]\t training loss: 0.005976\n",
      "epoch: 18 [33280/60000 (55%)]\t training loss: 0.065328\n",
      "epoch: 18 [33600/60000 (56%)]\t training loss: 0.001739\n",
      "epoch: 18 [33920/60000 (57%)]\t training loss: 0.000018\n",
      "epoch: 18 [34240/60000 (57%)]\t training loss: 0.011833\n",
      "epoch: 18 [34560/60000 (58%)]\t training loss: 0.000029\n",
      "epoch: 18 [34880/60000 (58%)]\t training loss: 0.000208\n",
      "epoch: 18 [35200/60000 (59%)]\t training loss: 0.000852\n",
      "epoch: 18 [35520/60000 (59%)]\t training loss: 0.002885\n",
      "epoch: 18 [35840/60000 (60%)]\t training loss: 0.001242\n",
      "epoch: 18 [36160/60000 (60%)]\t training loss: 0.010767\n",
      "epoch: 18 [36480/60000 (61%)]\t training loss: 0.015630\n",
      "epoch: 18 [36800/60000 (61%)]\t training loss: 0.017026\n",
      "epoch: 18 [37120/60000 (62%)]\t training loss: 0.000128\n",
      "epoch: 18 [37440/60000 (62%)]\t training loss: 0.313307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 [37760/60000 (63%)]\t training loss: 0.026158\n",
      "epoch: 18 [38080/60000 (63%)]\t training loss: 0.000009\n",
      "epoch: 18 [38400/60000 (64%)]\t training loss: 0.000611\n",
      "epoch: 18 [38720/60000 (65%)]\t training loss: 0.000028\n",
      "epoch: 18 [39040/60000 (65%)]\t training loss: 0.000704\n",
      "epoch: 18 [39360/60000 (66%)]\t training loss: 0.069232\n",
      "epoch: 18 [39680/60000 (66%)]\t training loss: 0.004169\n",
      "epoch: 18 [40000/60000 (67%)]\t training loss: 0.000003\n",
      "epoch: 18 [40320/60000 (67%)]\t training loss: 0.031080\n",
      "epoch: 18 [40640/60000 (68%)]\t training loss: 0.000014\n",
      "epoch: 18 [40960/60000 (68%)]\t training loss: 0.007975\n",
      "epoch: 18 [41280/60000 (69%)]\t training loss: 0.000135\n",
      "epoch: 18 [41600/60000 (69%)]\t training loss: 0.090104\n",
      "epoch: 18 [41920/60000 (70%)]\t training loss: 0.001513\n",
      "epoch: 18 [42240/60000 (70%)]\t training loss: 0.000019\n",
      "epoch: 18 [42560/60000 (71%)]\t training loss: 0.000129\n",
      "epoch: 18 [42880/60000 (71%)]\t training loss: 0.027592\n",
      "epoch: 18 [43200/60000 (72%)]\t training loss: 0.000669\n",
      "epoch: 18 [43520/60000 (73%)]\t training loss: 0.000296\n",
      "epoch: 18 [43840/60000 (73%)]\t training loss: 0.000327\n",
      "epoch: 18 [44160/60000 (74%)]\t training loss: 0.000459\n",
      "epoch: 18 [44480/60000 (74%)]\t training loss: 0.005337\n",
      "epoch: 18 [44800/60000 (75%)]\t training loss: 0.032377\n",
      "epoch: 18 [45120/60000 (75%)]\t training loss: 0.367876\n",
      "epoch: 18 [45440/60000 (76%)]\t training loss: 0.002617\n",
      "epoch: 18 [45760/60000 (76%)]\t training loss: 0.001317\n",
      "epoch: 18 [46080/60000 (77%)]\t training loss: 0.011167\n",
      "epoch: 18 [46400/60000 (77%)]\t training loss: 0.013997\n",
      "epoch: 18 [46720/60000 (78%)]\t training loss: 0.009896\n",
      "epoch: 18 [47040/60000 (78%)]\t training loss: 0.000310\n",
      "epoch: 18 [47360/60000 (79%)]\t training loss: 0.042272\n",
      "epoch: 18 [47680/60000 (79%)]\t training loss: 0.000047\n",
      "epoch: 18 [48000/60000 (80%)]\t training loss: 0.040886\n",
      "epoch: 18 [48320/60000 (81%)]\t training loss: 0.000235\n",
      "epoch: 18 [48640/60000 (81%)]\t training loss: 0.000196\n",
      "epoch: 18 [48960/60000 (82%)]\t training loss: 0.003546\n",
      "epoch: 18 [49280/60000 (82%)]\t training loss: 0.001296\n",
      "epoch: 18 [49600/60000 (83%)]\t training loss: 0.000040\n",
      "epoch: 18 [49920/60000 (83%)]\t training loss: 0.000091\n",
      "epoch: 18 [50240/60000 (84%)]\t training loss: 0.000006\n",
      "epoch: 18 [50560/60000 (84%)]\t training loss: 0.000013\n",
      "epoch: 18 [50880/60000 (85%)]\t training loss: 0.006528\n",
      "epoch: 18 [51200/60000 (85%)]\t training loss: 0.001845\n",
      "epoch: 18 [51520/60000 (86%)]\t training loss: 0.046895\n",
      "epoch: 18 [51840/60000 (86%)]\t training loss: 0.000660\n",
      "epoch: 18 [52160/60000 (87%)]\t training loss: 0.335669\n",
      "epoch: 18 [52480/60000 (87%)]\t training loss: 0.000035\n",
      "epoch: 18 [52800/60000 (88%)]\t training loss: 0.027665\n",
      "epoch: 18 [53120/60000 (89%)]\t training loss: 0.001009\n",
      "epoch: 18 [53440/60000 (89%)]\t training loss: 0.000465\n",
      "epoch: 18 [53760/60000 (90%)]\t training loss: 0.004962\n",
      "epoch: 18 [54080/60000 (90%)]\t training loss: 0.000258\n",
      "epoch: 18 [54400/60000 (91%)]\t training loss: 0.000477\n",
      "epoch: 18 [54720/60000 (91%)]\t training loss: 0.002787\n",
      "epoch: 18 [55040/60000 (92%)]\t training loss: 0.000402\n",
      "epoch: 18 [55360/60000 (92%)]\t training loss: 0.000085\n",
      "epoch: 18 [55680/60000 (93%)]\t training loss: 0.002589\n",
      "epoch: 18 [56000/60000 (93%)]\t training loss: 0.004619\n",
      "epoch: 18 [56320/60000 (94%)]\t training loss: 0.000102\n",
      "epoch: 18 [56640/60000 (94%)]\t training loss: 0.000106\n",
      "epoch: 18 [56960/60000 (95%)]\t training loss: 0.112325\n",
      "epoch: 18 [57280/60000 (95%)]\t training loss: 0.000143\n",
      "epoch: 18 [57600/60000 (96%)]\t training loss: 0.011405\n",
      "epoch: 18 [57920/60000 (97%)]\t training loss: 0.000233\n",
      "epoch: 18 [58240/60000 (97%)]\t training loss: 0.024031\n",
      "epoch: 18 [58560/60000 (98%)]\t training loss: 0.015373\n",
      "epoch: 18 [58880/60000 (98%)]\t training loss: 0.002894\n",
      "epoch: 18 [59200/60000 (99%)]\t training loss: 0.206862\n",
      "epoch: 18 [59520/60000 (99%)]\t training loss: 0.000995\n",
      "epoch: 18 [59840/60000 (100%)]\t training loss: 0.191043\n",
      "\n",
      "Test dataset: Overall Loss: 0.0372, Overall Accuracy: 9895/10000 (99%)\n",
      "\n",
      "epoch: 19 [0/60000 (0%)]\t training loss: 0.000003\n",
      "epoch: 19 [320/60000 (1%)]\t training loss: 0.000034\n",
      "epoch: 19 [640/60000 (1%)]\t training loss: 0.009539\n",
      "epoch: 19 [960/60000 (2%)]\t training loss: 0.002726\n",
      "epoch: 19 [1280/60000 (2%)]\t training loss: 0.004427\n",
      "epoch: 19 [1600/60000 (3%)]\t training loss: 0.105036\n",
      "epoch: 19 [1920/60000 (3%)]\t training loss: 0.000256\n",
      "epoch: 19 [2240/60000 (4%)]\t training loss: 0.000001\n",
      "epoch: 19 [2560/60000 (4%)]\t training loss: 0.000510\n",
      "epoch: 19 [2880/60000 (5%)]\t training loss: 0.001154\n",
      "epoch: 19 [3200/60000 (5%)]\t training loss: 0.012126\n",
      "epoch: 19 [3520/60000 (6%)]\t training loss: 0.003847\n",
      "epoch: 19 [3840/60000 (6%)]\t training loss: 0.000022\n",
      "epoch: 19 [4160/60000 (7%)]\t training loss: 0.011587\n",
      "epoch: 19 [4480/60000 (7%)]\t training loss: 0.000000\n",
      "epoch: 19 [4800/60000 (8%)]\t training loss: 0.007822\n",
      "epoch: 19 [5120/60000 (9%)]\t training loss: 0.000039\n",
      "epoch: 19 [5440/60000 (9%)]\t training loss: 0.000014\n",
      "epoch: 19 [5760/60000 (10%)]\t training loss: 0.010534\n",
      "epoch: 19 [6080/60000 (10%)]\t training loss: 0.000323\n",
      "epoch: 19 [6400/60000 (11%)]\t training loss: 0.003104\n",
      "epoch: 19 [6720/60000 (11%)]\t training loss: 0.002110\n",
      "epoch: 19 [7040/60000 (12%)]\t training loss: 0.018032\n",
      "epoch: 19 [7360/60000 (12%)]\t training loss: 0.000136\n",
      "epoch: 19 [7680/60000 (13%)]\t training loss: 0.000039\n",
      "epoch: 19 [8000/60000 (13%)]\t training loss: 0.000244\n",
      "epoch: 19 [8320/60000 (14%)]\t training loss: 0.451005\n",
      "epoch: 19 [8640/60000 (14%)]\t training loss: 0.000837\n",
      "epoch: 19 [8960/60000 (15%)]\t training loss: 0.106351\n",
      "epoch: 19 [9280/60000 (15%)]\t training loss: 0.000161\n",
      "epoch: 19 [9600/60000 (16%)]\t training loss: 0.003417\n",
      "epoch: 19 [9920/60000 (17%)]\t training loss: 0.006665\n",
      "epoch: 19 [10240/60000 (17%)]\t training loss: 0.000089\n",
      "epoch: 19 [10560/60000 (18%)]\t training loss: 0.006594\n",
      "epoch: 19 [10880/60000 (18%)]\t training loss: 0.000089\n",
      "epoch: 19 [11200/60000 (19%)]\t training loss: 0.214531\n",
      "epoch: 19 [11520/60000 (19%)]\t training loss: 0.007933\n",
      "epoch: 19 [11840/60000 (20%)]\t training loss: 0.003113\n",
      "epoch: 19 [12160/60000 (20%)]\t training loss: 0.000466\n",
      "epoch: 19 [12480/60000 (21%)]\t training loss: 0.000069\n",
      "epoch: 19 [12800/60000 (21%)]\t training loss: 0.001340\n",
      "epoch: 19 [13120/60000 (22%)]\t training loss: 0.000711\n",
      "epoch: 19 [13440/60000 (22%)]\t training loss: 0.043626\n",
      "epoch: 19 [13760/60000 (23%)]\t training loss: 0.000339\n",
      "epoch: 19 [14080/60000 (23%)]\t training loss: 0.003610\n",
      "epoch: 19 [14400/60000 (24%)]\t training loss: 0.001565\n",
      "epoch: 19 [14720/60000 (25%)]\t training loss: 0.000060\n",
      "epoch: 19 [15040/60000 (25%)]\t training loss: 0.001814\n",
      "epoch: 19 [15360/60000 (26%)]\t training loss: 0.007421\n",
      "epoch: 19 [15680/60000 (26%)]\t training loss: 0.003486\n",
      "epoch: 19 [16000/60000 (27%)]\t training loss: 0.000011\n",
      "epoch: 19 [16320/60000 (27%)]\t training loss: 0.001180\n",
      "epoch: 19 [16640/60000 (28%)]\t training loss: 0.000140\n",
      "epoch: 19 [16960/60000 (28%)]\t training loss: 0.001558\n",
      "epoch: 19 [17280/60000 (29%)]\t training loss: 0.005701\n",
      "epoch: 19 [17600/60000 (29%)]\t training loss: 0.000223\n",
      "epoch: 19 [17920/60000 (30%)]\t training loss: 0.000008\n",
      "epoch: 19 [18240/60000 (30%)]\t training loss: 0.000094\n",
      "epoch: 19 [18560/60000 (31%)]\t training loss: 0.005016\n",
      "epoch: 19 [18880/60000 (31%)]\t training loss: 0.000261\n",
      "epoch: 19 [19200/60000 (32%)]\t training loss: 0.000057\n",
      "epoch: 19 [19520/60000 (33%)]\t training loss: 0.000571\n",
      "epoch: 19 [19840/60000 (33%)]\t training loss: 0.010097\n",
      "epoch: 19 [20160/60000 (34%)]\t training loss: 0.001367\n",
      "epoch: 19 [20480/60000 (34%)]\t training loss: 0.000568\n",
      "epoch: 19 [20800/60000 (35%)]\t training loss: 0.000118\n",
      "epoch: 19 [21120/60000 (35%)]\t training loss: 0.000020\n",
      "epoch: 19 [21440/60000 (36%)]\t training loss: 0.000228\n",
      "epoch: 19 [21760/60000 (36%)]\t training loss: 0.000068\n",
      "epoch: 19 [22080/60000 (37%)]\t training loss: 0.000872\n",
      "epoch: 19 [22400/60000 (37%)]\t training loss: 0.002745\n",
      "epoch: 19 [22720/60000 (38%)]\t training loss: 0.003449\n",
      "epoch: 19 [23040/60000 (38%)]\t training loss: 0.001857\n",
      "epoch: 19 [23360/60000 (39%)]\t training loss: 0.143601\n",
      "epoch: 19 [23680/60000 (39%)]\t training loss: 0.004407\n",
      "epoch: 19 [24000/60000 (40%)]\t training loss: 0.125724\n",
      "epoch: 19 [24320/60000 (41%)]\t training loss: 0.002795\n",
      "epoch: 19 [24640/60000 (41%)]\t training loss: 0.004948\n",
      "epoch: 19 [24960/60000 (42%)]\t training loss: 0.121711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 [25280/60000 (42%)]\t training loss: 0.000060\n",
      "epoch: 19 [25600/60000 (43%)]\t training loss: 0.000120\n",
      "epoch: 19 [25920/60000 (43%)]\t training loss: 0.001141\n",
      "epoch: 19 [26240/60000 (44%)]\t training loss: 0.013657\n",
      "epoch: 19 [26560/60000 (44%)]\t training loss: 0.011211\n",
      "epoch: 19 [26880/60000 (45%)]\t training loss: 0.002881\n",
      "epoch: 19 [27200/60000 (45%)]\t training loss: 0.006862\n",
      "epoch: 19 [27520/60000 (46%)]\t training loss: 0.000507\n",
      "epoch: 19 [27840/60000 (46%)]\t training loss: 0.000264\n",
      "epoch: 19 [28160/60000 (47%)]\t training loss: 0.005901\n",
      "epoch: 19 [28480/60000 (47%)]\t training loss: 0.010817\n",
      "epoch: 19 [28800/60000 (48%)]\t training loss: 0.000177\n",
      "epoch: 19 [29120/60000 (49%)]\t training loss: 0.000001\n",
      "epoch: 19 [29440/60000 (49%)]\t training loss: 0.044780\n",
      "epoch: 19 [29760/60000 (50%)]\t training loss: 0.000109\n",
      "epoch: 19 [30080/60000 (50%)]\t training loss: 0.000000\n",
      "epoch: 19 [30400/60000 (51%)]\t training loss: 0.001393\n",
      "epoch: 19 [30720/60000 (51%)]\t training loss: 0.019474\n",
      "epoch: 19 [31040/60000 (52%)]\t training loss: 0.000123\n",
      "epoch: 19 [31360/60000 (52%)]\t training loss: 0.006524\n",
      "epoch: 19 [31680/60000 (53%)]\t training loss: 0.000015\n",
      "epoch: 19 [32000/60000 (53%)]\t training loss: 0.001353\n",
      "epoch: 19 [32320/60000 (54%)]\t training loss: 0.000061\n",
      "epoch: 19 [32640/60000 (54%)]\t training loss: 0.000212\n",
      "epoch: 19 [32960/60000 (55%)]\t training loss: 0.001750\n",
      "epoch: 19 [33280/60000 (55%)]\t training loss: 0.000748\n",
      "epoch: 19 [33600/60000 (56%)]\t training loss: 0.501502\n",
      "epoch: 19 [33920/60000 (57%)]\t training loss: 0.000001\n",
      "epoch: 19 [34240/60000 (57%)]\t training loss: 0.000059\n",
      "epoch: 19 [34560/60000 (58%)]\t training loss: 0.000051\n",
      "epoch: 19 [34880/60000 (58%)]\t training loss: 0.103192\n",
      "epoch: 19 [35200/60000 (59%)]\t training loss: 0.000002\n",
      "epoch: 19 [35520/60000 (59%)]\t training loss: 0.000202\n",
      "epoch: 19 [35840/60000 (60%)]\t training loss: 0.115049\n",
      "epoch: 19 [36160/60000 (60%)]\t training loss: 0.000018\n",
      "epoch: 19 [36480/60000 (61%)]\t training loss: 0.000017\n",
      "epoch: 19 [36800/60000 (61%)]\t training loss: 0.008662\n",
      "epoch: 19 [37120/60000 (62%)]\t training loss: 0.000483\n",
      "epoch: 19 [37440/60000 (62%)]\t training loss: 0.001943\n",
      "epoch: 19 [37760/60000 (63%)]\t training loss: 0.000694\n",
      "epoch: 19 [38080/60000 (63%)]\t training loss: 0.000000\n",
      "epoch: 19 [38400/60000 (64%)]\t training loss: 0.000002\n",
      "epoch: 19 [38720/60000 (65%)]\t training loss: 0.000266\n",
      "epoch: 19 [39040/60000 (65%)]\t training loss: 0.000146\n",
      "epoch: 19 [39360/60000 (66%)]\t training loss: 0.000050\n",
      "epoch: 19 [39680/60000 (66%)]\t training loss: 0.000183\n",
      "epoch: 19 [40000/60000 (67%)]\t training loss: 0.001416\n",
      "epoch: 19 [40320/60000 (67%)]\t training loss: 0.000185\n",
      "epoch: 19 [40640/60000 (68%)]\t training loss: 0.003806\n",
      "epoch: 19 [40960/60000 (68%)]\t training loss: 0.000092\n",
      "epoch: 19 [41280/60000 (69%)]\t training loss: 0.000241\n",
      "epoch: 19 [41600/60000 (69%)]\t training loss: 0.010038\n",
      "epoch: 19 [41920/60000 (70%)]\t training loss: 0.000088\n",
      "epoch: 19 [42240/60000 (70%)]\t training loss: 0.000157\n",
      "epoch: 19 [42560/60000 (71%)]\t training loss: 0.027911\n",
      "epoch: 19 [42880/60000 (71%)]\t training loss: 0.030839\n",
      "epoch: 19 [43200/60000 (72%)]\t training loss: 0.004112\n",
      "epoch: 19 [43520/60000 (73%)]\t training loss: 0.001879\n",
      "epoch: 19 [43840/60000 (73%)]\t training loss: 0.000693\n",
      "epoch: 19 [44160/60000 (74%)]\t training loss: 0.002638\n",
      "epoch: 19 [44480/60000 (74%)]\t training loss: 0.002084\n",
      "epoch: 19 [44800/60000 (75%)]\t training loss: 0.000008\n",
      "epoch: 19 [45120/60000 (75%)]\t training loss: 0.002214\n",
      "epoch: 19 [45440/60000 (76%)]\t training loss: 0.000070\n",
      "epoch: 19 [45760/60000 (76%)]\t training loss: 0.003215\n",
      "epoch: 19 [46080/60000 (77%)]\t training loss: 0.002076\n",
      "epoch: 19 [46400/60000 (77%)]\t training loss: 0.032817\n",
      "epoch: 19 [46720/60000 (78%)]\t training loss: 0.000023\n",
      "epoch: 19 [47040/60000 (78%)]\t training loss: 0.002861\n",
      "epoch: 19 [47360/60000 (79%)]\t training loss: 0.004780\n",
      "epoch: 19 [47680/60000 (79%)]\t training loss: 0.009217\n",
      "epoch: 19 [48000/60000 (80%)]\t training loss: 0.000143\n",
      "epoch: 19 [48320/60000 (81%)]\t training loss: 0.000550\n",
      "epoch: 19 [48640/60000 (81%)]\t training loss: 0.000891\n",
      "epoch: 19 [48960/60000 (82%)]\t training loss: 0.001074\n",
      "epoch: 19 [49280/60000 (82%)]\t training loss: 0.000252\n",
      "epoch: 19 [49600/60000 (83%)]\t training loss: 0.000162\n",
      "epoch: 19 [49920/60000 (83%)]\t training loss: 0.035042\n",
      "epoch: 19 [50240/60000 (84%)]\t training loss: 0.004229\n",
      "epoch: 19 [50560/60000 (84%)]\t training loss: 0.001879\n",
      "epoch: 19 [50880/60000 (85%)]\t training loss: 0.000115\n",
      "epoch: 19 [51200/60000 (85%)]\t training loss: 0.000040\n",
      "epoch: 19 [51520/60000 (86%)]\t training loss: 0.022446\n",
      "epoch: 19 [51840/60000 (86%)]\t training loss: 0.012188\n",
      "epoch: 19 [52160/60000 (87%)]\t training loss: 0.001235\n",
      "epoch: 19 [52480/60000 (87%)]\t training loss: 0.006279\n",
      "epoch: 19 [52800/60000 (88%)]\t training loss: 0.006077\n",
      "epoch: 19 [53120/60000 (89%)]\t training loss: 0.001578\n",
      "epoch: 19 [53440/60000 (89%)]\t training loss: 0.006882\n",
      "epoch: 19 [53760/60000 (90%)]\t training loss: 0.010229\n",
      "epoch: 19 [54080/60000 (90%)]\t training loss: 0.059944\n",
      "epoch: 19 [54400/60000 (91%)]\t training loss: 0.000489\n",
      "epoch: 19 [54720/60000 (91%)]\t training loss: 0.006898\n",
      "epoch: 19 [55040/60000 (92%)]\t training loss: 0.028539\n",
      "epoch: 19 [55360/60000 (92%)]\t training loss: 0.000223\n",
      "epoch: 19 [55680/60000 (93%)]\t training loss: 0.000014\n",
      "epoch: 19 [56000/60000 (93%)]\t training loss: 0.005248\n",
      "epoch: 19 [56320/60000 (94%)]\t training loss: 0.009635\n",
      "epoch: 19 [56640/60000 (94%)]\t training loss: 0.002622\n",
      "epoch: 19 [56960/60000 (95%)]\t training loss: 0.000131\n",
      "epoch: 19 [57280/60000 (95%)]\t training loss: 0.012535\n",
      "epoch: 19 [57600/60000 (96%)]\t training loss: 0.000076\n",
      "epoch: 19 [57920/60000 (97%)]\t training loss: 0.000057\n",
      "epoch: 19 [58240/60000 (97%)]\t training loss: 0.004964\n",
      "epoch: 19 [58560/60000 (98%)]\t training loss: 0.000053\n",
      "epoch: 19 [58880/60000 (98%)]\t training loss: 0.016509\n",
      "epoch: 19 [59200/60000 (99%)]\t training loss: 0.118218\n",
      "epoch: 19 [59520/60000 (99%)]\t training loss: 0.000097\n",
      "epoch: 19 [59840/60000 (100%)]\t training loss: 0.000271\n",
      "\n",
      "Test dataset: Overall Loss: 0.0387, Overall Accuracy: 9910/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 20):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run inference on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is : 9\n",
      "Ground truth is : 9\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
    "print(f\"Ground truth is : {sample_targets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###visualize filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1)),\n",
       " Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)),\n",
       " Dropout2d(p=0.1, inplace=False),\n",
       " Dropout2d(p=0.25, inplace=False),\n",
       " Linear(in_features=4608, out_features=64, bias=True),\n",
       " Linear(in_features=64, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_children_list = list(model.children())\n",
    "convolutional_layers = []\n",
    "model_parameters = []\n",
    "model_children_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model_children_list)):\n",
    "    if type(model_children_list[i]) == nn.Conv2d:\n",
    "        model_parameters.append(model_children_list[i].weight)\n",
    "        convolutional_layers.append(model_children_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "for i, flt in enumerate(model_parameters[0]):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.imshow(flt[0, :, :].detach(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x576 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 8))\n",
    "for i, flt in enumerate(model_parameters[1]):\n",
    "    plt.subplot(8, 4, i+1)\n",
    "    plt.imshow(flt[0, :, :].detach(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###visualize feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_layer_results = [convolutional_layers[0](sample_data)]\n",
    "for i in range(1, len(convolutional_layers)):\n",
    "    per_layer_results.append(convolutional_layers[i](per_layer_results[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 26, 26])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "layer_visualisation = per_layer_results[0][0, :, :, :]\n",
    "layer_visualisation = layer_visualisation.data\n",
    "print(layer_visualisation.size())\n",
    "for i, flt in enumerate(layer_visualisation):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(flt, cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 24, 24])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x576 with 32 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5, 8))\n",
    "layer_visualisation = per_layer_results[1][0, :, :, :]\n",
    "layer_visualisation = layer_visualisation.data\n",
    "print(layer_visualisation.size())\n",
    "for i, flt in enumerate(layer_visualisation):\n",
    "    plt.subplot(8, 4, i + 1)\n",
    "    plt.imshow(flt, cmap='gray')\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
