{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashish.jha/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "/Users/ashish.jha/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "from torchvision import datasets, transforms\n",
    "from autoPyTorch import AutoNetClassification\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
    "train_ds = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))]))\n",
    "\n",
    "test_ds = datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_ds.data.numpy().reshape(-1, 28*28), test_ds.data.numpy().reshape(-1, 28*28) ,train_ds.targets.numpy(), test_ds.targets.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:09:51 Start autonet with config:\n",
      "{'embeddings': ['none'], 'lr_scheduler': ['cosine_annealing'], 'networks': ['shapedresnet'], 'preprocessors': ['truncated_svd'], 'target_size_strategies': ['none'], 'over_sampling_methods': ['none'], 'under_sampling_methods': ['none'], 'batch_loss_computation_techniques': ['standard'], 'imputation_strategies': ['median'], 'initialization_methods': ['default'], 'loss_modules': ['cross_entropy_weighted'], 'normalization_strategies': ['standardize'], 'optimizer': ['sgd'], 'hyperparameter_search_space_updates': <autoPyTorch.utils.hyperparameter_search_space_update.HyperparameterSearchSpaceUpdates object at 0x121a696d0>, 'log_level': 'info', 'max_runtime': 2000, 'min_budget': 30, 'max_budget': 120, 'validation_split': 0.1, 'result_logger_dir': '.', 'categorical_features': None, 'dataset_name': None, 'run_id': '0', 'task_id': -1, 'algorithm': 'bohb', 'portfolio_type': 'greedy', 'budget_type': 'time', 'eta': 3, 'min_workers': 1, 'working_dir': '.', 'network_interface_name': 'en0', 'memory_limit_mb': 1000000, 'use_tensorboard_logger': False, 'run_worker_on_master_node': True, 'use_pynisher': True, 'refit_validation_split': 0.0, 'cross_validator': 'none', 'cross_validator_args': {}, 'min_budget_for_cv': 0, 'shuffle': True, 'final_activation': 'softmax', 'initializer': 'simple_initializer', 'additional_logs': [], 'optimize_metric': 'accuracy', 'additional_metrics': [], 'cuda': True, 'torch_num_threads': 1, 'full_eval_each_epoch': False, 'best_over_epochs': False, 'save_models': False, 'predict_model': None, 'early_stopping_patience': inf, 'early_stopping_reset_parameters': False, 'random_seed': 2966029839, 'num_iterations': inf}\n",
      "12:09:51 [AutoNet] Start bohb\n",
      "12:09:51 DISPATCHER: started the 'discover_worker' thread\n",
      "12:09:51 WORKER: start listening for jobs\n",
      "12:09:51 DISPATCHER: started the 'job_runner' thread\n",
      "12:09:51 DISPATCHER: Pyro daemon running on 192.168.1.6:51037\n",
      "12:09:51 DISPATCHER: discovered new worker, hpbandster.run_0.worker.LDN-C02Z96Q0LVDL.52094.-14453086656\n",
      "12:09:51 HBMASTER: adjusted queue size to (0, 1)\n",
      "12:09:51 DISPATCHER: A new worker triggered discover_worker\n",
      "12:09:51 HBMASTER: starting run at 1603521591.555632\n",
      "12:09:51 WORKER: start processing job (0, 0, 0)\n",
      "12:09:51 Fit optimization pipeline\n",
      "12:09:51 [AutoNet] CV split 0 of 1\n",
      "12:09:51 Reduced initial budget 39.83972382545471 to cv budget 39.83698105812073 compensate for 0.002742767333984375\n",
      "12:10:21 Finished train with budget 39.83698105812073: Preprocessing took 14s, Training took 15s, Wrap up took 0s. Total time consumption in s: 29\n",
      "12:10:21 [AutoNet] Done with current split!\n",
      "12:10:21 Aggregate the results across the splits\n",
      "12:10:21 Process 1 additional result(s)\n",
      "12:10:21 Training ['shapedresnet'] with budget 40.0 resulted in optimize-metric-loss: -85.88333333333334 took 30.295294046401978 seconds\n",
      "12:10:21 WORKER: registered result for job (0, 0, 0) with dispatcher\n",
      "12:10:21 WORKER: start processing job (0, 0, 1)\n",
      "12:10:21 Fit optimization pipeline\n",
      "12:10:22 [AutoNet] CV split 0 of 1\n",
      "12:10:22 Reduced initial budget 39.84408617019653 to cv budget 39.84155821800232 compensate for 0.002527952194213867\n",
      "12:10:52 Finished train with budget 39.84155821800232: Preprocessing took 14s, Training took 15s, Wrap up took 0s. Total time consumption in s: 29\n",
      "12:10:52 [AutoNet] Done with current split!\n",
      "12:10:52 Aggregate the results across the splits\n",
      "12:10:52 Process 1 additional result(s)\n",
      "12:10:52 Training ['shapedresnet'] with budget 40.0 resulted in optimize-metric-loss: -96.45 took 30.346881866455078 seconds\n",
      "12:10:52 WORKER: registered result for job (0, 0, 1) with dispatcher\n",
      "12:10:52 WORKER: start processing job (0, 0, 2)\n",
      "12:10:52 Fit optimization pipeline\n",
      "12:10:52 [AutoNet] CV split 0 of 1\n",
      "12:10:52 Reduced initial budget 39.842167139053345 to cv budget 39.83929395675659 compensate for 0.0028731822967529297\n",
      "12:11:23 Finished train with budget 39.83929395675659: Preprocessing took 14s, Training took 15s, Wrap up took 0s. Total time consumption in s: 30\n",
      "12:11:23 [AutoNet] Done with current split!\n",
      "12:11:23 Aggregate the results across the splits\n",
      "12:11:23 Process 1 additional result(s)\n",
      "12:11:23 Training ['shapedresnet'] with budget 40.0 resulted in optimize-metric-loss: -91.21666666666667 took 31.06108784675598 seconds\n",
      "12:11:23 WORKER: registered result for job (0, 0, 2) with dispatcher\n",
      "12:11:23 WORKER: start processing job (0, 0, 1)\n",
      "12:11:23 Fit optimization pipeline\n",
      "12:11:23 [AutoNet] CV split 0 of 1\n",
      "12:11:23 Reduced initial budget 119.8487937450409 to cv budget 119.84637594223022 compensate for 0.0024178028106689453\n",
      "12:13:13 Finished train with budget 119.84637594223022: Preprocessing took 13s, Training took 95s, Wrap up took 0s. Total time consumption in s: 109\n",
      "12:13:13 [AutoNet] Done with current split!\n",
      "12:13:13 Aggregate the results across the splits\n",
      "12:13:13 Process 1 additional result(s)\n",
      "12:13:13 Training ['shapedresnet'] with budget 120.0 resulted in optimize-metric-loss: -35.68333333333334 took 110.331059217453 seconds\n",
      "12:13:13 WORKER: registered result for job (0, 0, 1) with dispatcher\n",
      "12:13:13 WORKER: start processing job (1, 0, 0)\n",
      "12:13:13 Fit optimization pipeline\n",
      "12:13:13 [AutoNet] CV split 0 of 1\n",
      "12:13:13 Reduced initial budget 119.85235810279846 to cv budget 119.84982013702393 compensate for 0.002537965774536133\n",
      "12:15:03 Finished train with budget 119.84982013702393: Preprocessing took 14s, Training took 95s, Wrap up took 0s. Total time consumption in s: 110\n",
      "12:15:04 [AutoNet] Done with current split!\n",
      "12:15:04 Aggregate the results across the splits\n",
      "12:15:04 Process 1 additional result(s)\n",
      "12:15:04 Training ['shapedresnet'] with budget 120.0 resulted in optimize-metric-loss: -86.03333333333333 took 110.52541184425354 seconds\n",
      "12:15:04 WORKER: registered result for job (1, 0, 0) with dispatcher\n",
      "12:15:04 WORKER: start processing job (1, 0, 1)\n",
      "12:15:04 Fit optimization pipeline\n",
      "12:15:04 [AutoNet] CV split 0 of 1\n",
      "12:15:04 Reduced initial budget 119.85196900367737 to cv budget 119.84916400909424 compensate for 0.002804994583129883\n",
      "12:16:54 Finished train with budget 119.84916400909424: Preprocessing took 14s, Training took 95s, Wrap up took 0s. Total time consumption in s: 110\n",
      "12:16:54 [AutoNet] Done with current split!\n",
      "12:16:54 Aggregate the results across the splits\n",
      "12:16:54 Process 1 additional result(s)\n",
      "12:16:54 Training ['shapedresnet'] with budget 120.0 resulted in optimize-metric-loss: -79.13333333333334 took 110.39413595199585 seconds\n",
      "12:16:54 WORKER: registered result for job (1, 0, 1) with dispatcher\n",
      "12:16:54 WORKER: start processing job (2, 0, 0)\n",
      "12:16:54 Fit optimization pipeline\n",
      "12:16:54 [AutoNet] CV split 0 of 1\n",
      "12:16:54 Reduced initial budget 39.849900007247925 to cv budget 39.84688401222229 compensate for 0.0030159950256347656\n",
      "12:17:24 Finished train with budget 39.84688401222229: Preprocessing took 14s, Training took 15s, Wrap up took 0s. Total time consumption in s: 30\n",
      "12:17:24 [AutoNet] Done with current split!\n",
      "12:17:24 Aggregate the results across the splits\n",
      "12:17:25 Process 1 additional result(s)\n",
      "12:17:25 Training ['shapedresnet'] with budget 40.0 resulted in optimize-metric-loss: -30.833333333333336 took 30.53559899330139 seconds\n",
      "12:17:25 WORKER: registered result for job (2, 0, 0) with dispatcher\n",
      "12:17:25 WORKER: start processing job (2, 0, 1)\n",
      "12:17:25 Fit optimization pipeline\n",
      "12:17:25 [AutoNet] CV split 0 of 1\n",
      "12:17:25 Reduced initial budget 39.853229999542236 to cv budget 39.850656032562256 compensate for 0.0025739669799804688\n",
      "12:17:55 Finished train with budget 39.850656032562256: Preprocessing took 14s, Training took 15s, Wrap up took 0s. Total time consumption in s: 29\n",
      "12:17:55 [AutoNet] Done with current split!\n",
      "12:17:55 Aggregate the results across the splits\n",
      "12:17:55 Process 1 additional result(s)\n",
      "12:17:55 Training ['shapedresnet'] with budget 40.0 resulted in optimize-metric-loss: -90.83333333333333 took 30.31197500228882 seconds\n",
      "12:17:55 WORKER: registered result for job (2, 0, 1) with dispatcher\n",
      "12:17:55 WORKER: start processing job (2, 0, 2)\n",
      "12:17:55 Fit optimization pipeline\n",
      "12:17:55 [AutoNet] CV split 0 of 1\n",
      "12:17:55 Reduced initial budget 39.849709033966064 to cv budget 39.8465690612793 compensate for 0.003139972686767578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:18:26 Finished train with budget 39.8465690612793: Preprocessing took 14s, Training took 15s, Wrap up took 0s. Total time consumption in s: 30\n",
      "12:18:26 [AutoNet] Done with current split!\n",
      "12:18:26 Aggregate the results across the splits\n",
      "12:18:26 Process 1 additional result(s)\n",
      "12:18:26 Training ['shapedresnet'] with budget 40.0 resulted in optimize-metric-loss: -50.31666666666666 took 31.141352891921997 seconds\n",
      "12:18:26 WORKER: registered result for job (2, 0, 2) with dispatcher\n",
      "12:18:26 WORKER: start processing job (2, 0, 1)\n",
      "12:18:26 Fit optimization pipeline\n",
      "12:18:26 [AutoNet] CV split 0 of 1\n",
      "12:18:26 Reduced initial budget 119.8480851650238 to cv budget 119.845543384552 compensate for 0.002541780471801758\n",
      "12:20:16 Finished train with budget 119.845543384552: Preprocessing took 14s, Training took 95s, Wrap up took 0s. Total time consumption in s: 109\n",
      "12:20:16 [AutoNet] Done with current split!\n",
      "12:20:16 Aggregate the results across the splits\n",
      "12:20:16 Process 1 additional result(s)\n",
      "12:20:16 Training ['shapedresnet'] with budget 120.0 resulted in optimize-metric-loss: -82.86666666666666 took 110.33200788497925 seconds\n",
      "12:20:16 WORKER: registered result for job (2, 0, 1) with dispatcher\n",
      "12:20:16 WORKER: start processing job (3, 0, 0)\n",
      "12:20:16 Fit optimization pipeline\n",
      "12:20:17 [AutoNet] CV split 0 of 1\n",
      "12:20:17 Reduced initial budget 119.85416793823242 to cv budget 119.85147595405579 compensate for 0.002691984176635742\n",
      "12:22:07 Finished train with budget 119.85147595405579: Preprocessing took 14s, Training took 95s, Wrap up took 0s. Total time consumption in s: 110\n",
      "12:22:07 [AutoNet] Done with current split!\n",
      "12:22:07 Aggregate the results across the splits\n",
      "12:22:07 Process 1 additional result(s)\n",
      "12:22:07 Training ['shapedresnet'] with budget 120.0 resulted in optimize-metric-loss: -78.31666666666666 took 110.45583510398865 seconds\n",
      "12:22:07 WORKER: registered result for job (3, 0, 0) with dispatcher\n",
      "12:22:07 WORKER: start processing job (3, 0, 1)\n",
      "12:22:07 Fit optimization pipeline\n",
      "12:22:07 [AutoNet] CV split 0 of 1\n",
      "12:22:07 Reduced initial budget 119.84033036231995 to cv budget 119.83723020553589 compensate for 0.003100156784057617\n",
      "12:24:01 Finished train with budget 119.83723020553589: Preprocessing took 14s, Training took 95s, Wrap up took 4s. Total time consumption in s: 114\n",
      "12:24:01 [AutoNet] Done with current split!\n",
      "12:24:01 Aggregate the results across the splits\n",
      "12:24:01 Process 1 additional result(s)\n",
      "12:24:01 Training ['shapedresnet'] with budget 120.0 resulted in optimize-metric-loss: -83.3 took 114.4436297416687 seconds\n",
      "12:24:01 WORKER: registered result for job (3, 0, 1) with dispatcher\n",
      "12:24:01 WORKER: start processing job (4, 0, 0)\n",
      "12:24:01 Fit optimization pipeline\n",
      "12:24:02 [AutoNet] CV split 0 of 1\n",
      "12:24:02 Reduced initial budget 39.84242296218872 to cv budget 39.8395619392395 compensate for 0.00286102294921875\n",
      "12:24:32 Finished train with budget 39.8395619392395: Preprocessing took 14s, Training took 15s, Wrap up took 0s. Total time consumption in s: 29\n",
      "12:24:32 [AutoNet] Done with current split!\n",
      "12:24:32 Aggregate the results across the splits\n",
      "12:24:32 Process 1 additional result(s)\n",
      "12:24:32 Training ['shapedresnet'] with budget 40.0 resulted in optimize-metric-loss: -87.75 took 30.315235137939453 seconds\n",
      "12:24:32 WORKER: registered result for job (4, 0, 0) with dispatcher\n",
      "12:24:32 WORKER: start processing job (4, 0, 1)\n",
      "12:24:32 Fit optimization pipeline\n",
      "12:24:32 [AutoNet] CV split 0 of 1\n",
      "12:24:32 Reduced initial budget 39.84375715255737 to cv budget 39.840437173843384 compensate for 0.003319978713989258\n",
      "12:25:02 Finished train with budget 39.840437173843384: Preprocessing took 14s, Training took 15s, Wrap up took 0s. Total time consumption in s: 30\n",
      "12:25:02 [AutoNet] Done with current split!\n",
      "12:25:02 Aggregate the results across the splits\n",
      "12:25:02 Process 1 additional result(s)\n",
      "12:25:02 Training ['shapedresnet'] with budget 40.0 resulted in optimize-metric-loss: -87.6 took 30.6161527633667 seconds\n",
      "12:25:02 WORKER: registered result for job (4, 0, 1) with dispatcher\n",
      "12:25:02 WORKER: start processing job (4, 0, 2)\n",
      "12:25:02 Fit optimization pipeline\n",
      "12:25:03 [AutoNet] CV split 0 of 1\n",
      "12:25:03 Reduced initial budget 39.8415732383728 to cv budget 39.83828401565552 compensate for 0.0032892227172851562\n",
      "12:25:32 Finished train with budget 39.83828401565552: Preprocessing took 14s, Training took 15s, Wrap up took 0s. Total time consumption in s: 29\n",
      "12:25:33 [AutoNet] Done with current split!\n",
      "12:25:33 Aggregate the results across the splits\n",
      "12:25:33 Process 1 additional result(s)\n",
      "12:25:33 Training ['shapedresnet'] with budget 40.0 resulted in optimize-metric-loss: -87.75 took 30.3200740814209 seconds\n",
      "12:25:33 WORKER: registered result for job (4, 0, 2) with dispatcher\n",
      "12:25:33 WORKER: start processing job (4, 0, 0)\n",
      "12:25:33 Fit optimization pipeline\n",
      "12:25:33 [AutoNet] CV split 0 of 1\n",
      "12:25:33 Reduced initial budget 119.84636497497559 to cv budget 119.84359884262085 compensate for 0.002766132354736328\n",
      "12:27:23 Finished train with budget 119.84359884262085: Preprocessing took 14s, Training took 95s, Wrap up took 0s. Total time consumption in s: 109\n",
      "12:27:23 [AutoNet] Done with current split!\n",
      "12:27:23 Aggregate the results across the splits\n",
      "12:27:23 Process 1 additional result(s)\n",
      "12:27:23 Training ['shapedresnet'] with budget 120.0 resulted in optimize-metric-loss: -75.44999999999999 took 110.34230589866638 seconds\n",
      "12:27:23 WORKER: registered result for job (4, 0, 0) with dispatcher\n",
      "12:27:23 WORKER: start processing job (5, 0, 0)\n",
      "12:27:23 Fit optimization pipeline\n",
      "12:27:23 [AutoNet] CV split 0 of 1\n",
      "12:27:23 Reduced initial budget 119.84273219108582 to cv budget 119.84028315544128 compensate for 0.00244903564453125\n",
      "12:29:14 Finished train with budget 119.84028315544128: Preprocessing took 14s, Training took 95s, Wrap up took 1s. Total time consumption in s: 111\n",
      "12:29:14 [AutoNet] Done with current split!\n",
      "12:29:14 Aggregate the results across the splits\n",
      "12:29:14 Process 1 additional result(s)\n",
      "12:29:14 Training ['shapedresnet'] with budget 120.0 resulted in optimize-metric-loss: -92.25 took 111.4235668182373 seconds\n",
      "12:29:14 WORKER: registered result for job (5, 0, 0) with dispatcher\n",
      "12:29:14 WORKER: start processing job (5, 0, 1)\n",
      "12:29:14 Fit optimization pipeline\n",
      "12:29:15 [AutoNet] CV split 0 of 1\n",
      "12:29:15 Reduced initial budget 119.85264921188354 to cv budget 119.84993720054626 compensate for 0.0027120113372802734\n",
      "12:31:05 Finished train with budget 119.84993720054626: Preprocessing took 15s, Training took 94s, Wrap up took 0s. Total time consumption in s: 110\n",
      "12:31:05 [AutoNet] Done with current split!\n",
      "12:31:05 Aggregate the results across the splits\n",
      "12:31:05 Process 1 additional result(s)\n",
      "12:31:05 Training ['shapedresnet'] with budget 120.0 resulted in optimize-metric-loss: -87.58333333333333 took 110.50003004074097 seconds\n",
      "12:31:05 WORKER: registered result for job (5, 0, 1) with dispatcher\n",
      "12:31:05 WORKER: start processing job (6, 0, 0)\n",
      "12:31:05 Fit optimization pipeline\n",
      "12:31:05 [AutoNet] CV split 0 of 1\n",
      "12:31:05 Reduced initial budget 39.85239386558533 to cv budget 39.84985589981079 compensate for 0.002537965774536133\n",
      "12:31:35 Finished train with budget 39.84985589981079: Preprocessing took 14s, Training took 15s, Wrap up took 0s. Total time consumption in s: 30\n",
      "12:31:35 [AutoNet] Done with current split!\n",
      "12:31:35 Aggregate the results across the splits\n",
      "12:31:35 Process 1 additional result(s)\n",
      "12:31:35 Training ['shapedresnet'] with budget 40.0 resulted in optimize-metric-loss: -88.46666666666667 took 30.352888107299805 seconds\n",
      "12:31:35 WORKER: registered result for job (6, 0, 0) with dispatcher\n",
      "12:31:35 WORKER: start processing job (6, 0, 1)\n",
      "12:31:35 Fit optimization pipeline\n",
      "12:31:36 [AutoNet] CV split 0 of 1\n",
      "12:31:36 Reduced initial budget 39.85001993179321 to cv budget 39.84745407104492 compensate for 0.0025658607482910156\n",
      "12:32:06 Finished train with budget 39.84745407104492: Preprocessing took 14s, Training took 15s, Wrap up took 0s. Total time consumption in s: 30\n",
      "12:32:06 [AutoNet] Done with current split!\n",
      "12:32:06 Aggregate the results across the splits\n",
      "12:32:06 Process 1 additional result(s)\n",
      "12:32:06 Training ['shapedresnet'] with budget 40.0 resulted in optimize-metric-loss: -65.43333333333334 took 30.408524990081787 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:32:06 WORKER: registered result for job (6, 0, 1) with dispatcher\n",
      "12:32:06 WORKER: start processing job (6, 0, 2)\n",
      "12:32:06 Fit optimization pipeline\n",
      "12:32:06 [AutoNet] CV split 0 of 1\n",
      "12:32:06 Reduced initial budget 39.85001492500305 to cv budget 39.84752368927002 compensate for 0.0024912357330322266\n",
      "12:32:36 Finished train with budget 39.84752368927002: Preprocessing took 14s, Training took 15s, Wrap up took 0s. Total time consumption in s: 30\n",
      "12:32:36 [AutoNet] Done with current split!\n",
      "12:32:36 Aggregate the results across the splits\n",
      "12:32:36 Process 1 additional result(s)\n",
      "12:32:36 Training ['shapedresnet'] with budget 40.0 resulted in optimize-metric-loss: -87.76666666666667 took 30.414698123931885 seconds\n",
      "12:32:36 WORKER: registered result for job (6, 0, 2) with dispatcher\n",
      "12:32:36 WORKER: start processing job (6, 0, 0)\n",
      "12:32:36 Fit optimization pipeline\n",
      "12:32:36 [AutoNet] CV split 0 of 1\n",
      "12:32:36 Reduced initial budget 119.84491205215454 to cv budget 119.84225392341614 compensate for 0.0026581287384033203\n",
      "12:34:27 Finished train with budget 119.84225392341614: Preprocessing took 14s, Training took 95s, Wrap up took 0s. Total time consumption in s: 110\n",
      "12:34:27 [AutoNet] Done with current split!\n",
      "12:34:27 Aggregate the results across the splits\n",
      "12:34:27 Process 1 additional result(s)\n",
      "12:34:27 Training ['shapedresnet'] with budget 120.0 resulted in optimize-metric-loss: -84.7 took 110.60935091972351 seconds\n",
      "12:34:27 WORKER: registered result for job (6, 0, 0) with dispatcher\n",
      "12:34:27 WORKER: start processing job (7, 0, 0)\n",
      "12:34:27 Fit optimization pipeline\n",
      "12:34:27 [AutoNet] CV split 0 of 1\n",
      "12:34:27 Reduced initial budget 119.85114908218384 to cv budget 119.84841179847717 compensate for 0.002737283706665039\n",
      "12:36:17 Finished train with budget 119.84841179847717: Preprocessing took 14s, Training took 95s, Wrap up took 0s. Total time consumption in s: 110\n",
      "12:36:17 [AutoNet] Done with current split!\n",
      "12:36:17 Aggregate the results across the splits\n",
      "12:36:17 Process 1 additional result(s)\n",
      "12:36:17 Training ['shapedresnet'] with budget 120.0 resulted in optimize-metric-loss: -86.98333333333333 took 110.4909040927887 seconds\n",
      "12:36:17 WORKER: registered result for job (7, 0, 0) with dispatcher\n",
      "12:36:17 WORKER: start processing job (7, 0, 1)\n",
      "12:36:17 Fit optimization pipeline\n",
      "12:36:17 [AutoNet] CV split 0 of 1\n",
      "12:36:17 Reduced initial budget 119.84928584098816 to cv budget 119.84642791748047 compensate for 0.0028579235076904297\n",
      "12:38:08 Finished train with budget 119.84642791748047: Preprocessing took 14s, Training took 95s, Wrap up took 0s. Total time consumption in s: 110\n",
      "12:38:08 [AutoNet] Done with current split!\n",
      "12:38:08 Aggregate the results across the splits\n",
      "12:38:08 Process 1 additional result(s)\n",
      "12:38:08 Training ['shapedresnet'] with budget 120.0 resulted in optimize-metric-loss: -85.88333333333334 took 110.49421811103821 seconds\n",
      "12:38:08 WORKER: registered result for job (7, 0, 1) with dispatcher\n",
      "12:38:08 WORKER: start processing job (8, 0, 0)\n",
      "12:38:08 Fit optimization pipeline\n",
      "12:38:08 [AutoNet] CV split 0 of 1\n",
      "12:38:08 Reduced initial budget 39.852333068847656 to cv budget 39.849485874176025 compensate for 0.0028471946716308594\n",
      "12:38:41 Finished train with budget 39.849485874176025: Preprocessing took 14s, Training took 15s, Wrap up took 2s. Total time consumption in s: 32\n",
      "12:38:41 [AutoNet] Done with current split!\n",
      "12:38:41 Aggregate the results across the splits\n",
      "12:38:41 Process 1 additional result(s)\n",
      "12:38:41 Training ['shapedresnet'] with budget 40.0 resulted in optimize-metric-loss: -88.43333333333334 took 32.97603797912598 seconds\n",
      "12:38:41 WORKER: registered result for job (8, 0, 0) with dispatcher\n",
      "12:38:41 WORKER: start processing job (8, 0, 1)\n",
      "12:38:41 Fit optimization pipeline\n",
      "12:38:41 [AutoNet] CV split 0 of 1\n",
      "12:38:41 Reduced initial budget 39.85080289840698 to cv budget 39.84825396537781 compensate for 0.0025489330291748047\n",
      "12:39:11 Finished train with budget 39.84825396537781: Preprocessing took 14s, Training took 15s, Wrap up took 0s. Total time consumption in s: 29\n",
      "12:39:11 [AutoNet] Done with current split!\n",
      "12:39:11 Aggregate the results across the splits\n",
      "12:39:11 Process 1 additional result(s)\n",
      "12:39:11 Training ['shapedresnet'] with budget 40.0 resulted in optimize-metric-loss: -77.8 took 30.33025312423706 seconds\n",
      "12:39:11 WORKER: registered result for job (8, 0, 1) with dispatcher\n",
      "12:39:11 WORKER: start processing job (8, 0, 2)\n",
      "12:39:11 Fit optimization pipeline\n",
      "12:39:11 [AutoNet] CV split 0 of 1\n",
      "12:39:11 Reduced initial budget 39.85410189628601 to cv budget 39.851293087005615 compensate for 0.002808809280395508\n",
      "12:39:42 Finished train with budget 39.851293087005615: Preprocessing took 14s, Training took 15s, Wrap up took 0s. Total time consumption in s: 30\n",
      "12:39:42 [AutoNet] Done with current split!\n",
      "12:39:42 Aggregate the results across the splits\n",
      "12:39:42 Process 1 additional result(s)\n",
      "12:39:42 Training ['shapedresnet'] with budget 40.0 resulted in optimize-metric-loss: -84.41666666666666 took 30.534590005874634 seconds\n",
      "12:39:42 WORKER: registered result for job (8, 0, 2) with dispatcher\n",
      "12:39:42 WORKER: start processing job (8, 0, 0)\n",
      "12:39:42 Fit optimization pipeline\n",
      "12:39:42 [AutoNet] CV split 0 of 1\n",
      "12:39:42 Reduced initial budget 119.85154414176941 to cv budget 119.84885811805725 compensate for 0.002686023712158203\n",
      "12:41:35 Finished train with budget 119.84885811805725: Preprocessing took 14s, Training took 95s, Wrap up took 2s. Total time consumption in s: 112\n",
      "12:41:35 [AutoNet] Done with current split!\n",
      "12:41:35 Aggregate the results across the splits\n",
      "12:41:35 Process 1 additional result(s)\n",
      "12:41:35 Training ['shapedresnet'] with budget 120.0 resulted in optimize-metric-loss: -93.38333333333333 took 113.07796669006348 seconds\n",
      "12:41:35 WORKER: registered result for job (8, 0, 0) with dispatcher\n",
      "12:41:35 HBMASTER: Timelimit reached: wait for remaining 0 jobs\n",
      "12:41:35 DISPATCHER: Dispatcher shutting down\n",
      "12:41:35 DISPATCHER: shut down complete\n",
      "12:41:35 Start autonet with config:\n",
      "{'embeddings': ['none'], 'lr_scheduler': ['cosine_annealing'], 'networks': ['shapedresnet'], 'preprocessors': ['truncated_svd'], 'target_size_strategies': ['none'], 'over_sampling_methods': ['none'], 'under_sampling_methods': ['none'], 'batch_loss_computation_techniques': ['standard'], 'imputation_strategies': ['median'], 'initialization_methods': ['default'], 'loss_modules': ['cross_entropy_weighted'], 'normalization_strategies': ['standardize'], 'optimizer': ['sgd'], 'hyperparameter_search_space_updates': <autoPyTorch.utils.hyperparameter_search_space_update.HyperparameterSearchSpaceUpdates object at 0x103575dd0>, 'log_level': 'info', 'max_runtime': 2000, 'min_budget': 30, 'max_budget': 120, 'validation_split': 0.1, 'result_logger_dir': '.', 'categorical_features': None, 'dataset_name': None, 'run_id': '0', 'task_id': -1, 'algorithm': 'bohb', 'portfolio_type': 'greedy', 'budget_type': 'time', 'eta': 3, 'min_workers': 1, 'working_dir': '.', 'network_interface_name': 'en0', 'memory_limit_mb': 1000000, 'use_tensorboard_logger': False, 'run_worker_on_master_node': True, 'use_pynisher': True, 'refit_validation_split': 0.0, 'cross_validator': 'none', 'cross_validator_args': {}, 'min_budget_for_cv': 0, 'shuffle': True, 'final_activation': 'softmax', 'initializer': 'simple_initializer', 'additional_logs': [], 'optimize_metric': 'accuracy', 'additional_metrics': [], 'cuda': True, 'torch_num_threads': 1, 'full_eval_each_epoch': False, 'best_over_epochs': False, 'save_models': False, 'predict_model': None, 'early_stopping_patience': inf, 'early_stopping_reset_parameters': False, 'random_seed': 2966029839, 'num_iterations': inf, 'cv_splits': 1, 'increase_number_of_trained_datasets': False}\n",
      "12:41:35 Start Refitting\n",
      "12:41:36 [AutoNet] CV split 0 of 1\n",
      "12:41:36 Reduced initial budget 39.93844199180603 to cv budget 39.93775224685669 compensate for 0.0006897449493408203\n",
      "12:42:05 Finished train with budget 39.93775224685669: Preprocessing took 16s, Training took 13s, Wrap up took 0s. Total time consumption in s: 29\n",
      "12:42:06 [AutoNet] Done with current split!\n",
      "12:42:06 Aggregate the results across the splits\n",
      "12:42:06 Process 1 additional result(s)\n",
      "12:42:06 Done Refitting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'optimized_hyperparameter_config': {'CreateDataLoader:batch_size': 125,\n",
       "  'Imputation:strategy': 'median',\n",
       "  'InitializationSelector:initialization_method': 'default',\n",
       "  'InitializationSelector:initializer:initialize_bias': 'No',\n",
       "  'LearningrateSchedulerSelector:lr_scheduler': 'cosine_annealing',\n",
       "  'LossModuleSelector:loss_module': 'cross_entropy_weighted',\n",
       "  'NetworkSelector:network': 'shapedresnet',\n",
       "  'NormalizationStrategySelector:normalization_strategy': 'standardize',\n",
       "  'OptimizerSelector:optimizer': 'sgd',\n",
       "  'PreprocessorSelector:preprocessor': 'truncated_svd',\n",
       "  'ResamplingStrategySelector:over_sampling_method': 'none',\n",
       "  'ResamplingStrategySelector:target_size_strategy': 'none',\n",
       "  'ResamplingStrategySelector:under_sampling_method': 'none',\n",
       "  'TrainNode:batch_loss_computation_technique': 'standard',\n",
       "  'LearningrateSchedulerSelector:cosine_annealing:T_max': 10,\n",
       "  'LearningrateSchedulerSelector:cosine_annealing:eta_min': 2,\n",
       "  'NetworkSelector:shapedresnet:activation': 'relu',\n",
       "  'NetworkSelector:shapedresnet:blocks_per_group': 4,\n",
       "  'NetworkSelector:shapedresnet:max_units': 13,\n",
       "  'NetworkSelector:shapedresnet:num_groups': 2,\n",
       "  'NetworkSelector:shapedresnet:resnet_shape': 'brick',\n",
       "  'NetworkSelector:shapedresnet:use_dropout': 0,\n",
       "  'NetworkSelector:shapedresnet:use_shake_drop': 0,\n",
       "  'NetworkSelector:shapedresnet:use_shake_shake': 0,\n",
       "  'OptimizerSelector:sgd:learning_rate': 0.06829146967649465,\n",
       "  'OptimizerSelector:sgd:momentum': 0.9343847098348538,\n",
       "  'OptimizerSelector:sgd:weight_decay': 0.0002425066735211845,\n",
       "  'PreprocessorSelector:truncated_svd:target_dim': 100},\n",
       " 'budget': 40.0,\n",
       " 'loss': -96.45,\n",
       " 'info': {'loss': 0.12337125303244502,\n",
       "  'model_parameters': 176110.0,\n",
       "  'train_accuracy': 96.28550185873605,\n",
       "  'lr_scheduler_converged': 0.0,\n",
       "  'lr': 0.06829146967649465,\n",
       "  'val_accuracy': 96.45}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running Auto-PyTorch\n",
    "autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
    "                                    log_level='info',\n",
    "                                    max_runtime=20000,\n",
    "                                    min_budget=100,\n",
    "                                    max_budget=1500)\n",
    "\n",
    "autoPyTorch.fit(X_train, y_train, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score 0.964\n"
     ]
    }
   ],
   "source": [
    "y_pred = autoPyTorch.predict(X_test)\n",
    "\n",
    "print(\"Accuracy score\", np.mean(y_pred.reshape(-1) == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (1): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Linear(in_features=100, out_features=100, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Linear(in_features=100, out_features=100, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Linear(in_features=100, out_features=100, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Linear(in_features=100, out_features=100, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): ResBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Linear(in_features=100, out_features=100, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Linear(in_features=100, out_features=100, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Linear(in_features=100, out_features=100, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (layers): Sequential(\n",
      "        (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "        (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (4): ReLU()\n",
      "        (5): Linear(in_features=100, out_features=100, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (3): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pytorch_model = autoPyTorch.get_pytorch_model()\n",
    "print(pytorch_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'convnet_arch.pdf'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 784)\n",
    "y = pytorch_model(x)\n",
    "arch = make_dot(y.mean(), params=dict(pytorch_model.named_parameters()))\n",
    "arch.format=\"pdf\"\n",
    "arch.filename = \"convnet_arch\"\n",
    "arch.render(view=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Configuration space object:\n",
       "  Hyperparameters:\n",
       "    CreateDataLoader:batch_size, Type: Constant, Value: 125\n",
       "    Imputation:strategy, Type: Categorical, Choices: {median}, Default: median\n",
       "    InitializationSelector:initialization_method, Type: Categorical, Choices: {default}, Default: default\n",
       "    InitializationSelector:initializer:initialize_bias, Type: Constant, Value: No\n",
       "    LearningrateSchedulerSelector:cosine_annealing:T_max, Type: Constant, Value: 10\n",
       "    LearningrateSchedulerSelector:cosine_annealing:eta_min, Type: Constant, Value: 2\n",
       "    LearningrateSchedulerSelector:lr_scheduler, Type: Categorical, Choices: {cosine_annealing}, Default: cosine_annealing\n",
       "    LossModuleSelector:loss_module, Type: Categorical, Choices: {cross_entropy_weighted}, Default: cross_entropy_weighted\n",
       "    NetworkSelector:network, Type: Categorical, Choices: {shapedresnet}, Default: shapedresnet\n",
       "    NetworkSelector:shapedresnet:activation, Type: Constant, Value: relu\n",
       "    NetworkSelector:shapedresnet:blocks_per_group, Type: UniformInteger, Range: [1, 4], Default: 2\n",
       "    NetworkSelector:shapedresnet:max_units, Type: UniformInteger, Range: [10, 1024], Default: 101, on log-scale\n",
       "    NetworkSelector:shapedresnet:num_groups, Type: UniformInteger, Range: [1, 9], Default: 5\n",
       "    NetworkSelector:shapedresnet:resnet_shape, Type: Constant, Value: brick\n",
       "    NetworkSelector:shapedresnet:use_dropout, Type: Constant, Value: 0\n",
       "    NetworkSelector:shapedresnet:use_shake_drop, Type: Constant, Value: 0\n",
       "    NetworkSelector:shapedresnet:use_shake_shake, Type: Constant, Value: 0\n",
       "    NormalizationStrategySelector:normalization_strategy, Type: Categorical, Choices: {standardize}, Default: standardize\n",
       "    OptimizerSelector:optimizer, Type: Categorical, Choices: {sgd}, Default: sgd\n",
       "    OptimizerSelector:sgd:learning_rate, Type: UniformFloat, Range: [0.0001, 0.1], Default: 0.0031622777, on log-scale\n",
       "    OptimizerSelector:sgd:momentum, Type: UniformFloat, Range: [0.1, 0.999], Default: 0.5495\n",
       "    OptimizerSelector:sgd:weight_decay, Type: UniformFloat, Range: [1e-05, 0.1], Default: 0.050005\n",
       "    PreprocessorSelector:preprocessor, Type: Categorical, Choices: {truncated_svd}, Default: truncated_svd\n",
       "    PreprocessorSelector:truncated_svd:target_dim, Type: Constant, Value: 100\n",
       "    ResamplingStrategySelector:over_sampling_method, Type: Categorical, Choices: {none}, Default: none\n",
       "    ResamplingStrategySelector:target_size_strategy, Type: Categorical, Choices: {none}, Default: none\n",
       "    ResamplingStrategySelector:under_sampling_method, Type: Categorical, Choices: {none}, Default: none\n",
       "    TrainNode:batch_loss_computation_technique, Type: Categorical, Choices: {standard}, Default: standard\n",
       "  Conditions:\n",
       "    LearningrateSchedulerSelector:cosine_annealing:T_max | LearningrateSchedulerSelector:lr_scheduler == 'cosine_annealing'\n",
       "    LearningrateSchedulerSelector:cosine_annealing:eta_min | LearningrateSchedulerSelector:lr_scheduler == 'cosine_annealing'\n",
       "    NetworkSelector:shapedresnet:activation | NetworkSelector:network == 'shapedresnet'\n",
       "    NetworkSelector:shapedresnet:blocks_per_group | NetworkSelector:network == 'shapedresnet'\n",
       "    NetworkSelector:shapedresnet:max_units | NetworkSelector:network == 'shapedresnet'\n",
       "    NetworkSelector:shapedresnet:num_groups | NetworkSelector:network == 'shapedresnet'\n",
       "    NetworkSelector:shapedresnet:resnet_shape | NetworkSelector:network == 'shapedresnet'\n",
       "    NetworkSelector:shapedresnet:use_dropout | NetworkSelector:network == 'shapedresnet'\n",
       "    NetworkSelector:shapedresnet:use_shake_drop | NetworkSelector:network == 'shapedresnet'\n",
       "    NetworkSelector:shapedresnet:use_shake_shake | NetworkSelector:network == 'shapedresnet'\n",
       "    OptimizerSelector:sgd:learning_rate | OptimizerSelector:optimizer == 'sgd'\n",
       "    OptimizerSelector:sgd:momentum | OptimizerSelector:optimizer == 'sgd'\n",
       "    OptimizerSelector:sgd:weight_decay | OptimizerSelector:optimizer == 'sgd'\n",
       "    PreprocessorSelector:truncated_svd:target_dim | PreprocessorSelector:preprocessor == 'truncated_svd'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoPyTorch.get_hyperparameter_search_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
